<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Windows下查看GPU(NVIDI)使用情况</title>
      <link href="/2021/08/08/windows-xia-cha-kan-gpu-nvidia-shi-yong-qing-kuang/"/>
      <url>/2021/08/08/windows-xia-cha-kan-gpu-nvidia-shi-yong-qing-kuang/</url>
      
        <content type="html"><![CDATA[<p>我们使用GPU做计算的时候，想了解GPU的使用情况，但是任务管理器没有GPU的信息。还好NVIDIA提供了相关的命令。</p><h2 id="nvidia-smi-exe"><a href="#nvidia-smi-exe" class="headerlink" title="nvidia-smi.exe"></a>nvidia-smi.exe</h2><p>这个命令是在路径 <strong><code>C:\Program Files\NVIDIA Corporation\NVSMI</code></strong> 目录下的<code>nvidia-smi.exe</code>。</p><blockquote><p>nvidia-smi stands for <em>The NVIDIA System Management Interface</em></p></blockquote><p>我们在该目录下打开<code>powershell</code>软件，然后输入一下命令：</p><pre><code class="powershell">nvidia-smi.exe -l 10</code></pre><blockquote><p>上述命令表示<strong>10秒钟更新一次信息</strong>，<code>-l</code> stands for <code>-loop</code> </p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmrtm4xufyj20mb0by0ts.jpg"></p><h2 id="nvidia-smi-output-分析"><a href="#nvidia-smi-output-分析" class="headerlink" title="nvidia-smi output 分析"></a>nvidia-smi output 分析</h2><p>上图是服务器上 <code>GeForce GTX 1050 Ti</code> 的信息，下面一一解读参数。 上面的表格中的红框中的信息与信息对应：</p><ul><li><p><strong>GPU</strong>：GPU 编号（即ID）； </p></li><li><p><strong>Name</strong>：GPU 型号；</p></li><li><p><strong>TCC/WDDM</strong>：NVIDIA Tesla/Quadro 系列高端 GPU 在 <strong>Windows 环境</strong>下可以配置为 Tesla 计算集群（Tesla Compute Cluster，简称 TCC）模式或 Windows 显示驱动模型（Windows Display Driver Model，简称 WDDM）模式。两种模式有不同适用场景。</p><ul><li><p><code>TCC</code>：该模式下，GPU 完全用于计算，不能作为本地显示输出。</p></li><li><p><code>WDDM</code>：该模式下，GPU 既用于计算又用于本地显示输出。</p></li></ul></li><li><p><strong>Fan</strong>：风扇转速，从0到100%之间变动； 有的设备不会返回转速，因为它不依赖风扇冷却而是通过其他外设保持低温。 N/A 代表 Not Available。</p></li><li><p><strong>Temp</strong>：温度，单位是摄氏度； </p></li><li><p><strong>Perf</strong>：性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能。</p></li><li><p><strong>Pwr:Usage/Cap</strong>：能耗； </p></li></ul><hr><ul><li><p><strong>Bus-Id</strong>：涉及GPU总线的东西，domain: bus :device.function； </p></li><li><p><strong>Disp.A</strong>：Display Active，表示GPU的显示是否初始化； </p></li><li><p><strong>Memory Usage</strong>：显存使用率； </p></li></ul><hr><ul><li><strong>Volatile  Uncorr. ECC</strong>：Error Correcting Code，错误检查与纠正； </li><li><strong>GPU-Util</strong>：GPU利用率； </li><li><strong>Compute M</strong>：compute mode，计算模式。</li></ul><hr><ul><li><p><strong>Processes</strong>：表示每个进程对 GPU 的显存使用率。</p><p>关于为什么上面的Processes中GPU Memory Usage会显示为<code>N/A</code>（Not Available），原因在于当前GPU处于<code>WDDW</code>下，NVIDIA Driver没有权限访问这个内容，所以显示<code>N/A</code>，但是不必慌张，此时GPU依然被使用了，只是不能显示使用率罢了。</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmruai8y9nj20ke02s3za.jpg"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.cnblogs.com/cute/p/13426515.html">Windows下查看GPU(NVIDIA)使用情况</a></li><li><a href="https://huangfei.blog.csdn.net/article/details/79230446">GPU状态监测 nvidia-smi 命令详解</a></li><li><a href="https://www.programmersought.com/article/45543524705/">About nvidia-smi display GPU Memory Usage is N / A</a></li><li><a href="https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf">nvidia-smi.txt——NVIDIA的官方文档</a></li><li><a href="https://medium.com/analytics-vidhya/explained-output-of-nvidia-smi-utility-fc4fbee3b124">Explained Output of nvidia-smi Utility——超详细讲解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> GPU </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/08/08/hello-world/"/>
      <url>/2021/08/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
      <link href="/2021/07/11/simcse-simple-contrastive-learning-of-sentence-embeddings/"/>
      <url>/2021/07/11/simcse-simple-contrastive-learning-of-sentence-embeddings/</url>
      
        <content type="html"><![CDATA[<h1 id="SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings"><a href="#SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings" class="headerlink" title="SimCSE: Simple Contrastive Learning of Sentence Embeddings"></a>SimCSE: Simple Contrastive Learning of Sentence Embeddings</h1><h2 id="1-Background-要解决的问题"><a href="#1-Background-要解决的问题" class="headerlink" title="1. Background(要解决的问题)"></a>1. Background(要解决的问题)</h2><p>得到更好的Sentence Embedding</p><h2 id="2-Contributions-创新点"><a href="#2-Contributions-创新点" class="headerlink" title="2. Contributions(创新点)"></a>2. Contributions(创新点)</h2><p>论文的出发点是设计一个模型，它能够学习到更好的句子表征（Sentence Embedding）。为了达到这一目标，论文采用对比学习的框架训练模型。</p><p>在构建对比学习任务时，为了构造对比学习任务的数据集（即正样本对和负样本对），论文采用了两种方式：</p><ol><li><p><strong>通过无监督方式构造数据集。</strong>论文认为在<em>无标签数据集</em>中，将同一个sentence进行dropout操作两次，相当于是对这个sentence进行了数据增强，两次dropout的结果可以作为一个正样本对，而负样本对则是其他的句子；</p><blockquote><ul><li>这种方式效果很好，能够与前人的有监督方式的效果持平；</li><li>如果移除Dropout操作将会导致representation collapse；</li></ul></blockquote></li><li><p><strong>通过有监督方式构造数据集。</strong>论文认为<em>NLI数据集</em>中，premise和entailed hypothesis可以作为正样本，而premise和contradiction hypothesis可以作为难负（hard negative）样本，其他的句子作为一般的负样本；</p></li></ol><p>具体的构造方式如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gscutjy8urj21d80kd0xp.jpg"></p><p>通过上面的对比学习策略，论文的实验结果刷新了Semantic Textual Similarity的SoTA。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><h3 id="Unsupervised-SimCSE"><a href="#Unsupervised-SimCSE" class="headerlink" title="Unsupervised SimCSE"></a>Unsupervised SimCSE</h3><p>由于Unsupervised SimCSE采用的数据集是无监督数据集，为了构造$(x_i,x_i^{+})$对，需要使用数据增强。在这里，论文巧妙地将同一个句子$x_i$输入到$BERT$两次，由于$BERT$里面有Dropout操作，导致了两次$x_i$的输出结果是不同的$h_i$，而这两个不同的$h_i$则代表了一个positive pair。而Dropout操作则承担了数据增强的作用。因此，Unsupervised SimCSE的损失函数定义如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd0payamqj20yw0753z5.jpg"></p><blockquote><ul><li>其中$z_i$代表一种dropout mask，$z_{i}^{\prime}$代表另外一种dropout mask。</li><li><strong>需要理解的是，虽然dropout rate=0.1，但是每次执行dropout时，Transformer选择的dropout mask都是随机的，是不一样的~</strong></li></ul></blockquote><h4 id="Dropout-noise-as-data-augmentation"><a href="#Dropout-noise-as-data-augmentation" class="headerlink" title="Dropout noise as data augmentation"></a>Dropout noise as data augmentation</h4><p>论文将Dropout操作看作是最小形式的数据增强方式，并且与一些常见的数据增强方法和一些其他的训练目标在STS-B development set上进行了性能对比：</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizly1gsd19gjevnj60jm0m6n0g02.jpg"></p><p>传统的数据增强的方法有：</p><ol><li>Crop：裁剪。就是将句子裁剪出一段连续的部分；</li><li>Word deletion：随机地删除句子中的某些单词；</li><li>MLM：用[mask]来代替句子中的某些单词；</li><li>w/o dropout：在BERT中不使用Dropout操作（等价于将Dropout Rate设置成0？）</li></ol><p>通过上面的结果可以发现，没有一种数据增强方法的性能能够超越论文提出的Dropout操作，并且仅仅只是Delete one word都会极大地降低模型性能。</p><p>同时，论文也对比了不同的训练目标带来的结果差异，对比的双方是本文提出的self-prediction training objective和ICLR一篇论文提出的next sentence objective：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd1mdda6gj20o00jin0m.jpg"></p><p>可以通过上表发现，Unsupervised SimCSE的结果是优于Next-Sentence Objective的，并且使用dual encoder不如使用single encoder。</p><h4 id="Why-does-it-work？"><a href="#Why-does-it-work？" class="headerlink" title="Why does it work？"></a>Why does it work？</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd1teeoynj20nj0e5wgb.jpg"></p><p>为了进一步探讨dropout noise带来的影响，论文对不同的dropout rate进行了对比，发现原Transformer的0.1才是最好的。比较能够说明问题的是：当$p=0$或者$Fixed \  0.1$时，得到的$x_i$和$x_i^{+}$的Embedding是相同的，而这两种情况是表中最差的结果。再计算它们在训练过程中的alignment和uniformity，可以得到下面的图：</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizly1gsd1zv2sphj60j10j00y302.jpg"></p><ul><li>对比$No \ dropout$，$Fixed \ 0.1$，<strong>Unsup. SimCSE能够在提升uniformity的同时，保持alignment。</strong></li><li>对比Delete one word，<strong>SimCSE能够达到alignment和uniformity上的overall best；</strong></li></ul><h3 id="Supervised-SimCSE"><a href="#Supervised-SimCSE" class="headerlink" title="Supervised SimCSE"></a>Supervised SimCSE</h3><h4 id="Exploiting-supervised-data"><a href="#Exploiting-supervised-data" class="headerlink" title="Exploiting supervised data"></a>Exploiting supervised data</h4><p>为了找到合适的supervised dataset，论文对比了以下4种数据集：</p><ol><li>QQP</li><li>Flickr30k</li><li>ParaNMT</li><li>SNLI+MNLI</li></ol><p>它们的结果如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd2mpv2nsj20hx0m2jux.jpg"></p><p>如上图所示，SNLI+MNLI的结果是最好的。因此，论文最终也是选用该数据训练Sentence Encoder。</p><h4 id="Contraction-as-hard-negatives"><a href="#Contraction-as-hard-negatives" class="headerlink" title="Contraction as hard negatives"></a>Contraction as hard negatives</h4><p>论文在传统的$(x_i,x_{i}^{+})$基础上，又将NLI数据集中的<strong>contradiction hypotheses</strong>引入作为hard negative（未使用neural hypotheses作为难负样本），得到$(x_i,x_{i}^{+},x_i^{-})$。此时，损失函数被定义为：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd337gyj1j20n305eq38.jpg"></p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>衡量对比学习表征能力有两个重要的评价指标：</p><ol><li><strong>alignment</strong> between semantically-related positive pairs；</li><li><strong>uniformity</strong> of the whole representation space；</li></ol><p>关于这两个指标的意义可以参考论文——<a href="http://proceedings.mlr.press/v119/wang20k/wang20k.pdf">Understanding contrastive representation learning through alignment and uniformity on the hypersphere.</a></p><h3 id="Evaluation-Setup"><a href="#Evaluation-Setup" class="headerlink" title="Evaluation Setup"></a>Evaluation Setup</h3><p>在实验方面，采用了7个standard semantic textual similarity（STS） tasks和7个transfer learning tasks作为评测任务。</p><h4 id="Semantic-textual-similarity-tasks"><a href="#Semantic-textual-similarity-tasks" class="headerlink" title="Semantic textual similarity tasks"></a>Semantic textual similarity tasks</h4><p>7个STS tasks分别是：STS 2012-2016，STS Benchmark和SICK-Relatedness，需要计算sentence embedding之间的cosine similarity。</p><h4 id="Transfer-tasks"><a href="#Transfer-tasks" class="headerlink" title="Transfer tasks"></a>Transfer tasks</h4><p>同时也在7个transfer learning tasks上进行了实验：</p><ol><li>MR</li><li>CR</li><li>SUBJ</li><li>MPQA</li><li>SST-2</li><li>TREC</li><li>MRPC</li></ol><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><h4 id="Semantic-textual-similarity"><a href="#Semantic-textual-similarity" class="headerlink" title="Semantic textual similarity"></a>Semantic textual similarity</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd4b5eq2sj20ug0mz457.jpg"></p><p>由上图可见，在STS任务上，无论是无监督还是有监督，SimCSE都达到了SoTA的水平。</p><h4 id="Transfer-tasks-1"><a href="#Transfer-tasks-1" class="headerlink" title="Transfer tasks"></a>Transfer tasks</h4><p><img src="http://ww1.sinaimg.cn/large/002QqMizly1gsd4u6qeedj60ue0nj0z602.jpg"></p><p>从上面的结果可以发现，supervised SimCSE的结果与之前方法的结果相同或者更优，而unsupervised SimCSE的结果就并不是那么好，但是引入了MLM任务之后，性能明显提升了许多。原因在于：<strong>无论是训练unsupervised还是supervised SimCSE，它们的目标都是similarity，并没有体现transfer任务的特性，所以在进行transfer任务的评估时，会不尽人意。</strong></p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>论文对different batch sizes，pooling methods和MLM auxiliary objectives进行消融实验，验证它们对模型的影响。</p><h4 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd57yk5stj20hp05ydgh.jpg"></p><p>可以发现：当Batch Size（也就是前面公式5中的$N$）增加到512之后，增大batch size会损害模型性能。</p><h4 id="Pooling-methods"><a href="#Pooling-methods" class="headerlink" title="Pooling methods"></a>Pooling methods</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd5bvkd33j20hh0ci3zw.jpg"></p><p>可以发现，Pooling Methods对SimCSE没有影响。</p><h4 id="MLM-auxiliary-task"><a href="#MLM-auxiliary-task" class="headerlink" title="MLM auxiliary task"></a>MLM auxiliary task</h4><p>通过表9可以发现，MLM对STS-B任务是有损害作用的，但是对Transfer Task是有益的。</p><h2 id="5-Analysis"><a href="#5-Analysis" class="headerlink" title="5. Analysis"></a>5. Analysis</h2><h3 id="Uniformity-and-alignment"><a href="#Uniformity-and-alignment" class="headerlink" title="Uniformity and alignment"></a>Uniformity and alignment</h3><p><img src="http://ww1.sinaimg.cn/large/002QqMizly1gsd8zoicu2j60no0lyjy302.jpg"></p><p>通过上图可以发现，Uniformity和Alignment表现好的模型，在average STS上也表现得不错。此外，我们还可以发现：</p><ol><li>尽管BERT能够取得不错的alignment，但是它的uniformity并不好；</li><li>post-processing methods（比如BERT-flow和BERT-whitening）虽然能够取得不错的uniformity，但是不能有很好地alignment；</li><li>Unsupervised SimCSE 能够同时保持良好的alignment和uniformity；</li><li>引入监督信息之后，SimCSE能够得到更好的alignment</li></ol><h3 id="Cosine-similarity-distribution"><a href="#Cosine-similarity-distribution" class="headerlink" title="Cosine-similarity distribution"></a>Cosine-similarity distribution</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsd9cgvkl5j21080ko7ao.jpg"></p><p>我们发现SimCSE的分布通常比BERT或SBERT更分散，但与白化分布相比，在语义相似的句子对上保持了较低的方差。这一观察结果进一步验证了SimCSE可以达到更好的alignment和uniformity。</p><h3 id="Qualitative-comparison"><a href="#Qualitative-comparison" class="headerlink" title="Qualitative comparison"></a>Qualitative comparison</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gsda00xbfzj21bf0ge782.jpg"></p><p>对$SBERT_{base}$和$Supervised \ SimSCE-BERT_{base}$进行召回实验，发现后者召回的结果要优于前者，这更加说明了后者的Sentence Embedding的强大之处。</p><h2 id="6-Thinking"><a href="#6-Thinking" class="headerlink" title="6. Thinking"></a>6. Thinking</h2><p>这篇论文是上半年非常火的一篇论文，之前由于觉得跟自己研究的事件抽取课题不怎么相关，所以一直没有了解。但是，对比学习的确是一个比较大的趋势，在今年ACL上，刘知远老师组就利用对比学习框架，提出了实体关系抽取和事件抽取两个任务的预训练语言模型：</p><ol><li><a href="https://arxiv.org/pdf/2012.15022.pdf">ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning</a></li><li><a href="https://arxiv.org/pdf/2105.14485.pdf">CLEVE: Contrastive Pre-training for Event Extraction</a></li></ol><p>对比学习能够利用无监督数据训练得到一个更强大的task specific特征提取器，这对于小样本学习也是很有意义的。</p><p>除此之外，这篇论文的写作也是十分优雅，逻辑严密，而且实验做得非常完备，实验参数细节都有详细写出。</p><h3 id="有价值的参考文献"><a href="#有价值的参考文献" class="headerlink" title="有价值的参考文献"></a>有价值的参考文献</h3><ol><li>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks.</li><li>On the sentence embeddings from pre-trained language models.</li><li>Whitening sentence representations for better semantics and faster retrieval.</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对比学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Event Time Extraction and Propagation via Graph Attention Networks</title>
      <link href="/2021/06/12/event-time-extraction-and-propagation-via-graph-attention-networks/"/>
      <url>/2021/06/12/event-time-extraction-and-propagation-via-graph-attention-networks/</url>
      
        <content type="html"><![CDATA[<h1 id="Event-Time-Extraction-and-Propagation-via-Graph-Attention-Networks"><a href="#Event-Time-Extraction-and-Propagation-via-Graph-Attention-Networks" class="headerlink" title="Event Time Extraction and Propagation via Graph Attention Networks"></a>Event Time Extraction and Propagation via Graph Attention Networks</h1><blockquote><ul><li>作者：</li></ul></blockquote><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><p>当前的事件时间抽取（<strong>temporal event grounding</strong>，识别出事件的start time和end time）存在的问题</p><ol><li>文本中<strong>关于时间描述的模糊性</strong>，例如2010-Week-01这种；</li><li>相互关联的事件之间存在着信息的传递；</li></ol><h2 id="Contributions-创新点"><a href="#Contributions-创新点" class="headerlink" title="Contributions(创新点)"></a>Contributions(创新点)</h2><ol><li><p>为了解决模糊的时间区间问题（例如2010-Week-01），文章引入了4元组时间表示方法——<code>(earliest possible start date, latest possible start date, earliest possible end date , latest possible end date)</code>这里的日期是天级别的。</p><p>（这个思想来自于TAC-KBP temporal slot filling task）；</p></li><li><p>为了实现相互关联的事件之间的信息传递，对于一个document，构建event graph，event之间的边通过共享的entity arguments和temporal relation得来。最后，在event graph之上执行GAT，就可以实现事件之间的信息传递了~</p></li><li><p>基于ACE 2005，<strong>重新构建了一个用于event time extraction的数据集</strong>。</p></li></ol><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><h3 id="4-tuple-Event-Time-Representation"><a href="#4-tuple-Event-Time-Representation" class="headerlink" title="4-tuple Event Time Representation"></a>4-tuple Event Time Representation</h3><p>event time extraction的目标是就是确定事件的开始时间和结束时间，但是在ACE 2005数据集中，大多数事件都没有明确的start time和end time。为了解决这种不确定性，采用TAC-KBP2011 temporal slot filling task的方式，采用四元组表示事件$e$的时间→$&lt;\tau^{-}_{start},\tau^{+}_{start},\tau^{-}_{end},\tau^{+}_{end}&gt;$，它们分别代表了<code>(earliest possible start date, latest possible start date, earliest possible end date , latest possible end date)</code>，并且它们满足下面的关系：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1grfvp7s89sj20hq03zt8r.jpg"></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><h3 id="有价值的参考文献"><a href="#有价值的参考文献" class="headerlink" title="有价值的参考文献"></a>有价值的参考文献</h3><h3 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h3><h3 id="不懂的地方"><a href="#不懂的地方" class="headerlink" title="不懂的地方"></a>不懂的地方</h3><h3 id="可以借鉴的地方"><a href="#可以借鉴的地方" class="headerlink" title="可以借鉴的地方"></a>可以借鉴的地方</h3>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 事件抽取 </tag>
            
            <tag> 篇章级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实体关系抽取新范式！TPLinker：单阶段联合抽取，并解决暴漏偏差～</title>
      <link href="/2021/05/28/shi-ti-guan-xi-chou-qu-xin-fan-shi-tplinker-dan-jie-duan-lian-he-chou-qu-bing-jie-jue-bao-lou-pian-chai/"/>
      <url>/2021/05/28/shi-ti-guan-xi-chou-qu-xin-fan-shi-tplinker-dan-jie-duan-lian-he-chou-qu-bing-jie-jue-bao-lou-pian-chai/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>划重点</strong>：TPLinker是实体关系抽取的新范式，巧妙设计了统一的联合抽取标注框架，可实现单阶段联合抽取、并解决暴露偏差，同时依旧可以解决复杂的重叠关系抽取。</p></blockquote><p><strong>实体关系抽取</strong>是NLP社区备受关注的子任务之一，热度很高，在2020年SOTA就更换了好几次。本文主要针对实体关系的「<strong>联合抽取</strong>」进行介绍；Pipeline方式不再赘述，可到知乎搜索JayJay的《<strong>nlp中的实体关系抽取方法总结</strong>》进行查阅。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>联合抽取主要分为2种范式：</p><ul><li><strong>多任务学习</strong>：即实体和关系任务共享同一个编码器，但通常会依赖先后的抽取顺序：关系判别通常需要依赖实体抽取结果。这种方式会存在暴漏偏差，会导致误差积累。</li><li><strong>结构化预测</strong>：即统一为全局优化问题进行联合解码，<strong>只需要一个阶段解码</strong>（损失函数和解码是不一样的概念），解决暴漏偏差。</li></ul><blockquote><p><strong>暴漏偏差</strong>：指在训练阶段时gold实体输入进行关系预测，而在推断阶段是上一步的预测实体输入进行关系判断；导致训练和推断存在不一致的问题。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqxx6o4lujj61re0z0wkv02.jpg"></p><blockquote><p>可以发现，在思维导图中，多任务学习会出现<strong>不同的抽取顺序</strong>的问题。</p></blockquote><p>JayJay这里也将最近几年来主要的 <strong>【实体关系联合抽取】</strong> 方法做了一个总结，如上图所示。</p><p>从上面的分析我们可以看出：结构化预测方式可以解决暴漏偏差问题；基于此，本文就介绍一篇来自COLING20的论文《<em><strong>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</strong></em>》，其主要亮点为：</p><ul><li>TPLinker将抽取标注框架统一为<strong>字符对链接问题</strong>，即<strong>T</strong>oken <strong>P</strong>air <strong>Link</strong>ing problem；</li><li>TPLinker<strong>既可以解决重叠关系问题、同时也可解决暴漏偏差问题！</strong></li><li>TPLinker是<strong>单阶段解码</strong>，训练和推断阶段抽取三元组不存在差异。</li></ul><p><em>论文地址</em>：<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2010.13415">https://arxiv.org/pdf/2010.13415</a></p><p><em>代码地址</em>：<a href="https://link.zhihu.com/?target=https://github.com/131250208/TPlinker-joint-extraction">https://github.com/131250208/TP</a></p><p>本文组织结构如下：</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqxxgth0g4j60ia08p3yt02.jpg"></p><h2 id="1、TPLinker要解决什么问题？"><a href="#1、TPLinker要解决什么问题？" class="headerlink" title="1、TPLinker要解决什么问题？"></a>1、TPLinker要解决什么问题？</h2><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqxxikgp5uj60fe031dgj02.jpg"></p><p>基于<strong>结构化预测</strong>的联合抽取方法，最早出现在17年论文《Joint extraction of entities and relations based on a novel tagging scheme》中，这篇论文用一个统一的序列标注框架抽取实体关系，如上图所示：直接以关系标签进行BIOES标注，subject实体序号为1，object实体序号为2。</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqxxje2asgj616c0f8q5302.jpg"></p><p>不过，以关系标签进行BIOES标注这种方式不能解决<strong>关系重叠</strong>问题，如上图所示：</p><ol><li><strong>SEO</strong>：SingleEntityOverlap，一个实体出现在多个关系三元组中；</li><li><strong>EPO</strong>：EntityPairOverlap，一个实体pair有多种关系；</li></ol><p>也就是说：<strong>结构化预测不能在解决暴漏偏差的同时，却不能cover关系重叠问题</strong>。因此，TPLinker要同时能够解决这两个问题。</p><h2 id="2、TPLinker的标注框架"><a href="#2、TPLinker的标注框架" class="headerlink" title="2、TPLinker的标注框架"></a>2、TPLinker的标注框架</h2><p>TPLinker整体标注Tag框架是基于token pair进行的，其本质上就是一个span矩阵。JayJay自己叫这种标注方式是多头标注（以区别于传统的序列标注和指针标注）。</p><p>关系数据集中一般会存在<strong>嵌套NER</strong>问题，因此需要首先解决这一问题。</p><p>这里给出一个例子：span「呼吸中枢受累」中，存在两个实体嵌套：「症状：呼吸中枢受累」和「部位：呼吸中枢」。</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqy70lnv88j60k00oqabp02.jpg"></p><p>要想解决这一问题，可以基于token pair构建一个$N \times N \times C$的Span矩阵($N$为序列长度，$C$为实体类别总数)，如上图所示，其中：</p><ul><li>Span{呼}{枢}=1，代表「呼吸中枢」是一个部位实体；</li><li>Span{呼}{累}=2，代表「呼吸中枢」是一个症状实体。</li><li>下三角矩阵为-1，是因为其不能构成一个实体、不具备真实意义；矩阵其余位置为0。</li></ul><p>通过上述分析，我们发现多头标注(token pair方式)可以很好解决嵌套实体问题。那么能不能用统一的标注方式，单阶段就能抽取出实体关系三元组呢？</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqy74daq59j60f90gugm602.jpg"></p><p>当然可以！TPLinker其实就是通过链接(linking)3种类型的Span矩阵来实现的，为方便起见，论文作者将3种标注方式画到一个图里了，如上图所示（记关系类别总数为$R$个）：</p><ol><li><strong>紫色标注</strong>：EH to ET，表示实体的头尾关系，是1个$N \times N$矩阵；如两个实体：New York City:M(New, City) =1; De Blasio:M(De, Blasio) =1。</li><li><strong>红色标注</strong>：SH to OH，表示subject和object的头部token间的关系，是R个$N \times N$矩阵；如三元组(New York City, mayor,De Blasio):M(New, De)=1。</li><li><strong>蓝色标注</strong>：ST to OT，表示subject和object的尾部token间的关系，是R个$N \times N$矩阵；如三元组(New York City, mayor,De Blasio):M(City, Blasio)=1。</li></ol><p>因此，可以得到TPLinker共有$2R+1$个矩阵。值得注意的是：为防止稀疏计算，下三角矩阵不参与计算；虽然实体标注不会存在于下三角矩阵种，但关系标注是会存在的。</p><p>为此，论文采取转换操作是：如果关系存在于下三角，则将其转置到上三角，并有“标记1”变为“标记2”。</p><h2 id="3、TPLinker的解码过程"><a href="#3、TPLinker的解码过程" class="headerlink" title="3、TPLinker的解码过程"></a>3、TPLinker的解码过程</h2><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqy79g9r4aj60k00c00u002.jpg"></p><p>上图给出了一个完整的标注和编码示意图，标记有“0、1、2”三种。</p><p>编码部分实际上是将原始的Span矩阵会展开为一个$\frac {N \times (N + 1)} {2}$序列进行编码，也就是将token pair的每一个token编码拼接在一起。</p><p>TPLinker的解码过程为：</p><ol><li>解码EH-to-ET可以得到句子中所有的实体，用实体头token idx作为key，实体作为value，存入字典D中；</li><li>对每种关系r，解码ST-to-OT得到token对存入集合E中，解码SH-to-OH得到token对并在D中关联其token idx的实体value；</li><li>对上一步中得到的SH-to-OH token对的所有实体value对，在集合E中依次查询是否其尾token对在E中，进而可以得到三元组信息。</li></ol><p> 结合上图的具体case,我们具体描述一下解码过程：</p><p>解码EH-to-ET中得到3个实体：{New York,New York City,De Blasio}; 字典D为：<code>{New:(New York,New York City),De:(De Blasio)}</code></p><p>以关系“<strong>mayor</strong>”为例,</p><ol><li>解码ST-to-OT得到集合E：{(City,Blasio)};解码SH-to-OH得到{(New,De)}，其在字典D中可关联的subject实体集合为{New York,New York City};object集合{De Blasio};</li><li>遍历上述subject集合和object集合，并在集合E中查询尾token，发现只有一个实体三元组{New York City,mayor,De Blasio}</li></ol><p>以关系“<strong>born in</strong>”为例,</p><ol><li>解码ST-to-OT得到集合E：{(Blasio,York),(Blasio,City)};解码SH-to-OH得到{(De,New)}，其在字典D中可关联的subject实体集合为{De Blasio};object集合为{New York,New York City};</li><li>遍历上述subject集合和object集合，并在集合E中查询尾token，可得到2个实体三元组：{De Blasio,born in,New York}和{De Blasio,born in,New York City}</li></ol><p>由于关系live in与born in一样，所以我们最终可得到5个三元组：</p><p>(New York City, mayor, De Blasio), (De Blasio, born in, New York), (De Blasio, born in, New York City), (De Blasio, live in, New York), (De Blasio, live in, New York City)</p><p>其实，只要TPLinker的解码过程，对这篇论文就会有深刻的理解了！大家一定要多看哦～</p><h2 id="4、TPLinker表现如何？"><a href="#4、TPLinker表现如何？" class="headerlink" title="4、TPLinker表现如何？"></a>4、TPLinker表现如何？</h2><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqy7moplq2j60we0c70ur02.jpg"></p><p>如上图所示，TPLinker也超越了之前的一众SOTA。此外，TPLinker相较于之前的SOTA——CasRel，推断速度也快3.6倍。</p><h2 id="5、总结与展望"><a href="#5、总结与展望" class="headerlink" title="5、总结与展望"></a>5、总结与展望</h2><p>本文介绍的TPLinker是实体关系抽取的新范式，<strong>巧妙设计了统一的联合抽取标注框架，可实现单阶段联合抽取、并解决暴漏偏差，同时依旧可以解决复杂的重叠关系抽取</strong>。下一步，可以将这套标注框架应用到实体和事件抽取中尝试哦。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</title>
      <link href="/2021/05/28/tplinker-single-stage-joint-extraction-of-entities-and-relations-through-token-pair-linking/"/>
      <url>/2021/05/28/tplinker-single-stage-joint-extraction-of-entities-and-relations-through-token-pair-linking/</url>
      
        <content type="html"><![CDATA[<h1 id="TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking"><a href="#TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking" class="headerlink" title="TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking"></a>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</h1><pre><code class="BibTeX">@article{zhang2020unsupervised,  title={Unsupervised Label-aware Event Trigger and Argument Classification},  author={Zhang, Hongming and Wang, Haoyu and Roth, Dan},  journal={arXiv preprint arXiv:2012.15243},  year={2020}}</code></pre><blockquote><ul><li>作者：</li></ul></blockquote><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><ol><li><p><strong>重叠关系</strong>（overlapping relation）。<strong>关系与关系之间可能共享1个或者2个实体</strong>，这种现象称为重叠关系（overlapping relation）问题。目前，有些实体关系抽取方法可以有效地解决这个问题，例如上上周学的多头标注的机制。例如下面的例子：</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqy8p8k1joj60j306lq3x02.jpg"></p><ul><li><strong>Normal</strong>：两个关系之间无重叠的情况；</li><li><strong>SEO</strong>（Single Entity Overlap）：两个关系之间重叠了一个实体，例如上图中的两个关系共享了<code>Atlanta</code>这个实体；</li><li><strong>EPO</strong>（Entity Pair Overlap）：两个关系的实体完全重合，例如上图中的<code>California</code>和<code>Sacramento</code>；</li></ul></li><li><p><strong>暴露偏差</strong>（exposed bias）。实体关系抽取的解决范式包括Pipeline和Joint两种，上上周阅读了两篇Joint的论文，它们的本质是<strong>多任务学习</strong>，1个任务是实体抽取，1个任务是给定gold实体的情况下，进行实体对的关系预测。这种方式存在着暴露偏差的问题。<strong>暴露偏差</strong>是指在<code>training stage</code>时gold实体输入进行关系预测，而在<code>inference stage</code>是上一步的预测实体输入进行关系判断；导致训练和推断存在不一致的问题;</p></li><li><p>目前为了解决SEO，EPO的问题，有两种方法：decoder-based and decomposition-based。但是它们却有暴露偏差的问题。</p></li></ol><h2 id="Contributions-创新点"><a href="#Contributions-创新点" class="headerlink" title="Contributions(创新点)"></a>Contributions(创新点)</h2><ol><li>提出了一个<strong>单阶段</strong>的实体关系联合抽取方法——TPLinker，该框架将实体关系联合抽取任务看作是<strong>T</strong>oken <strong>P</strong>air <strong>Link</strong>ing Problem，能够<strong>同时解决重叠关系和暴露偏差问题</strong>；</li></ol><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="TPLinker的标注框架"><a href="#TPLinker的标注框架" class="headerlink" title="TPLinker的标注框架"></a>TPLinker的标注框架</h3><p>TPLinker的标注框架采用的是<strong>多头标注</strong>，之前说过这种方式就是暴力的构造了一个$N \times N$的Span矩阵（$N$是句子的长度）。那么，为了完成单阶段的联合实体关系抽取，作者是按照如下方式进行的：</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqye77wxmnj60z00nkgpv02.jpg"></p><p>TPLinker中涉及到了三种矩阵：</p><ol><li>Entity Head to Entity Tail（EH-to-ET）</li><li>Subject Head to Object Head（SH-to-OH）</li><li>Subject Tail to Object Tail（ST-to-OT）</li></ol><p>作者为了表示方便，所以在Figure 2左图中将3个矩阵压缩到一起，并用不同的颜色进行区分：</p><ol><li><p><strong>紫色标记：Entity Head to Entity Tail（EH-to-ET）</strong>，表示构成一个实体的头尾。是一个$N\times N$的矩阵。</p><p>例如，<code>(New,City)=1</code>，表示New York City是一个实体；<code>(De,Blasio)=1</code>，表示De Blasio是一个实体。</p></li><li><p><strong>红色标记：Subject Head to Object Head（SH-to-OH）</strong>，表示构成一个关系的subject的起始token和object的起始token。由于关系抽取中涉及$R$中不同的关系，所以有$R$个这样的矩阵。</p><p>例如，<code>(New,De)=1</code>，表示以<code>New</code>开头的实体和以<code>De</code>开头的实体可能构成关系<code>mayor</code>；</p></li><li><p><strong>蓝色标记：Subject Tail to Object Tail（ST-to-OT）</strong>，表示构成一个关系的subject的终止token和object的终止token。由于关系抽取中涉及$R$中不同的关系，所以也有$R$个这样的矩阵。</p><p>例如，<code>(City,Blasio)=1</code>，表示以<code>City</code>结尾的实体和以<code>Blasio</code>结尾的实体可能构成关系<code>mayor</code>。</p></li></ol><p>因此，TPLinker模型涉及到了$2R+1$个$N \times N$的矩阵。<strong>为了避免稀疏运算，作者摒弃了每个矩阵的左下三角，再将矩阵flatten成单个序列（下图Figure 3中的橘色部分）</strong>：</p><ul><li>对于EH-to-ET的实体矩阵，它的左下三角可以直接摒弃，不会影响结果；</li><li>对于每个关系对应的SH-to-OH和ST-to-OT矩阵，它们的左下三角是不可以直接摒弃的，如Figure 2右图所示，对于关系<code>born in</code>，在文本中，subject De Blasio是出现在object New York/New York City之后的（灰色的1）。为了既能够压缩矩阵，又能够保留信息，作者将数字<code>2</code>用作反向关系，如Figure 2右图所示，于是<code>(New,De)=2</code>表示以<code>De</code>开头的实体和以<code>New</code>开头的实体可能构成关系<code>born in</code>；</li></ul><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqyf6o0cn4j60xx0ns78u02.jpg"></p><p>于是，之前的$2R+1$个$N \times N$矩阵标注问题，转换成了$2R+1$个$\frac {N^2+N} 2$序列标注问题。</p><h3 id="TPLinker的解码方法"><a href="#TPLinker的解码方法" class="headerlink" title="TPLinker的解码方法"></a>TPLinker的解码方法</h3><p>在上一小节中讲解了标注框架，得到了EH-to-ET、SH-to-OH、ST-to-OT三种不同的标注序列之后，如何将它们组合到一起，得到最后的关系三元组呢？</p><p><strong>其实解码的方式非常简单，说白了就是三个标注序列能对上就行！</strong></p><p>以关系“<code>mayor</code>”为例：</p><ol><li>通过EH-to-ET得到实体集合$E$：<code>{New York, New York City, De Blasio}</code>；</li><li>通过ST-to-OT得到subject和object终止token对集合$T$：<code>{(City,Blasio)}</code>；</li><li>通过SH-to-OH得到subject和object起始token对$H$：<code>{(New,De)}</code>，根据$E$，遍历以<code>New</code>开头的实体和以<code>De</code>开头的实体的所有可能组合，看是否出现在$T$中，出现则说明是一个关系三元组。</li></ol><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><h4 id="Token-Pair的表示方式"><a href="#Token-Pair的表示方式" class="headerlink" title="Token Pair的表示方式"></a>Token Pair的表示方式</h4><p>很简单，公式如下：<br>$$<br>h_{i,j}=tanh(W_h,[h_i;h_j]+b_h)<br>$$<br>其中，$h_{i,j}$表示token pair$(i,j)$对应的特征向量。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqygk9vdkyj60zw06zgm702.jpg"></p><p>就是分类任务的交叉熵损失函数，公式倒是写得很高大上。。。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>采用NYT和WebNLG。这两个数据集在人工标注时，有两种方式：</p><ol><li>只标注每个实体的最后一个word；</li><li>标注整个实体span；</li></ol><p>在本文中，第一种方式用NYT$^\star$，WebNLG$^\star$表示。第二种方式用NYT，WebNLG表示。</p><p>数据集的划分如下：</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqz39jn20ej60of077q4n02.jpg"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqz4j6r9wfj614a0ipdl102.jpg"></p><p>如上图所示，TPLinker达到了SOTA的水平。</p><p>此外作者对TPLinker在testset上不同类型的句子上的抽取能力进行了分析，句子的类型包括：</p><ol><li>出现Normal的句子</li><li>出现SEO的句子</li><li>出现EPO的句子</li><li>出现1个、2个、3个、4个、5个及5个以上个关系的句子</li></ol><p>结果如下表所示：</p><p><img src="http://ww1.sinaimg.cn/large/002QqMizgy1gqz4q3klcjj60od08xmzb02.jpg"></p><p>通过上表可以发现，TPLinker的确能够在<strong>避免暴露偏差</strong>的情况下，有效地解决了<strong>重叠关系</strong>问题，这也达到了论文最初的motivation！</p><h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><h3 id="有价值的参考文献"><a href="#有价值的参考文献" class="headerlink" title="有价值的参考文献"></a>有价值的参考文献</h3><ol><li>A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</li><li>Joint Entity and Relation Extraction with Set Prediction Networks</li></ol><h3 id="不懂的地方"><a href="#不懂的地方" class="headerlink" title="不懂的地方"></a>不懂的地方</h3><ol><li>NYT、WebNLG数据集的组成</li><li>代码部分</li></ol><h3 id="可以借鉴的地方"><a href="#可以借鉴的地方" class="headerlink" title="可以借鉴的地方"></a>可以借鉴的地方</h3><ol><li>本文最大的亮点就是解决了<strong>传统联合实体关系抽取方法的暴漏偏差问题</strong>。能够很好地解决训练和预测时的误差。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition</title>
      <link href="/2021/05/23/empirical-analysis-of-unlabeled-entity-problem-in-named-entity-recognition/"/>
      <url>/2021/05/23/empirical-analysis-of-unlabeled-entity-problem-in-named-entity-recognition/</url>
      
        <content type="html"><![CDATA[<h1 id="Empirical-Analysis-of-Unlabeled-Entity-Problem-in-Named-Entity-Recognition"><a href="#Empirical-Analysis-of-Unlabeled-Entity-Problem-in-Named-Entity-Recognition" class="headerlink" title="Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition"></a>Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition</h1><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><ol><li>在NER任务中，存在着<strong>未标注实体问题（unlabeled entity problem）</strong>——数据集中存在被标注工作人员漏标的实体。一般模型会把这类实体当作“负样本”，这会严重误导模型的训练。</li></ol><h2 id="Contributions-贡献"><a href="#Contributions-贡献" class="headerlink" title="Contributions(贡献)"></a>Contributions(贡献)</h2><ol><li>通过<strong>实验详细地分析了未标注实体问题对NER任务的影响</strong>；</li><li>提出采用<strong>负采样策略</strong>，减小未标注实体对NER任务的影响。实验表明，未标注实体问题得到极大改善，超过一众SOTA方法。</li></ol><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>论文的方法部分很简单，总共分为两步：</p><ol><li>标注框架；</li><li>负采样训练；</li></ol><h3 id="标注框架"><a href="#标注框架" class="headerlink" title="标注框架"></a>标注框架</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqsj1o2y9aj20ih06x3z0.jpg"></p><p>本文采用的标注框架是上周学习到的<strong>片段排列</strong>标注框架，即对句子中存在的每个span $(i,j)$进行二分类，其计算公式如下：<br>$$<br>s_{i,j} = h_i \oplus h_j \oplus(h_i-h_j)\oplus(h_i\odot h_j)<br>$$</p><p>$$<br>o_{i,j} = Softmax({\bf U} tanh({\bf V}s_{i,j}))<br>$$</p><h3 id="负采样进行训练"><a href="#负采样进行训练" class="headerlink" title="负采样进行训练"></a>负采样进行训练</h3><p>论文具体的降噪方式很简单，就是<strong>对所有非实体片段进行负采样</strong>（下采样）。采样进行loss计算的非实体片段共有$\lambda n$个，$n$为序列长度，$\lambda=0.1,0.2,…,0.9$。</p><p>模型的损失函数如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqsj6m1gtdj20gy02adfu.jpg"></p><p>前者为标注损失，后者为负采样损失。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="未标注实体（实体漏标）现象带来的问题"><a href="#未标注实体（实体漏标）现象带来的问题" class="headerlink" title="未标注实体（实体漏标）现象带来的问题"></a>未标注实体（实体漏标）现象带来的问题</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqsi5e2w6lj20ui0lvdjs.jpg"></p><p>论文采用3种指标来衡量实体漏标带来的影响，分别是F1，侵蚀率$\alpha_p$，误导率$\beta_p$。如上图可见，随着Mask Probability（漏标的概率）的增加，F1下降，侵蚀率上升，误导率上升，模型的性能出现显著地下降。</p><h3 id="负采样效果"><a href="#负采样效果" class="headerlink" title="负采样效果"></a>负采样效果</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqsj8n0f94j20hv067my2.jpg"></p><p>从上图可以看出，随着漏标概率$p$的增加，传统BERT Tagging模型的性能会剧烈下降，而负采样方法则变化并不是很明显。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqsj9cgw51j20id09kmy0.jpg"></p><p>从上图可以看出，随着漏标概率$p$的增加，负采样方法可以使得误导率保持一个很低的水平。</p><p>此外，在实际的NER数据集上，基于负采样的模型也取得了SOTA：<img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqsj9y4c8kj20j707m3zz.jpg"></p><h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><ol><li>本文的方法很简单，基本就是上周那篇论文<a href="https://arxiv.org/pdf/1909.07755.pdf">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</a>中讲到的东西用到NER任务上罢了；</li><li>不过比较震惊的是，从文章的结果可以看出，引入负采样技术减弱漏标数据的影响却能够带来如此之大的性能提升！</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Named Entity Recognition as Dependency Parsing</title>
      <link href="/2021/05/23/named-entity-recognition-as-dependency-parsing/"/>
      <url>/2021/05/23/named-entity-recognition-as-dependency-parsing/</url>
      
        <content type="html"><![CDATA[<h1 id="Named-Entity-Recognition-as-Dependency-Parsing"><a href="#Named-Entity-Recognition-as-Dependency-Parsing" class="headerlink" title="Named Entity Recognition as Dependency Parsing"></a>Named Entity Recognition as Dependency Parsing</h1><h1 id="消融实验-双仿射分类器-ICLR-未标注实体问题"><a href="#消融实验-双仿射分类器-ICLR-未标注实体问题" class="headerlink" title="消融实验 双仿射分类器 ICLR 未标注实体问题"></a>消融实验 双仿射分类器 ICLR <strong>未标注实体问题</strong></h1><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><p>命名实体识别（NER）任务涉及到识别表示实体引用的文本范围。<strong>NER的研究通常只关注flat NER，忽略了实体引用之间的嵌套（nested）关系。</strong></p><h2 id="Contributions-创新点"><a href="#Contributions-创新点" class="headerlink" title="Contributions(创新点)"></a>Contributions(创新点)</h2><ol><li>将<strong>依存分析中的biffine机制</strong>引入NER，遍历整个句子，模型对每个span的起点和终点打分以得到结果，从而解决nested NER问题；</li><li>在8个数据集上都达到了SOTA水平；</li></ol><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>有了上周的学习的基础，本文的方法理解起来还是很简单的：</p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqs55grmulj20ay0bmaak.jpg"></p><p>嵌入层：</p><ol><li>采用$BERT_{large}$和fastText构造Word Embedding；</li><li>使用CNN构造Character Embedding；</li></ol><p>将1，2得到的特征进行拼接，输入到BiLSTM中，得到最终的Word Representation $x$。</p><p>论文再用了两个FFNN——<strong>FFNN_Start</strong>和<strong>FFNN_End</strong>处理$x$，分别得到具有span start特性的representation $h_s$和具有span end特性的representation $h_e$。最后，使用一个<strong>biaffine model</strong>生成一个$l \times l \times c$的得分向量$r_m$。</p><p>例如，下面是为某一个span $i$预测实体类型的过程：<br>$$<br>h_s(i) = FFNN_s(x_{s_i})<br>$$</p><p>$$<br>h_e(i) = FFNN_e(x_{e_i})<br>$$</p><p>$$<br>r_m(i) = h_s(i)^TU_mh_e(i) + W_m(h_s(i)\oplus h_e(i))+b_m<br>$$</p><p>其中$s_i$和$e_i$表示span $i$对应的开始和结束索引。$U_m$是一个$d \times c \times d$的向量，$W_m$是一个$2d \times c$的矩阵，$b_m$是偏置。</p><p>那么，span $i$对应的category $y \prime$就是：<br>$$<br>y \prime (i) = \arg \max r_m(i)<br>$$<br>文章的损失函数就是对于每一个valid span的multi-class交叉熵函数：<br>$$<br>p_m(i_c) = \frac {exp(r_m(i_c))} {\sum^{C}<em>{\hat c=1} exp(r_m(i</em>{\hat c}))}<br>$$</p><p>$$<br>loss = - \sum^N_{i=1}\sum^{C}<em>{c=1}y</em>{i_c}\log p_m(i_c)<br>$$</p><h3 id="预测解码"><a href="#预测解码" class="headerlink" title="预测解码"></a>预测解码</h3><p>首先对所有的$r_m(i_{y\prime})$按照分数的大小进行<strong>降序排列</strong>。</p><p><strong>对于nested NER解码</strong>，一个entity只要不破坏排在它前面的entity那么就会被解码为最终的entity。对于如何定义entity $i$破坏了entity $j$呢？其定义如下：<br>$$<br>s_i \lt s_j \le e_i \lt e_j \ or \ s_j \lt s_i \le e_j \lt e_i<br>$$<br>例如在<em>the Bank of China</em>中，实体<em>the Bank of</em>破坏了实体<em>Bank of China</em>。</p><p><strong>对于flat NER解码</strong>，也有着一个规则：任何包含前面的实体，或者被包含在前面的实体 的实体 都不会被选择。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>Nested Dataset：ACE 2004、ACE 2005、GENIA</p><p>Flast Dataset：CONLL 2002、CONLL 2003、ONTONOTES</p><p>结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqscuwajgej20ny0ntwka.jpg"></p><p>其次，作者对模型中的一些重要组件进行了消融实验，得到以下结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqscz48lmgj20et09amxv.jpg"></p><p>由上图可见，biaffine和BERT embedding对模型的性能影响更大。</p><h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><ol><li>这篇文章采用的方法还是上周说的粗暴的枚举所有的span区间，<strong>只不过是在多分类时，使用了biaffine而不是简单的FFNN</strong>；</li><li>文章在嵌入层的操作好复杂，又是BERT，又是fastText，又是BiLSTM。又是word embedding，又是character embedding。。。</li><li>而且文章标题中说<strong>as Dependency Parsing</strong>，但是论文中并没有提到依存分析，就<strong>只是用到了依存分析中的biaffine方法</strong>，感觉作者非常的标题党啊！</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Negative Sampling in Relation Extraction</title>
      <link href="/2021/05/16/negative-sampling-in-relation-extraction/"/>
      <url>/2021/05/16/negative-sampling-in-relation-extraction/</url>
      
        <content type="html"><![CDATA[<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/214127337">Negative Sampling负采样采的究竟是什么</a></li><li><a href="">Understanding Negative Sampling in Knowledge Graph Embedding</a></li><li><a href="http://deepdive.stanford.edu/generating_negative_examples">Generating negative evidence</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Span-based Joint Entity and Relation Extraction with Transformer Pre-training</title>
      <link href="/2021/05/15/span-based-joint-entity-and-relation-extraction-with-transformer-pre-training/"/>
      <url>/2021/05/15/span-based-joint-entity-and-relation-extraction-with-transformer-pre-training/</url>
      
        <content type="html"><![CDATA[<h1 id="Span-based-Joint-Entity-and-Relation-Extraction-with-Transformer-Pre-training"><a href="#Span-based-Joint-Entity-and-Relation-Extraction-with-Transformer-Pre-training" class="headerlink" title="Span-based Joint Entity and Relation Extraction with Transformer Pre-training"></a>Span-based Joint Entity and Relation Extraction with Transformer Pre-training</h1><blockquote><p>发表在ECAI 2020上的一篇文章。</p></blockquote><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><ol><li>传统的BIO/BILOU等序列标注方式不能很好地解决重叠实体的问题，例如实体<code>codeine</code>被实体<code>codeine intoxication</code>所包含；</li><li>进行关系抽取时，大多的方法没有应用到实体与实体之间相隔的局部上下文关系。</li></ol><h2 id="Contributions-创新点"><a href="#Contributions-创新点" class="headerlink" title="Contributions(创新点)"></a>Contributions(创新点)</h2><ol><li>抛弃了传统的BIO/BILOU标注实体的方式，构建了一个<strong>完全基于跨度的</strong>联合实体识别和关系抽取模型——SpBERT；</li><li>使用了<strong>负采样</strong>的方式增强模型；</li><li>通过max pooling的方式，提取了一个关系中<strong>实体对之间的文本特征</strong>；</li></ol><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="0-An-Example-of-the-Method-Overview"><a href="#0-An-Example-of-the-Method-Overview" class="headerlink" title="0. An Example of the Method(Overview)"></a>0. An Example of the Method(Overview)</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqjf1w42oyj214h0iv76i.jpg"></p><p>如上图所示，模型主要由<code>span classification</code>，<code>span filtering</code>，<code>relation classification</code>三个部分构成。其中<code>span classification</code>和<code>span filtering</code>用于实体的识别和分类，<code>relation classification</code>用于关系抽取。</p><h3 id="1-Span-Classification"><a href="#1-Span-Classification" class="headerlink" title="1. Span Classification"></a>1. Span Classification</h3><p>通过BERT之后，模型获得了句子的向量表示：$(c,e_1,e_2,…,e_n)$。之后模型将在任意的跨度（即span）内判断其是否为实体。例如，对于句子<code>(we,will,rock,you)</code>，将要检测<code>(we)</code>，<code>(we will)</code>，<code>(rock,you)</code>等等跨度是否为实体。在实现阶段，模型不会对所有的实体和关系进行beam search，而是通过<code>positive entity span + negative sampling</code>的方式构建训练集，具体来说是：使用所有标注为实体的跨度$S^{gt}$作为正样本，同时从不是实体的跨度中，只随机抽取其中的$N_e$个作为负样本。</p><p>如上图所示，Span Classifier的输入包含三个部分：<strong>跨度的向量表示</strong>（红色），<strong>宽度嵌入</strong>（蓝色），<strong>CLS的向量表示</strong>（绿色）。</p><p>其中，跨度的向量表示是通过max pooling操作得来的。宽度嵌入的含义是不同的跨度宽度，所表达的信息是不一样的，因此需要加入到特征中。CLS的向量表示用于提供语境上下文的信息。</p><p>最后，过一个线性分类层就可以得到跨度对应的实体类型了：<br>$$<br>{\bf {\hat y}} ^s = softmax(W^s \cdot {\bf x}^s + b)<br>$$</p><h3 id="2-Span-Filtering"><a href="#2-Span-Filtering" class="headerlink" title="2. Span Filtering"></a>2. Span Filtering</h3><p>这个步骤是衔接Span Classification和Relation Classification，功能很简单，就是剔除Span Classification中预测不是实体的跨度，将是实体的跨度输入到Relation Classification之中。</p><h3 id="3-Relation-Classification"><a href="#3-Relation-Classification" class="headerlink" title="3. Relation Classification"></a>3. Relation Classification</h3><p>在上一步中，我们得到了所有预测为实体的跨度，接下来就是预测它们之间的关系，与Span Classification相似，这里同样使用<code>positive relation + negative sampling</code>的方式，具体来说就是：使用所有标注为存在关系的entity pair作为正样本，其他不存在关系的entity pair中，我们随机采样$N_r$个作为负样本。</p><p>如上图所示，Relation Classification的输入包含三个部分：$Entity_i$的特征向量，$Entity_i$和$Entity_j$之间的local context的特征向量，以及$Entity_j$的特征向量。</p><p>最后将其输入到一个<strong>多标签的线性分类器</strong>（而不是多分类）中：<br>$$<br>{\bf \hat y_{1/2}}  := \sigma(W^r \cdot x^r_{1/2} + b^r)<br>$$<br>因为关系是非对称的，所以要分开识别$(Entity_i,Entity_j)$和$(Entity_j,Entity_i)$。</p><p>最后的损失函数就是常见的Joint损失函数：<br>$$<br>\mathcal L = \mathcal L^s + \mathcal L^r<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者在三个数据集：CoNLL04，SciERC，ADE上进行了实验，得到的结果都是SOTA：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqk1g319n8j20rf0jx0xf.jpg"></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>这篇文章的思路其实也很简单，但是结果却是很有效的。其改进了上篇文章中会存在的<strong>overlapping entity</strong>的问题。</p><p>同时，它的思想更像是一种非常暴力地枚举每个可能的span，然后一一进行判断，假设一个句子有$N$个token，那么理论上需要判断的span有$N \cdot (N+1) /2$，计算量应该会很大，好在其在训练时，提出了有效的negative sampling技术，保证了计算量不会太大。下图是我在知乎文章中看到的更加形象的描述图：</p><p><img src="https://pic3.zhimg.com/80/v2-3007d9b207341c4be77e09bba57a30aa_720w.jpg"></p><h3 id="后续阅读"><a href="#后续阅读" class="headerlink" title="后续阅读"></a>后续阅读</h3><ol><li><a href="https://www.aclweb.org/anthology/D19-1585.pdf">Entity, Relation, and Event Extraction with Contextualized Span Representations</a></li><li><a href="https://arxiv.org/abs/2012.05426">Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition</a></li></ol><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/326302618">刷爆3路榜单，信息抽取冠军方案分享：嵌套NER+关系抽取+实体标准化</a></li><li><a href="https://www.cnblogs.com/sandwichnlp/p/12049829.html">信息抽取——实体关系联合抽取</a></li><li><a href="https://zhuanlan.zhihu.com/p/348780181">知识图谱之nlp端到端实体关系抽取最强综述</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Joint entity recognition and relation extraction as a multi-head selection problem</title>
      <link href="/2021/05/14/joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem/"/>
      <url>/2021/05/14/joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem/</url>
      
        <content type="html"><![CDATA[<h1 id="Joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem"><a href="#Joint-entity-recognition-and-relation-extraction-as-a-multi-head-selection-problem" class="headerlink" title="Joint entity recognition and relation extraction as a multi-head selection problem"></a>Joint entity recognition and relation extraction as a multi-head selection problem</h1><blockquote><p>发表在Expert Systems with Applications上的一篇文章。</p></blockquote><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><ol><li>人工构造的特征或者是NLP tools得到的特征存在误差传播；</li><li>pipeline方法忽略了任务之间的相关性。</li></ol><h2 id="Contributions-创新点"><a href="#Contributions-创新点" class="headerlink" title="Contributions(创新点)"></a>Contributions(创新点)</h2><ol><li>模型不依赖于人工构造的特征或者是NLP tools得到的特征，实现了在不同的领域（金融，生物等）和不同的语言（英语，德语等）的泛化；</li><li>能同时进行实体和关系的抽取；</li><li><strong>定义实体关系联合抽取问题为multi-head selection problem</strong>，解决一个实体可能与多个实体都存在关系的问题；</li></ol><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="0-An-Example-of-the-Method-Overview"><a href="#0-An-Example-of-the-Method-Overview" class="headerlink" title="0. An Example of the Method(Overview)"></a>0. An Example of the Method(Overview)</h3><p>​           <img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqiaejd7jrj20zl0m1mzz.jpg"></p><p>如上图所示，该模型包含Embedding Layer，BiLSTM Layer，CRF Layer，Sigmoid Layer四个主要的层。下面对它们进行逐一地阐述：</p><h3 id="1-Embedding-Layer"><a href="#1-Embedding-Layer" class="headerlink" title="1. Embedding Layer"></a>1. Embedding Layer</h3><p>通常情况下，embedding都是直接用pre-trained好的word embedding。但是<strong>在Embedding Laye还使用BiLSTM引入character级别的embedding</strong>。作者认为character级别的embedding可以学习到prefix和suffix之类的词法特征，从而提供更多的信息（例如bedroom和restroom共享了后缀“room”）。示例如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqivhuqqw5j20h60da74r.jpg"></p><h3 id="2-BiLSTM-Layer"><a href="#2-BiLSTM-Layer" class="headerlink" title="2. BiLSTM Layer"></a>2. BiLSTM Layer</h3><p>就是传统的双向LSTM。</p><h3 id="3-CRF-Layer-for-Named-Entity-Recognition"><a href="#3-CRF-Layer-for-Named-Entity-Recognition" class="headerlink" title="3. CRF Layer for Named Entity Recognition"></a>3. CRF Layer for Named Entity Recognition</h3><p>也是传统的技术，在BiLSTM后面接CRF来更好地进行NER，论文中实体识别部分的标注方式用的是BIO。</p><h3 id="4-Sigmoid-Layer-for-Relation-Extraction"><a href="#4-Sigmoid-Layer-for-Relation-Extraction" class="headerlink" title="4. Sigmoid Layer for Relation Extraction"></a>4. Sigmoid Layer for Relation Extraction</h3><p><strong>该层采用multi-head selection作为关系抽取的解答范式。</strong>输入是Embedding Layer中得到的<code>word embedding</code> concatenate上对应的实体类型的<code>label embedding</code>。</p><p><strong>multi-head</strong>的意思是对于一个实体，它可能与句子中的多个实体之间存在关系。同时，考虑到一个实体包含多个token，作者规定使用每个实体的最后一个token来代表这个实体。最后一个token就称为head。</p><p>例如，上面的example图中，对于实体<code>John Smith</code>，存在着两个关系——<code>&lt;John Smith, works for, Disease Control Center&gt;</code>和<code>&lt;John Smith, lives in, Atlanta&gt;</code>。</p><p>假定当前需要判断$token_i$和$token_j$之间是否存在关系$r_k$，则计算公式如下：<br>$$<br>s(z_j, z_i, r_k) = V^{(r_k)}f(U^{(r_k)}z_j + W^{(r_k)}z_i + b^{(r_k)})<br>$$<br>其中，$z_j$和$z_i$是token的embedding，式子中的$f(\cdot)$是element-wise级别的激活函数（例如relu，tanh），$V^{(r_k)},U^{(r_k)},W^{(r_k)},b^{(r_k)}$都是根据关系$r_k$的不同而不同的参数。</p><p>将$s(z_j, z_i, r_k)$输入到一个Sigmoid层进行归一化，得到预测概率：<br>$$<br>Pr(head=w_j, label=r_k|w_i)=\sigma(s(z_i, z_j, r_k))<br>$$<br>从而可以进一步定义损失函数为：<br>$$<br>L_{rel} = \sum_{i=0}^{n}\sum_{j=0}^{m}-log\ Pr(head=y_{i,j}, relation=r_{i,j}|w_i)<br>$$<br>因此，结合NER和Relation Extraction两个任务，模型的整体损失为$L_{ner}+L_{rel}$。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>正如作者在introduction中说的，其提出的模型能够独立于数据集的领域和语言，所以其在4个不同的数据集上进行了实验以此来证明模型的泛化性：</p><ol><li>ACE 04</li><li>CoNLL 04</li><li>DREC</li><li>ADE</li></ol><p>如下图所示，论文在多个数据集上都达到了SOTA的成绩：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqiwsrh6wej212r0fg0xf.jpg"></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>其实这篇论文的思想说来也很简单：就是<strong>构建了一个$N \times N \times C$的分类矩阵</strong>。其中$N$表示的是一个句子中token的数量，$C$是pre-defined relation type的数量。此外，不同于以往对一个token pair$&lt;token_i,token_j&gt;$上的$C$个关系进行$softmax$操作，这里是对每个关系都进行单独的$Sigmoid$二元分类。由此可见，<strong>模型的训练量应该是非常大的而且还存在着很大的0-1标签稀疏的问题</strong>！</p><p>虽然这篇论文是用在关系抽取任务上的，但是它提出的多头机制（multi-head）也是可以用在NER之上的，例如下面是知乎文章上的一个用多头机制解决NER的示例：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gqixedz2xqj20aq0d9wen.jpg"></p><p>如上图所示，<code>Span[呼][枢]=1</code>代表呼吸中枢是一个部位实体；<code>Span[呼][累]=2</code>，代表呼吸中枢是一个症状实体；</p><h3 id="后续阅读"><a href="#后续阅读" class="headerlink" title="后续阅读"></a>后续阅读</h3><ul><li>该团队还在EMNLP2018上发表了一篇相似的工作<a href="https://arxiv.org/pdf/1808.06876.pdf">Adversarial training for multi-context joint entity and relation extraction</a>，可以继续阅读；</li><li>ACL20的<a href="https://arxiv.org/abs/2005.07150">《Named Entity Recognition as Dependency Parsing》</a>采取Biaffine机制构造Span矩阵</li><li>EMNLP20的<a href="https://www.aclweb.org/anthology/2020.emnlp-main.486.pdf">《HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interactio》</a>采取focal loss来解决0-1标签不平衡问题。</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/326302618">刷爆3路榜单，信息抽取冠军方案分享：嵌套NER+关系抽取+实体标准化</a></li><li><a href="https://www.cnblogs.com/sandwichnlp/p/12049829.html">信息抽取——实体关系联合抽取</a></li><li><a href="https://zhuanlan.zhihu.com/p/46507254">对抗训练多头选择的实体识别和关系抽取的联合模型</a></li><li><a href="https://zhuanlan.zhihu.com/p/147537898">序列标注方法BIO、BIOSE、IOB、BILOU、BMEWO、BMEWO+的异同</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Conditional Layer Normalization</title>
      <link href="/2021/05/03/conditional-layer-normalization/"/>
      <url>/2021/05/03/conditional-layer-normalization/</url>
      
        <content type="html"><![CDATA[<p>本文思路来源于苏建林大佬的<a href="https://kexue.fm/archives/7124">Conditional Layer Normalization</a>，本人整理消化后写的笔记。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Layer Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What the role is vs. What plays the role: Semi-supervised Event Argument Extraction via Dual Question Answering</title>
      <link href="/2021/05/02/what-the-role-is-vs-what-plays-the-role-semi-supervised-event-argument-extraction-via-dual-question-answering/"/>
      <url>/2021/05/02/what-the-role-is-vs-what-plays-the-role-semi-supervised-event-argument-extraction-via-dual-question-answering/</url>
      
        <content type="html"><![CDATA[<h1 id="论文名字"><a href="#论文名字" class="headerlink" title="论文名字"></a>论文名字</h1><pre><code class="BibTeX">@article{zhou2021role,  title={What the role is vs. What plays the role: Semi-supervised Event Argument Extraction via Dual Question Answering},  author={Zhou, Yang and Chen, Yubo and Zhao, Jun and Wu, Yin and Xu, Jiexin and Li, Jinlong},  year={2021}}</code></pre><blockquote><ul><li>作者：自动化所<a href="http://www.nlpr.ia.ac.cn/cip/yubochen/publications_cn.html">陈玉博</a>老师</li></ul></blockquote><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><h2 id="Contributions-创新点"><a href="#Contributions-创新点" class="headerlink" title="Contributions(创新点)"></a>Contributions(创新点)</h2><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><h3 id="有价值的参考文献"><a href="#有价值的参考文献" class="headerlink" title="有价值的参考文献"></a>有价值的参考文献</h3><h3 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h3><h3 id="不懂的地方"><a href="#不懂的地方" class="headerlink" title="不懂的地方"></a>不懂的地方</h3><h3 id="可以借鉴的地方"><a href="#可以借鉴的地方" class="headerlink" title="可以借鉴的地方"></a>可以借鉴的地方</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Reading the Manual: Event Extraction as Definition Comprehension</title>
      <link href="/2021/04/29/reading-the-manual-event-extraction-as-definition-comprehension/"/>
      <url>/2021/04/29/reading-the-manual-event-extraction-as-definition-comprehension/</url>
      
        <content type="html"><![CDATA[<h1 id="Reading-the-Manual-Event-Extraction-as-Definition-Comprehension"><a href="#Reading-the-Manual-Event-Extraction-as-Definition-Comprehension" class="headerlink" title="Reading the Manual: Event Extraction as Definition Comprehension"></a>Reading the Manual: Event Extraction as Definition Comprehension</h1><pre><code class="BibTex">@inproceedings{chen-etal-2020-reading,    title = "Reading the Manual: Event Extraction as Definition Comprehension",    author = "Chen, Yunmo  and      Chen, Tongfei  and      Ebner, Seth  and      White, Aaron Steven  and      Van Durme, Benjamin",    booktitle = "Proceedings of the Fourth Workshop on Structured Prediction for NLP",(EMNLP的SPNLP workshop)    month = nov,    year = "2020",    address = "Online",    publisher = "Association for Computational Linguistics",}</code></pre><blockquote><ul><li>原文链接：<a href="https://www.aclweb.org/anthology/2020.spnlp-1.9/">https://www.aclweb.org/anthology/2020.spnlp-1.9/</a></li><li>作者：<a href="https://tongfei.me/">Chen, Tongfei</a></li><li>代码：未开源</li></ul></blockquote><h2 id="Background-要解决的问题"><a href="#Background-要解决的问题" class="headerlink" title="Background(要解决的问题)"></a>Background(要解决的问题)</h2><ol><li>人工事件抽取和机器事件抽取之间的存在一些区别。人工进行事件抽取通常是给个Annotation Manual以及几个examples，人去抽取。<strong>而机器事件抽取则是给定大量examples，机器自己去学习，缺少了Annotation Manual这一环</strong>；</li></ol><h2 id="Contributions-创新点"><a href="#Contributions-创新点" class="headerlink" title="Contributions(创新点)"></a>Contributions(创新点)</h2><ol><li>通过为每一个event type构造**<code>bleached statement</code><strong>引入Annotation Manual中的外部知识，采用</strong><code>cloze style MRC</code>（not traditional！）**来求解event extraction；</li><li>为解决一个Argument Role可能存在Multiple-Span（即多个答案）的问题，<strong>提出了一个Multiple-Span Selector</strong>；</li><li>能够运用到few-shot和zero-shot上；</li></ol><h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><h3 id="1-A-Bleached-Statement"><a href="#1-A-Bleached-Statement" class="headerlink" title="1. A Bleached Statement"></a>1. A Bleached Statement</h3><p>漂白的陈述？hhh，这个名字有点高大上。以ACE 2005数据集中的<code>LIFE:BE-BORN</code>事件作为一个例子，它对应的bleached statement是：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gq0p41j3naj20i0023q2z.jpg"></p><p>填充bleached statement之后得到：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gq0p4yuejrj20hj01ydfx.jpg"></p><p>本论文的工作就是给定 <code>bleached statement</code> 和 <code>text</code>，得到填充后的bleached statement。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="0-An-Example-of-the-Method-Overview"><a href="#0-An-Example-of-the-Method-Overview" class="headerlink" title="0. An Example of the Method(Overview)"></a>0. An Example of the Method(Overview)</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gq0pqdk2ayj210s0geten.jpg"></p><p>如上图可见，采用的是<strong>逐步求精（refine step by step）</strong>的方式，一步一步地填充上bleached statement。</p><h3 id="1-Multiple-Argument-Selector"><a href="#1-Multiple-Argument-Selector" class="headerlink" title="1. Multiple Argument Selector"></a>1. Multiple Argument Selector</h3><p>本论文的问题场景与traditional MRC存在些许不同：</p><ol><li>traditional MRC的Query是问句，而本文是cloze-style problem；</li><li>traditional MRC的answer span通常只有1个。而在本文中，对于一个Argument Role，可能存在多个Argument，需要考虑Multiple Argument的情况。</li></ol><p>论文通过使用sequence tagging的方式来解决这个问题，使用的sequence tagging的方法是CRF，tagging schema是<code>BIO</code>。</p><p>下面通过一个实例来理解文章CRF的运行机制：</p><p>对于前面提到的example中的text。当进行到Round 2的时候，placeholder是<code>someone else</code>，此时，将text中的每一个token作为query，placeholder中的每一个token作为key和value。得到placeholder的attentive representation：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gq0wikyhr8j20hs059t8w.jpg"></p><p>然后使用下面拼接起来的特征作为CRF中potential function的输入：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gq0wiwi9u0j20hv01n3ye.jpg"></p><p>potential function就是一个Multi-layer Feed-forward neural network：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gq0wj6pu25j20hm01ma9z.jpg"></p><h3 id="2-Trigger-Identification"><a href="#2-Trigger-Identification" class="headerlink" title="2. Trigger Identification"></a>2. Trigger Identification</h3><p>Trigger能够被看作是一个特殊的argument，所以论文采用前文提到的argument selection model用于trigger identification，而placeholder是除去argument placeholder之后的bleached statement中的所有单词，例如下面划线部分：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gq0wno5hkej20ig02rwel.jpg"></p><h3 id="3-Training-Data-Generation"><a href="#3-Training-Data-Generation" class="headerlink" title="3. Training Data Generation"></a>3. Training Data Generation</h3><p>argument extractor的输入是$(S,I,T)$形式的三元组。在训练的过程中，每一个refined bleached statement使用gold bleached statement，而不是前一步的predicted bleached statement。（这个训练方式好像是常规操作，之前看的Joint Model也是这么训练的）</p><h3 id="4-Negative-Sampling"><a href="#4-Negative-Sampling" class="headerlink" title="4. Negative Sampling"></a>4. Negative Sampling</h3><p>对trigger identification进行负采样来扩充数据集。对于每一个example，构建负样本的方法是选择其$\alpha$%的负样本事件类型。（毕竟只有trigger identification可以这样做）</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>ACE 2005~</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>由于是探索性的实验，所以结果并不是很好，实验结果如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gq0xt3okd8j20of0algnr.jpg"></p><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><p>文章对于argument extractor的error analysis倒是挺有趣的：</p><ol><li><strong>Relative clauses</strong>。例如论文模型抽取<em>Mosul</em>作为argument，而gold answer却是*Mosul, where U.S. troops killed 17 people in clashes earlier in the week.*这两个本质上是一样的，但是由于clause的存在，导致了error；</li><li><strong>Counts</strong>。例如gold answer是<em>300 billion yen</em>，而论文模型抽取是<em>300  billion</em>；</li><li><strong>Durations</strong>。例如gold answer是<em>lasted two hours</em>，而论文模型抽取是<em>two hours</em>；</li></ol><p>从上面的error analysis可以看出，没有entity mention，单纯基于start-end span的抽取方式会存在semantic相同，但是span精度出现问题。</p><h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><h3 id="有价值的参考文献"><a href="#有价值的参考文献" class="headerlink" title="有价值的参考文献"></a>有价值的参考文献</h3><ol><li>Trung Minh Nguyen and Thien Huu Nguyen. 2019. One for all: Neural joint modeling of entities and events. In Proc. AAAI, pages 6851–6858.</li><li>Lifu Huang, Heng Ji, Kyunghyun Cho, Ido Dagan, Sebastian Riedel, and Clare Voss. 2018. Zero-shot transfer learning for event extraction. In Proc. ACL, pages 2160–2170.</li><li>Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-relation extraction as multi-turn question answering. In Proc. ACL, pages 1340–1350.<strong>（QA for relation extraction）</strong></li><li>Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning.<br>In Proc. ACL, pages 2895–2905.<strong>（Cloze style MRC for relation extraction）</strong></li><li>Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer extraction as sequence tagging with tree edit distance. In Proc. NAACL, pages 858–867.<strong>（远古时期，使用序列标注解决抽取式阅读理解的模型）</strong></li><li>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML, pages 282–289.<strong>（将如何将CRF运用到序列标注的经典论文）</strong></li></ol><h3 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h3><ol><li>采用refine bleached statement step by step的方式，间接规定了论元抽取是有顺序的。这里面并没有什么逻辑，而且抽取在前的argument可以为后继argument提供信息，而后继argument却不能影响抽取在前的argument，似乎并不是很好；</li></ol><h3 id="不懂的地方"><a href="#不懂的地方" class="headerlink" title="不懂的地方"></a>不懂的地方</h3><ol><li><p>对于论文中提到的Potential Function of CRF，之前没有了解，里面有很多学问：</p><blockquote><p>The potential function at each position of the input sequence in a neural CRF is typically decomposed into <strong>an emission function (of the current label and the vector representation of the current word)</strong> and <strong>a transition function (of the previous and current labels)</strong></p><p><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.236.pdf">https://www.aclweb.org/anthology/2020.findings-emnlp.236.pdf</a></p></blockquote></li><li><p>不太明白identify trigger之后，进行的<code>ANCHOTTRIGGER(S,t)</code>是什么意思？也不知道trigger如何影响argument的抽取。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gq0xqxkwb8j20ga0fnmys.jpg"></p></li></ol><h3 id="可以借鉴的地方"><a href="#可以借鉴的地方" class="headerlink" title="可以借鉴的地方"></a>可以借鉴的地方</h3><ol><li><strong>Trigger based method</strong>，根据Trigger去划分数据集，而且一定要引入Trigger的信息，甚至可以引入Trigger的位置信息，因为Trigger的位置对于论元的抽取非常重要。</li><li>可以作为文章的对比实验！</li><li>论文采用的是<strong>CRF</strong>来标注Argument Span，我们之前都是用<strong>Start-End指针标注</strong>的方式，不知道这两者对实验的结果会有什么影响？</li><li>引入新的预训练语言模型。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MRC </tag>
            
            <tag> Event Extraction </tag>
            
            <tag> EMNLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised Label-aware Event Trigger And Argument Classification</title>
      <link href="/2021/04/27/unsupervised-label-aware-event-trigger-and-argument-classification/"/>
      <url>/2021/04/27/unsupervised-label-aware-event-trigger-and-argument-classification/</url>
      
        <content type="html"><![CDATA[<h1 id="Unsupervised-Label-aware-Event-Trigger-And-Argument-Classification"><a href="#Unsupervised-Label-aware-Event-Trigger-And-Argument-Classification" class="headerlink" title="Unsupervised Label-aware Event Trigger And Argument Classification"></a>Unsupervised Label-aware Event Trigger And Argument Classification</h1><pre><code class="BibTeX">@article{zhang2020unsupervised,  title={Unsupervised Label-aware Event Trigger and Argument Classification},  author={Zhang, Hongming and Wang, Haoyu and Roth, Dan},  journal={arXiv preprint arXiv:2012.15243},  year={2020}}</code></pre><blockquote><p>作者：<a href="https://www.cse.ust.hk/~hzhangal/">Hongming Zhang</a> , <a href="https://why2011btv.github.io/">Haoyu Wang</a> , <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Event Extraction </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何有效地研读论文？</title>
      <link href="/2021/04/25/ru-he-you-xiao-di-yan-du-lun-wen/"/>
      <url>/2021/04/25/ru-he-you-xiao-di-yan-du-lun-wen/</url>
      
        <content type="html"><![CDATA[<p>私聊王博的小伙伴在阅读文献的时候遇到了很多的困惑，王博对其总结如下：</p><ol><li><p>拿到一篇文章无从下手，有时甚至怀疑是否自己应该选择读书这条路？</p></li><li><p>看到英文文章就头大，咬牙读了摘要后，几天后拿到同一篇文章还是迷迷糊糊</p></li><li><p>读一篇文章花费太久时间，一篇文章花费3天甚至一周，然而导师叫你做报告的时候，你又觉得自己啥也没读懂，讲不出来</p></li><li><p>读了一篇文章，仅仅是读了，感觉读和没读一个概念，和“消磨时间”类似</p></li></ol><p>你读文献的时候是否也有这种感受？其实，有这种感受是很正常的，不不必过于担心怀疑自己，考研难么难都挺过来了，又怎能被文献给吓到呢？</p><p>读文献是做科研或者工作时必不可少的一项工作，无法回避，<strong>重要的是要有信心！</strong>(小杨小杨，信心满满(ง •_•)ง)</p><p>今天我们就聊聊读哪些论文和如何读论文：</p><h2 id="读哪些论文"><a href="#读哪些论文" class="headerlink" title="读哪些论文"></a>读哪些论文</h2><ul><li><strong>高质量期刊会议论文</strong>，AI领域，我们所说的顶会文章，ACL、EMNLP、NAACL、COLING、NIPS、AAAI、ICLR等</li><li><strong>经典论文：</strong>某个领域的一些经典文章，比如网络结构中的经典文章Transformer、BERT、ELMo、GPT等</li><li><strong>高引用论文：</strong>高引用，代表被同行普遍认可、借鉴的文章</li><li><strong>知名学者团队：</strong>你的研究领域知名研究团队的文章，比如AI领域的吴恩达、Aann LeCun等。</li><li><strong>有代码的论文：</strong>Talk is cheap，show me the code。能说算不上什么，有本事把你代码给我看看。</li></ul><h2 id="如何找论文"><a href="#如何找论文" class="headerlink" title="如何找论文"></a>如何找论文</h2><p>找论文，<strong>既要找经典文献（经典著作）</strong>，了解研究方向的发展脉络，同时也要找<strong>前沿文献</strong>，保持知识的实效性，能够让自己把握整个研究方向的发展前沿，使得自己对自己的研究方向有个系统的宏观把握和前沿预知。</p><p>下图列了一些找论文的途径：</p><blockquote><p>重要的**Google搜索工具(Google学术)**不要忘记哟~</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpwavi3fozj20i006k74d.jpg"></p><ul><li><p><font color="red"><strong>课题组：</strong></font>首先，文献的第一来源是你的导师，查询你导师在你的研究方向上的研究成果，这也是最接近你的研究方向的文献；其次，师兄师姐的研究成果，请教师兄师姐有时可能比导师还管用哟（毕竟导师那么忙，可能顾不上你这个小喽啰），师兄师姐能够亲自辅导你，在你研究道路上难得的一手资源！</p></li><li><p><strong>方向大牛</strong>：查看学术大牛的主页，或者学者的社交平台，一般都会搜索到大牛的学术成果。</p><blockquote><p>信息抽取领域：</p><ul><li>中科院自动化所：刘康，陈玉博老师</li><li>Heng Ji</li><li>宾夕法尼亚大学<a href="https://cogcomp.seas.upenn.edu/">Cognitive Computation Group of University of Pennsylvania</a></li><li>普林斯顿大学：<a href="https://www.cs.princeton.edu/~danqic/papers.html">陈丹琦</a>、高天宇</li><li>刘知远</li><li>张岳</li></ul></blockquote></li><li><p><strong>厉害的机构单位：</strong>有一些机构也是很高产的，并且文献成果质量很高，也是值得关注的资源。</p></li><li><p><strong>会议论文集：</strong>实效性高，质量上乘，比如计算机视觉顶会CVPR、ECCV、ICCV，人工智能会议AAAI，NeurIPS，ICML，ICLR，ACL……等等</p></li><li><p><strong>预印网站：</strong>当然是再熟悉不过的arxiv，实效性很高，有些成果会提前就提交在此网站，不过质量参差不齐。</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpwaxn47ruj20g90dyt9n.jpg"></p><hr><p>下边是几个常用的论文下载平台：</p><ul><li><p><strong>知网：</strong>多数是中文论文，对于刚入门，可能比较友好一些</p></li><li><p><strong>百度学术、Google scholar：</strong>搜集论文</p></li><li><p><strong>arXiv</strong>：论文预印本，有时候还没发表的论文，可能会提前发布在此平台上，所以有些论文会比较超前</p></li><li><p><strong>顶会论文</strong>：CVPR、ICCV、ECCV等顶会论文，并且每一年都有人进行总结归纳。举例联盟Sophia总结的几个论文合集GitHub链接：</p><ul><li><p><a href="https://github.com/Sophia-11/Awesome-CVPR-Paper">https://github.com/Sophia-11/Awesome-CVPR-Paper</a> (CVPR论文)</p></li><li><p><a href="https://github.com/Sophia-11/Awesome-ICCV">https://github.com/Sophia-11/Awesome-ICCV</a> (ICCV论文)</p></li><li><p><a href="https://github.com/Sophia-11/Awesome-AAAI">https://github.com/Sophia-11/Awesome-AAAI</a> (AAAI论文)</p></li></ul></li><li><p><strong>PapersWithCode</strong>：<a href="https://paperswithcode.com/">https://paperswithcode.com/</a></p></li><li><p><strong>OpenReview</strong>：<a href="https://openreview.net/">https://openreview.net/</a></p></li><li><p><strong>GitHub</strong>：<a href="https://github.com/">https://github.com</a></p></li><li></li></ul><h2 id="如何读论文"><a href="#如何读论文" class="headerlink" title="如何读论文"></a>如何读论文</h2><p>那么问题来啦，我们如何阅读一篇论文呢？很多小伙伴经常懊恼，文献看了一遍又一遍，标记的密密麻麻的（看着满满的成就感），但第二天再看，<strong>这论文讲了啥呀？我竟然度过这篇论文，我怎么什么都不知道呀？</strong>（我本人了😓）</p><h3 id="第一遍：泛读"><a href="#第一遍：泛读" class="headerlink" title="第一遍：泛读"></a>第一遍：泛读</h3><p>重点阅读论文的<strong>标题、摘要、结论和图表</strong>。泛读的之后，对文章有一个大概的整体把握：</p><ul><li><strong>论文解决的问题是什么？</strong></li><li><strong>论文使用的方法是什么？</strong></li><li><strong>论文的效果是什么？</strong></li></ul><p>一般一个论文会包括以下几个部分：</p><ol start="0"><li><p><strong>Abstract</strong>，摘要，论文的整体概括，使用的方法，解决的问题，达到的效果一般都会被概括其中</p></li><li><p><strong>Introduction</strong>，介绍，研究背景及意义，存在的问题</p></li><li><p><strong>Related Work</strong>，相关工作，相关工作的简介，分析优缺点</p></li><li><p><strong>Method</strong>，方法，使用的方法及实现细节</p></li><li><p><strong>Experiments</strong>，实验，实验结果分析</p></li><li><p><strong>Discussion</strong>，讨论，论文结论及未来的方向</p></li></ol><p>第一遍泛读，通过文章标题、摘要、结论，结合图表，我们基本能确定上边的<strong>三个“是什么”</strong>的答案。</p><h3 id="第二遍：精读"><a href="#第二遍：精读" class="headerlink" title="第二遍：精读"></a>第二遍：精读</h3><p>通过第一遍的泛读，我们了解了整个文章的脉络和分布，找出文章的重要内容，进行仔细研读。</p><p>一般的，我们需要对<font color="red"><strong>方法及实验部分</strong></font>进行详细阅读：</p><ul><li><strong>方法部分</strong>：方法的基本原理，作者改进的部分；</li><li><strong>实验部分</strong>：作者的实验细节，这可能也导致我们能否复现此篇论文的影响因素；</li></ul><p>当然了，如果你同时需要积累如何写论文的话，在精读时候，也需要学习作者对结果的分析和讨论。</p><h3 id="第三遍：总结归纳记录"><a href="#第三遍：总结归纳记录" class="headerlink" title="第三遍：总结归纳记录"></a>第三遍：总结归纳记录</h3><p>好记性不如烂笔头，尤其是需要读很多论文的时候，<strong>要养成归纳总结记录的好习惯</strong>，文章中的<strong>创新点是什么？这篇论文对我的领域有什么可借鉴的吗？</strong></p><p>总之，多问自己为什么？最后能够达到闭着眼睛，将<strong>论文解决的问题、使用的方法、方法的结果、方法的创新点、结论等关键内容</strong>叙述出来。</p><h2 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h2><p>首先晒一下阅读论文的模板，简单明了，论文模板是为了对文献内容的记录，<strong>王博觉得能够将文献记录的清晰明了就是好的模板</strong>。样式不用太花哨，这又不是什么模板比赛，何必花费很多时间在外观上呢？</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpwasngjzqj20h40mojsv.jpg"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>微信公众号——计算机视觉联盟</li><li><a href="https://mp.weixin.qq.com/s/OidUnjhSb1q-VCiaQFtBdg">聊聊研读论文有哪些经验之谈？</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU2NTc5MjIwOA==&amp;mid=2247497362&amp;idx=2&amp;sn=5b3436738550a7e4d62c2bb98befe3df&amp;chksm=fcb4ea2dcbc3633b228e5712ecfc9d302ff56b7d96e83232f1c7ffb0a8bd2ffc908b5003b9d9&amp;scene=21#wechat_redirect">收藏 | 有哪些相见恨晚的科研经验？</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzU2NTc5MjIwOA==&amp;mid=2247489104&amp;idx=1&amp;sn=189ff572dc14ee137e83e7e4e7a4cc9f&amp;chksm=fcb70aefcbc083f9b351b1b29c56d70cc1748181583490c3769b77a495e74747b8f7491ee39a&amp;scene=21#wechat_redirect">博士如何高效率阅读文献？有哪些技巧可以借鉴？</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 科研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>碎碎念20210425</title>
      <link href="/2021/04/25/sui-sui-nian-20210425/"/>
      <url>/2021/04/25/sui-sui-nian-20210425/</url>
      
        <content type="html"><![CDATA[<h1 id="碎碎念20210425"><a href="#碎碎念20210425" class="headerlink" title="碎碎念20210425"></a>碎碎念20210425</h1><h2 id="关于Top-2博士就业（好像与我无关-）"><a href="#关于Top-2博士就业（好像与我无关-）" class="headerlink" title="关于Top 2博士就业（好像与我无关~）"></a>关于Top 2博士就业（好像与我无关~）</h2><p>这里总结的王博Kings好友的情况<strong>基本是本科985保清北直博或者本科就是清北又继续在清北读书的</strong>，涵盖了计算机相关专业，比如电子信息EE其实也算计算机相关，毕竟人家去了互联网。</p><p><strong>科研如果做得比较好的</strong>，CCF-A得有个四五篇以上的，大部分都选择了继续攻读，要么出国要么留下来做博后，虽然待遇比不上互联网，但是top2的平台和人脉，以及感情，大家还是愿意留下来，由于疫情，出国计划的虽然拿到了offer，但是还没有去办签证。同时这一批小伙伴里，因为论文和代码写的好，其实没有找工作，然后那些大厂的HR通过论文的邮箱直接联系看愿意不愿意毕业来这边；其实更简单的是，我们有很多内部的群，比如计算机视觉群这类的，国内一些大佬还有一些师兄师弟发顶刊的都在里面，有时公司的leader直接通过微信群加这些人，问愿意不愿意加入课题组。所以找工作不像想象那么难，你所渴望的，甚至这些大佬都不用去投简历</p><p><strong>科研如果做得一般</strong>，又不像做学术的，一般都选择互联网的居多。这些人基本都有实习的经历，总体而言，今年行情在50w-100w之间的居多，大部分选择留北京或者去上海深圳，很少回二线城市的，留北京的也基本都保证解决北京户口</p><p><strong>一些公司情况（大家可能最感兴趣的就是钱了）：</strong></p><p><strong>华为：</strong>华为还是有天才少年，其实难度不算很大，尤其在top2这种的博士里，科研大牛基本都能获得，要不就是中科院计算所了自动化所了，天才少年的话薪资基本都在100w以上了；周围普通同学拿到的大部分在65w-90w之间，北京户口解决，但是公积金了只有5%，有同学担心工作压力，不想那么累</p><p><strong>快手：</strong>快手和拼多多有一拼，给钱不含糊，基本在80w左右，似乎要上市了，所以招人挺猛，但是听说压力大小周？</p><p><strong>百度：</strong>百度的光芒似乎没有前几年那么强烈了，似乎没有几个人去面或者投百度？offer也在70w左右</p><p><strong>美团：</strong>今年美团的硕士有一部分都开45w，博士在65-75w左右比较多</p><p><strong>阿里：</strong>一般定岗为P6，60w左右；好一点的能拿到80w左右</p><p><strong>字节：</strong>基本都在80w左右</p><p><strong>拼多多：</strong>最近大家反映不太好，虽然给钱多，但是在上海，还有名声，所以选择的似乎不多或者没有？</p><p>王博Kings的<strong>感受</strong>：今年互联网的薪资确实非常高了，以前都是SSP，现在都是一些天才计划，博士有个好处就是户口基本都解决了，offer都在五六十万以上，清北的计算机博士就是厉害，让人羡慕</p><p>王博Kings<strong>给大家的一些建议：</strong></p><ol><li><p>博士狂发论文，工作根本不用愁</p></li><li><p>有实习了就参与一下，到时候一般实习组会捞你起来，最起码是个保底的offer</p></li><li><p>秋招的话尽量越早越好，越早难度越小，offer越早，心里越踏实</p></li><li><p>HR只是谈生意的，你就把对话当做生意，要会斡旋</p></li></ol><h2 id="如何选择导师？"><a href="#如何选择导师？" class="headerlink" title="如何选择导师？"></a>如何选择导师？</h2><p>一个比较正确的做法：<strong>加一些你要报考的学校的院系的学生，然后别问他导师，问其他导师他的看法，因为你问他导师，他基本不会给你说实话，不信你细细品。</strong></p><h2 id="如何选择方向？"><a href="#如何选择方向？" class="headerlink" title="如何选择方向？"></a>如何选择方向？</h2><ul><li>读综述，英文最新的综述和知名学者的中文综述</li><li>读博士论文，最新的博士论文读吧</li><li>读最新文献，看最后discussion，看哪里没完善</li><li>大概2-3个月，然后基本就能找到几个点</li><li>这些点，肯定都是没有解决的，但是你要注意，你是否能解决？</li><li>你课题组的能力在哪里？有强大的算力和资源？有其他的优势？</li><li>如果不是顶级课题组，你追最新最热的，大概率你跑不过人家试验的速度</li><li>交叉结合才是王道</li></ul><p><strong>其实方向对于一名硕士、博士有着非凡的意义，选择比努力重要，请大家一定要慎重慎重！</strong></p><h2 id="如何发论文应对投稿审稿"><a href="#如何发论文应对投稿审稿" class="headerlink" title="如何发论文应对投稿审稿"></a>如何发论文应对投稿审稿</h2><ul><li>你写的每一篇文章，你需要自己以审稿人角度去评判</li><li>假如你自己就知道还有个试验没做，你觉得审稿人是傻子看不出来吗？</li><li>根据自己领域写文章的风格，模仿学习写作</li><li>根据课题组投稿情况去实际投稿，你课题组都没人投CNS，你非要投，这不是完全浪费时间吗？量力而行，除非你的创新点确实很有干货</li><li>不要因为审稿专家意见觉得自己水平不行，可能审稿专家水平真的不行</li><li>当有一天你是审稿专家的时候，你或许也看不懂</li></ul><p>最好有师兄师姐带你，否则这个难度真的非常大！</p><h2 id="参与会议？"><a href="#参与会议？" class="headerlink" title="参与会议？"></a>参与会议？</h2><ul><li>导师让你参加的你再去，别自作主张，否则谁报销？</li><li>会议去不了的会后去看slides或者找人要视频ppt</li><li>会议去学习交流扩大朋友圈，交叉，而非就是旅游</li><li>学好英语很重要</li></ul><h2 id="参与实习交流？"><a href="#参与实习交流？" class="headerlink" title="参与实习交流？"></a>参与实习交流？</h2><ul><li>运气好，除了课题组项目，导师会安排你去参与实习</li><li>大公司实习的话首选AI Lab研究岗位，和学校里没啥区别，由于没有什么利益瓜葛，更纯粹的学习</li><li>拓展人脉，方便后期帮你内推不同公司</li></ul><h2 id="工资水平的差异，能差一个人出来"><a href="#工资水平的差异，能差一个人出来" class="headerlink" title="工资水平的差异，能差一个人出来"></a>工资水平的差异，能差一个人出来</h2><p>我问了一下去年秋招的那一批小伙伴，以研究生为例，985硕研究生，搞开发和算法的，<strong>好一点的一线知名互联网大厂基本都在40w+，再高的能到接近60w</strong>，而整天投简历的，甚至20w的都很难拿到，有的退而求其次去做产品经理等不是开发的工作，40w和20w中间可不就差出一个人吗？</p><p>现在普通的说是白菜价，然后出来一个sp，然后又出来ssp，然后又出来什么北斗计划，什么阿里星，什么天才少年，这种再往上级别的，差出来的就不止一两人了</p><h2 id="工作状态，最常听到的莫过于996和955"><a href="#工作状态，最常听到的莫过于996和955" class="headerlink" title="工作状态，最常听到的莫过于996和955"></a>工作状态，最常听到的莫过于996和955</h2><p>955公司呢，我的印象里，国内互联网公司似乎对996比较信奉，很少有公司愿意955，毕竟现在竞争激烈，大鱼吃小鱼，小鱼吃虾米，搞不好就倒闭了，除非上市才能在没钱可烧的时候活下来。但是955公司我说接触的似乎只有google和microsoft这两家公司？其他的apple了，NVIDIA也听说过，但是接触不多。大家以两面性去看955和996，不管自己所处哪个公司，<strong>如果年龄在35岁之下，一定要抓紧时间学习，为35岁之后做打算，不管是退休还是跳槽，都提前规划，避免到时候因为跟不上节奏和时代被淘汰。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 就业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Closer Look At Few-Shot Classification</title>
      <link href="/2021/04/24/a-closer-look-at-few-shot-classification/"/>
      <url>/2021/04/24/a-closer-look-at-few-shot-classification/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Closer-Look-At-Few-Shot-Classification"><a href="#A-Closer-Look-At-Few-Shot-Classification" class="headerlink" title="A Closer Look At Few-Shot Classification"></a>A Closer Look At Few-Shot Classification</h1><pre><code class="BibTeX">@article{chen2019closer,  title={A closer look at few-shot classification},  author={Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt and Wang, Yu-Chiang Frank and Huang, Jia-Bin},  journal={arXiv preprint arXiv:1904.04232},  year={2019}}</code></pre><h2 id="一、文章针对的问题和主要贡献"><a href="#一、文章针对的问题和主要贡献" class="headerlink" title="一、文章针对的问题和主要贡献"></a>一、文章针对的问题和主要贡献</h2><p>在小样本学习（Few-shot Learning）方面，近几年各种方法层出不穷，模型结构和学习算法也越来越复杂。然而，这些方法之间没有在统一的框架下进行比较。这篇文章针对几个关键问题，如<u>数据集对小样本学习方法的影响</u>、<u>网络深度对这些方法的性能的影响</u>以及<u>领域漂移下这些方法的表现</u>，对当前的小样本学习方法进行性能比较和阐释。具体地，本文有以下三个主要贡献：</p><ol><li>对已有的representative few-shot classification algorithms做了比较。特别地，实验了基础模型（<code>backbone</code>）的能力对这些方法的性能的影响，结果显示，在<strong>领域差异比较小</strong>的情况下（如CUB这种细粒度（fine-grained）分类任务），随着基础即特征提取神经网络的能力的提高（从四层的CNN提升到resnet），这些方法之间的性能差异越来越小；相反地，在<strong>领域差异比较大</strong>的情况下（如miniImageNet），随着基础即特征提取神经网络的能力的提高，这些方法的性能差异越来越大；</li><li>文章建立了两个普通简单的baseline，发现在CUB和miniImageNet上的性能足以和当前最先进的基于元学习的方法媲美；</li><li>基于（1）的结果和一些已有的研究，作者特别强调了小样本学习任务中的领域自适应问题，并且设计实验显示当前这些state-of-the-art的小样本学习方法在领域漂移的情况下表现相当不好，没有baseline表现好，提醒人们对这个方向多关注。</li></ol><h2 id="二、模型介绍"><a href="#二、模型介绍" class="headerlink" title="二、模型介绍"></a>二、模型介绍</h2><h3 id="Baseline-and-Baseline"><a href="#Baseline-and-Baseline" class="headerlink" title="Baseline and Baseline++"></a>Baseline and Baseline++</h3><p><img src="https://pic2.zhimg.com/80/v2-fabd9da29458ebc98362e055f777ad61_720w.jpg" alt="Baseline and Baseline++ few-shot classification methods"></p><p>上图就是本文提出的模型。</p><p>其中包括<code>baseline</code>和<code>baseline++</code>模型，两个模型都是采用标准的<code>transfer learning</code>学习方法——<code>pre-training</code>和<code>finetuning</code>。</p><p>它们都包括两个阶段<code>Training stage</code>和<code>Fine-tuning stage</code>：</p><ul><li><strong>Training stage</strong>：输入是base dataset，作用是得到$f(\theta)$用来抽取特征。</li><li><strong>Finetuning stage</strong>：输入是support set，作用是得到一个用于novel class之上的classifier。</li></ul><blockquote><p>论文中没有提到的是——<strong>通常support set很小，因此在finetuning的时候，需要加入regularization来防止过拟合。</strong>（参考王树森老师的lecture中提到的论文——A baseline for few-shot image classification）</p></blockquote><p>baseline在分类时使用了线性分类器(即Linear Layer)；baseline++在分类时使用了cos距离(cosine distance)的分类器，论文作者提出<strong>cosine distance能够降低intra-class variations</strong>（好像只要是distance就可以降低intra-class variations，而且$W$里面的向量也可以看作是每个类别的prototype）。</p><p>下图显示了本文比较的四个基于meta-learning模型，分别是MatchingNet，ProtoNet，RelationNet，MAML：</p><p><img src="https://pic2.zhimg.com/80/v2-b793e65bc5001498ce4b4ca0a2c19d95_720w.jpg" alt="Meta-learning few-shot classification algorithms"></p><p>如上图所示，基于<code>meta-learning</code>的方式通常包括两个阶段：<code>meat-training stage</code>和<code>meta-testing stage</code>，这与前文提到的baseline和baseline++有所不同。</p><h2 id="三、实验"><a href="#三、实验" class="headerlink" title="三、实验"></a>三、实验</h2><h3 id="1-Evaluation-Using-The-Standard-Setting"><a href="#1-Evaluation-Using-The-Standard-Setting" class="headerlink" title="(1) Evaluation Using The Standard Setting"></a>(1) Evaluation Using The Standard Setting</h3><p>所谓的standard setting就是few-shot论文中常见的evaluation setting，直接看下面这张表就是了：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpv7ee9ugvj20la08s40f.jpg"></p><h3 id="2-Effect-Of-Increasing-The-Network-Depth"><a href="#2-Effect-Of-Increasing-The-Network-Depth" class="headerlink" title="(2) Effect Of Increasing The Network Depth"></a>(2) Effect Of Increasing The Network Depth</h3><p>如下图所示，结果显示：</p><ul><li>在<strong>领域差异比较小的情况下</strong>，随着基础即特征提取神经网络的能力的提高（从四层的CNN提升到resnet），这些方法之间的性能差异越来越小；</li><li>相反地，在<strong>领域差异比较大的情况下</strong>，随着基础即特征提取神经网络的能力的提高，这些方法的性能差异越来越大。</li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpuqawaslkj216u0i2tci.jpg"></p><h3 id="3-Effect-Of-Domain-Differences-Between-Base-And-Novel-Classes"><a href="#3-Effect-Of-Domain-Differences-Between-Base-And-Novel-Classes" class="headerlink" title="(3) Effect Of Domain Differences Between Base And Novel Classes"></a>(3) Effect Of Domain Differences Between Base And Novel Classes</h3><p>state-of-the-art的小样本学习方法在**领域漂移(domain shift)**的情况下表现相当不好，没有baseline表现好</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpuqf4w4j2j20y90ejwhk.jpg"></p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><ol><li><p><strong>本文没有提出什么先进的模型，但是却进行了一些有意义的实验</strong>，提出了几个大家容易忽略的问题，对小样本分类这一问题有了更加深刻的认识，也指出了一些存在的问题，因此是比较有价值的。特别地，<strong>文章强调了特征提取网络(即backbone)的能力、数据集的差异性以及领域自适应问题对小样本学习任务的影响</strong>，特别强调了应该关注小样本学习中的领域自适应问题。不过在这篇文章的评审过程中，有审稿人觉得领域自适应不应该在小样本学习任务中强调，但是作者坚持自己的看法。</p></li><li><p>这一研究也启示我们在研究过程中应该全面考虑问题，自己得到的模型性能也很大程度上受限于数据集、基础模型等问题的影响，<strong>在其他条件下，可能最简单的模型最具有竞争力。</strong></p></li><li><p>阅读这篇文章的主要原因是我毕业设计用的few-shot方法就是这篇论文中说到的pre-training和finetuning的，想读文章更加深入了解一下这种方法。</p></li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/64672817">[ICLR2019]A Closer Look at Few-shot Classification</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot </tag>
            
            <tag> meta-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Concise Review of Recent Few-shot Meta-learning Methods</title>
      <link href="/2021/04/19/a-concise-review-of-recent-few-shot-meta-learning-methods/"/>
      <url>/2021/04/19/a-concise-review-of-recent-few-shot-meta-learning-methods/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Concise-Review-of-Recent-Few-shot-Meta-learning-Methods"><a href="#A-Concise-Review-of-Recent-Few-shot-Meta-learning-Methods" class="headerlink" title="A Concise Review of Recent Few-shot Meta-learning Methods"></a>A Concise Review of Recent Few-shot Meta-learning Methods</h1><blockquote><p><code>few-shot meta-learning</code>的预期是希望model能像人一样，在prior knowledge的基础上，快速学会新的concepts（有点举一反三内味）。</p></blockquote><h2 id="The-Framework-of-Few-shot-Meta-learning"><a href="#The-Framework-of-Few-shot-Meta-learning" class="headerlink" title="The Framework of Few-shot Meta-learning"></a>The Framework of Few-shot Meta-learning</h2><h3 id="Notation-and-definitions"><a href="#Notation-and-definitions" class="headerlink" title="Notation and definitions"></a>Notation and definitions</h3><h4 id="两个dataset"><a href="#两个dataset" class="headerlink" title="两个dataset"></a>两个dataset</h4><ul><li>${\mathcal {D}}<em>{base} = \lbrace (X</em>{i},Y_{i}),Y_i \in {\mathcal C}<em>{base} \rbrace^{N</em>{base}}_{i=1}$</li><li>${\mathcal {D}}<em>{noval} = \lbrace (\tilde X</em>{i},\tilde Y_{i}),\tilde Y_i \in {\mathcal C}<em>{noval} \rbrace^{N</em>{noval}}_{i=1}$</li></ul><p>其中$\mathcal C_{base}$和$\mathcal {C}_{noval}$是不相交的。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Meta Learning </tag>
            
            <tag> Few-shot Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning</title>
      <link href="/2021/04/17/simple-and-effective-few-shot-named-entity-recognition-with-structured-nearest-neighbor-learning/"/>
      <url>/2021/04/17/simple-and-effective-few-shot-named-entity-recognition-with-structured-nearest-neighbor-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Simple-and-Effective-Few-Shot-Named-Entity-Recognition-with-Structured-Nearest-Neighbor-Learning"><a href="#Simple-and-Effective-Few-Shot-Named-Entity-Recognition-with-Structured-Nearest-Neighbor-Learning" class="headerlink" title="Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning"></a>Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning</h1><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>作者引入了<code>STRUCTSHOT</code>，这是一个简单的few-shot NER System，在<strong>不需要任何用于few-shot上的specific training</strong>的基础上达到了SOTA；</li><li><code>STRUCTSHOT</code>中的<strong>nearest neighbor learning 和 structured decoding</strong>策略解决了现有的few-shot NER模型在O class和model label dependencies上的不足；</li></ol><h2 id="A-standard-evaluation-setup"><a href="#A-standard-evaluation-setup" class="headerlink" title="A standard evaluation setup"></a>A standard evaluation setup</h2><blockquote><p><strong>一个标准的评估设置</strong></p></blockquote><p>以往的few-shot NER是仿照few-shot classification literature的方式，采用episode evaluation。特别的，一个NER system是在多个evaluation episodes上进行evaluation的。</p><ul><li>An episode includes <strong>a sampled K-shot support set of labeled examples</strong> and <strong>a few sampled K-shot test sets</strong>. </li><li>In addition to these prior practices, we propose a more realistic evaluation setting by <strong>sampling only the support sets</strong> and <strong>testing the model on the standard test sets</strong> from NER benchmarks.</li></ul><h3 id="Test-set-construction"><a href="#Test-set-construction" class="headerlink" title="Test set construction"></a>Test set construction</h3><p>如上所述，不使用episode的方式，直接用test set！论文作者说这样能够更加真实地反映realistic data。</p><h3 id="Support-set-construction"><a href="#Support-set-construction" class="headerlink" title="Support set construction"></a>Support set construction</h3><p>与常见的方式一样。</p><h2 id="Model-STRUCTSHOT"><a href="#Model-STRUCTSHOT" class="headerlink" title="Model: STRUCTSHOT"></a>Model: STRUCTSHOT</h2><p>大概的流程如下：</p><ol><li>在source domain上训练一个NER Model，这个NER Model的主要作用就是生成contextual representation；</li><li>在inference结果，1中得到的static representation用于nearest neighbor token classification；</li><li>得到token classification之后，就是NER的传统艺能，使用Viterbi decoder来捕捉标签之间的依赖关系。</li></ol><h3 id="Nearest-neighbor-classification-for-few-shot-NER——得到token-tag-emission-scores"><a href="#Nearest-neighbor-classification-for-few-shot-NER——得到token-tag-emission-scores" class="headerlink" title="Nearest neighbor classification for few-shot NER——得到token-tag emission scores"></a>Nearest neighbor classification for few-shot NER——得到token-tag emission scores</h3><h4 id="Pre-trained-NER-models-as-token-embedders"><a href="#Pre-trained-NER-models-as-token-embedders" class="headerlink" title="Pre-trained NER models as token embedders"></a>Pre-trained NER models as token embedders</h4><p>NLP中，许多元学习方法的思想是在training time中模仿testing time。即，<strong>这些方法在训练过程中反复从training data中抽取多个support set和test set，然后学习representation来最小化在source domain上的few-shot loss。</strong>而在这篇论文中，作者采用的是机器学习小白都会的fully-supervised的方式pre-train NER Model来学习token-level representation。<strong>至于原因是什么？作者没说。</strong></p><p>接着采用Nearest Neighbor Classification的方式得到token-tag emission scores：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpno87jv3gj20hw0b375w.jpg"></p><h3 id="Structured-nearest-neighbor-learning——得到tag-tag-transition-scores"><a href="#Structured-nearest-neighbor-learning——得到tag-tag-transition-scores" class="headerlink" title="Structured nearest neighbor learning——得到tag-tag transition scores"></a>Structured nearest neighbor learning——得到tag-tag transition scores</h3><p>CRF：</p><ul><li>token-tag emission scores</li><li>tag-tag transition scores</li></ul><p>暴力few shot的话，是无法做到实现source domain的tag-tag transition scores迁移到target domain上的。STRUCTSHOT通过使用在source domain上估计来的<strong>abstract tag transition distribution</strong>来解决这个问题。此外，STRUCTSHOT舍弃了CRF中的训练阶段，仅在inference时使用Viterbi decoder。</p><p>前面讲到abstract tag，它是抽象的tag，有三类：$O,I,I-Other$。论文中关于它们的描述如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpn1gv1rhmj20f8088myq.jpg"></p><p>从Abstract transition probabilities向Target transition probabilities迁移的过程：s</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpn1ldx8k9j20fa09q75p.jpg"></p><p>abstract transition probabilities的计算也非常简单，就是基于统计的计算。</p><h3 id="Viterbi-Algorithm"><a href="#Viterbi-Algorithm" class="headerlink" title="Viterbi Algorithm"></a>Viterbi Algorithm</h3><p>得到token-tag emission scores和tag-tag transition scores之后，就可以使用Viterbi Algorithm求解了。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验作用于两个few-shot NER scenarios：</p><ul><li><strong>tag set extension</strong></li><li><strong>domain transfer</strong></li></ul><blockquote><p>这还是我第一次听说few-shot还可以再细分。</p></blockquote><p>实验结果如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpnnxeuzcrj20p30jtdkt.jpg"></p><p>从上可知，STRUCTSHOT确实可以达到两个任务上SOTA的成绩。</p><h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><ul><li>论文的方法未免也太短小精悍了，只用了小小的Pre-train的方法，连Fine-tune都没有就达到了SOTA，比起那些做了各种tricks的方法，有点惊艳！</li><li>打算再阅读一些Few-shot的论文补充盲点。</li></ul><h3 id="知识补充：Training-and-evaluation-of-few-shot-meta-learning"><a href="#知识补充：Training-and-evaluation-of-few-shot-meta-learning" class="headerlink" title="知识补充：Training and evaluation of few-shot meta-learning"></a>知识补充：Training and evaluation of few-shot meta-learning</h3><p>few-shot meta-learning通常是形成<strong>few-shot episodes（集）</strong>来训练和评估。An episode就是指代一个task $\mathcal T$。除了C-way K-shot定义的标准few-shot episodes之外，其他episodes也能被使用，只要它们不影响meta-validation或meta-testing中的评估结果。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpn3f71vjjj20j505sab5.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is verbose?</title>
      <link href="/2021/04/16/what-is-verbose/"/>
      <url>/2021/04/16/what-is-verbose/</url>
      
        <content type="html"><![CDATA[<p>z在Deep Learning的编程中，总是会遇到<code>verbose</code>这个概念，我一直理解这个单词就是控制程序打印信息的意思，但是具体是怎么控制打印信息，我一直没理解，查阅资料之后发现，这个参数在Keras中常见，stackoverflow关于它的解释如下：</p><p><code>verbose: Integer</code>. 0, 1, or 2. Verbosity mode.</p><ul><li><p><strong>Verbose=0 (silent)</strong></p></li><li><p><strong>Verbose=1 (progress bar)</strong></p></li></ul><pre><code class="tex">Train on 186219 samples, validate on 20691 samplesEpoch 1/2186219/186219 [==============================] - 85s 455us/step - loss: 0.5815 - acc: 0.7728 - val_loss: 0.4917 - val_acc: 0.8029Train on 186219 samples, validate on 20691 samplesEpoch 2/2186219/186219 [==============================] - 84s 451us/step - loss: 0.4921 - acc: 0.8071 - val_loss: 0.4617 - val_acc: 0.8168</code></pre><ul><li><strong>Verbose=2 (one line per epoch)</strong></li></ul><pre><code class="tex">Train on 186219 samples, validate on 20691 samplesEpoch 1/1 - 88s - loss: 0.5746 - acc: 0.7753 - val_loss: 0.4816 - val_acc: 0.8075Train on 186219 samples, validate on 20691 samplesEpoch 1/1 - 88s - loss: 0.4880 - acc: 0.8076 - val_loss: 0.5199 - val_acc: 0.8046</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Everything You Need To Know About Saving Weights In PyTorch</title>
      <link href="/2021/04/11/everything-you-need-to-know-about-saving-weights-in-pytorch/"/>
      <url>/2021/04/11/everything-you-need-to-know-about-saving-weights-in-pytorch/</url>
      
        <content type="html"><![CDATA[<p>在使用<code>huggingface transformers</code>时经常需要用到保存<strong>model</strong>，或者说是保存model的<strong>parameters</strong>。看了一篇medium上的blog，感觉很有用，解决了我很多的困扰，下面是blog里面的精髓：</p><ol><li><p>Applying <strong><code>named_parameters()</code></strong> on an <em>nn.Module</em> object e.g. <em>model</em> or<br><em>model.layer2</em> or <em>model.fc</em> returns all the names and the respective parameters. <strong>These parameters are <em>nn.Parameter</em> (subclass of <em>torch.Tensor</em>) objects</strong> and therefore they have <em>shape</em> and <em>requires_grad</em> attributes.</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpg4t0u2fij20jt0ia75n.jpg"></p></li><li><p>The <strong><code>requires_grad</code></strong> attribute of a <em>nn.Parameter</em> object (learnable parameter object) decides whether to train or freeze a particular parameter. </p></li><li><p> Applying <strong><code>named_children()</code></strong> on any <em>nn.Module</em> object returns all it’s immediate（直系的） children (also <em>nn.Module</em> objects).</p></li></ol><p>   <img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpg4nbxvrmj20tx0cbjsc.jpg"></p><ol start="4"><li><p><strong><code>state_dict()</code></strong> of any <em>nn.Module</em> object e.g. <em>model</em> or <em>model.layer2</em> or <em>model.fc</em> is simply a python <strong>ordered dictionary</strong> object that maps each parameter to its parameter tensor (<em>torch.Tensor</em> object). The <strong>keys</strong> of this ordered dictionary are the names of the parameters, which can be used to access the respective parameter tensors.</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpg4rwog9gj20te0ijq5h.jpg"></p><blockquote><p>在state_dict()中，原本的parameters从较为复杂的<code>nn.Parameter</code>对象变成了当初只有值的<code>torch.Tensor</code>对象。</p></blockquote></li><li><p>Saving a <em>nn.Module</em> object’s <em>state_dict</em> only <strong>saves the</strong> <strong>weights</strong> of the various parameters of that object and <strong>not the model architecture</strong>. Neither does it involve the <strong>requires_grad</strong> attribute of the weights. So before loading the <em>state_dict</em>, one must define the model first.</p></li><li><p>Entire model (<em>nn.Module</em> object) can also be saved which would include the <strong>model architecture as well as its weights</strong>. Since we are saving the <em>nn.Module</em> object, the <strong>requires_grad</strong> attribute is also <strong>saved</strong> this way. Also we don’t need to define the model architecture before loading the saved file since the saved file already has the model architecture saved in it.</p></li><li><p>Saving the state_dict can be used to only save the weights of the model. It doesn’t save the <em>required_grad</em> flag, whereas saving the entire model does save the model architecture, it’s weights and the <em>requires_grad</em> attributes of all its parameters.</p></li><li><p>Both state_dict as well as the entire model can be saved to make inferences.</p></li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>在huggingface transformers中，使用<code>model.save_pretrained()</code>就可以实现保存模型，采用的方法是第一种——<code>state_dict</code>的方式。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://towardsdatascience.com/everything-you-need-to-know-about-saving-weights-in-pytorch-572651f3f8de">Everything You Need To Know About Saving Weights In PyTorch</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is Gradient Clipping</title>
      <link href="/2021/04/11/what-is-gradient-clipping/"/>
      <url>/2021/04/11/what-is-gradient-clipping/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么需要gradient-clipping？"><a href="#为什么需要gradient-clipping？" class="headerlink" title="为什么需要gradient clipping？"></a>为什么需要gradient clipping？</h2><p>在DL的项目中常常会看到<code>gradient clipping</code>的身影，命令行传入参数<code>grad_clip</code>，然后再调用<code>clip_grad_norm_()</code>函数，如下：</p><pre><code class="python">parser.add_argument("--grad_clip", default=1.0, type=float)</code></pre><pre><code class="python">clip_grad_norm_(self.model.parameters(), self.args.grad_clip)</code></pre><p>查阅资料后，得出结论：<strong>gradient clipping是用来解决exploding gradients问题的</strong>。关于exploding gradients的细节理解，我并未做深入学习，只对它有浅显的intuition：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpfm7dk0avj20ob0c6ad6.jpg"></p><p>如上图，左边图片中的右斜线就是明显的exploding gradients，这样带来的问题就是过大的梯度，一下子就让模型的参数带离了“good region”。因此如果gradients过大，就要适当缩小gradients，这就是gradient clipping要完成的任务。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>其数学形式如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gptqp85k03j20o10bq0u1.jpg"></p><p>PyTorch中使用<code>clip_grad_norm_</code>来实现gradient clipping，它的函数签名是：</p><pre><code class="python">def clip_grad_norm_(parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0) -&gt; torch.Tensor:    r"""Clips gradient norm of an iterable of parameters.    The norm is computed over all gradients together, as if they were    concatenated into a single vector. Gradients are modified in-place.    Args:        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a            single Tensor that will have gradients normalized        max_norm (float or int): max norm of the gradients        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for            infinity norm.    Returns:        Total norm of the parameters (viewed as a single vector).    """</code></pre><p>如上所述，<strong>所有的parameters tensor是一起计算norm的</strong>。<code>max_norm</code>则是一个类似于阈值的东西，当原始norm大于max_norm，才使用max_norm进行gradient clipping。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48">What is Gradient Clipping?</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
            <tag> 梯度裁剪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few-Shot Learning</title>
      <link href="/2021/04/06/few-shot-learning/"/>
      <url>/2021/04/06/few-shot-learning/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Few-Shot-Learning与传统监督学习的不同"><a href="#Few-Shot-Learning与传统监督学习的不同" class="headerlink" title="Few-Shot Learning与传统监督学习的不同"></a>Few-Shot Learning与传统监督学习的不同</h3><p>Few-Shot Learning的数据集一般包括：<code>training set</code>，<code>support set</code>和<code>query</code>。</p><p>Few-Shot Learning的目标是让模型能够在一个很大的training set上<strong>学会区分事物的能力</strong>，而不是学会对training set上的样本进行分类的能力（这是传统监督学习做的事情）。</p><p>Traditional supervised learning：</p><ul><li>Test samples are never seen before.</li><li>Test samples are from <strong>known classes</strong>.</li></ul><p>Few-Shot learning:</p><ul><li>Query samples are never seen before.</li><li>Query samples are from <strong>unknown classes</strong>.</li></ul><p>support set与training set的区别：</p><ul><li>区别体现在数据量上，training set（成百上千）很大，大到能够训练一个DNN。而support set很小很小（个位数），不足以训练一个DNN，<strong>support set只能在predict的时候提供一些额外的信息。</strong></li></ul><h3 id="k-way-n-shot-Support-Set"><a href="#k-way-n-shot-Support-Set" class="headerlink" title="k-way n-shot Support Set"></a>k-way n-shot Support Set</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp9vo513swj20mn0bado6.jpg" alt="4-way 2-shot"></p><ul><li><strong>k-way</strong>: the support set has k classes.</li><li><strong>n-shot</strong>: every class has n samples.</li></ul><p>k值大小与模型accuracy的关系：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp9vpmodjbj20ma0cu3yu.jpg"></p><p>n值大小与模型accuracy的关系：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp9vva16qrj20mt0cr3yu.jpg"></p><h3 id="Few-Shot-Learning-and-Meta-Learning"><a href="#Few-Shot-Learning-and-Meta-Learning" class="headerlink" title="Few-Shot Learning and Meta Learning"></a>Few-Shot Learning and Meta Learning</h3><ul><li><strong>Few-shot learning is a kind of meta learning</strong></li><li><strong>Meta Learning: learn to learn</strong>(太故弄玄虚了，直接当做Few-Shot Learning理解吧，即培养模型进行自主学习（例如学会区分动物），从而能够在low resource数据上表现好)</li></ul><h3 id="Basic-Idea-Learn-a-similarity-function"><a href="#Basic-Idea-Learn-a-similarity-function" class="headerlink" title="Basic Idea: Learn a similarity function"></a>Basic Idea: Learn a <strong>similarity function</strong></h3><p>Basic Idea: Learn a <strong>similarity function</strong> from large-scale <strong>training dataset</strong>.(比如孪生网络)</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpigie0424j20lq0adq92.jpg" alt="image.png"></p><h2 id="Pretraining-and-Fine-Tuning"><a href="#Pretraining-and-Fine-Tuning" class="headerlink" title="Pretraining and Fine-Tuning"></a>Pretraining and Fine-Tuning</h2><p>前面介绍了一些</p><p>Few-Shot Prediction Using Pretrained CNN</p><p>pretrain a CNN for feature extraction（aka embedding）</p><p>如果只是使用pretraining，那么pretraining的作用就是用来提取特征的，pretraining的Network在pretraining之后是不会改变的~如果想改变，那么得使用fine-tuning。</p><p>pretraining+fine-tuning is better than pretraining alone~</p><p>关于模型参数的更新，可以更新pretraining的部分，也可以不更新pretraining的部分。</p><p> 可以训练加入<strong>regularization</strong>防止过拟合。</p><h3 id="Pretraining-Alone"><a href="#Pretraining-Alone" class="headerlink" title="Pretraining Alone"></a>Pretraining Alone</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpj0jmcjijj20it0bnwfo.jpg" alt="Pretraining Alone"></p><h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpj0lhgw9pj20iv0cz756.jpg" alt="Fine-tuning"></p><p>注意，在<code>Training a classifier on the support set</code>的时候，除了训练$W$和$b$，也可以选择继续训练pretraining network(比如cv中的CNN，nlp中的BERT)。</p><p>Entropy regularization是作用在query之上的。。。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gpj15534vlj20je0a00tw.jpg" alt="Entropy Regularization"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.bilibili.com/video/BV1qK411T7a2">Shushen Wang’s Lecture</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Few-Shot Learning </tag>
            
            <tag> 课程笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test1</title>
      <link href="/2021/04/06/test1/"/>
      <url>/2021/04/06/test1/</url>
      
        <content type="html"><![CDATA[<object data="./test1.pdf" type="application/pdf" width="100%" height="1000px"></object>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>test</title>
      <link href="/2021/04/06/test-1/"/>
      <url>/2021/04/06/test-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy</title>
      <link href="/2021/04/03/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/"/>
      <url>/2021/04/03/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/</url>
      
        <content type="html"><![CDATA[<h1 id="Joint-Extraction-of-Entities-and-Relations-Based-on-a-Novel-Decomposition-Strategy"><a href="#Joint-Extraction-of-Entities-and-Relations-Based-on-a-Novel-Decomposition-Strategy" class="headerlink" title="Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy"></a>Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ol><li><strong>Joint Extraction of Entities and Relations</strong> 是指用<strong>一个model</strong>来检测句子中的entity pair以及entity pair之间的relation。以往的method通常采用<strong>extract-then-classify</strong>或者是<strong>unified labeling manner</strong>来求解。这些方案会带来很多冗余的entity pairs以及忽视了实体抽取和关系抽取的内在联系；</li><li>如果一个模型不能完全感知head entity的语义，那么提取相应的尾实体和关系就不可靠；</li></ol><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>为了解决Background中的问题，本文将joint extraction拆解成两个子任务——<strong>H</strong>ead-<strong>E</strong>ntity Extraction和<strong>T</strong>ail-<strong>E</strong>ntity <strong>R</strong>elation Extraction。这是一种<strong>extract-then-label</strong>的方法，能够减少冗余的entity pair。</li><li>HE 和 TER extraction采用的是相同的<strong>span-based extraction framework</strong>。<strong>HE</strong>子任务用于抽取出一个句子中所有可能的Head-Entities，<strong>TER</strong>子任务则是对于每个Head -Entity，抽取出其对应的Tail-Entity和Relation。</li><li>TER能够充分利用head-entity的语义和位置信息。</li></ol><h2 id="Terminology-Related-Work"><a href="#Terminology-Related-Work" class="headerlink" title="Terminology/Related Work"></a>Terminology/Related Work</h2><h3 id="Overlapping-Relation"><a href="#Overlapping-Relation" class="headerlink" title="Overlapping Relation"></a>Overlapping Relation</h3><p>multiple relations share a common entity.</p><h3 id="Pipeline-Relation-Extraction"><a href="#Pipeline-Relation-Extraction" class="headerlink" title="Pipeline Relation Extraction"></a>Pipeline Relation Extraction</h3><blockquote><p>Traditional pipelined methods divide this task into two <strong>separate subtasks</strong>: first extract the token spans in the text to detect entity mentions, and then discover the relational structures between entity mentions.</p></blockquote><p>先用一个model进行实体识别，再用另一个model寻找实体对之间的关系。这种方式忽略了子任务之间的内在联系，<strong>存在error propagation</strong>。解决的办法之一就是通过<strong>parameter sharing</strong>来进行<strong>联合训练</strong>。[4, 17, 23]</p><h3 id="Joint-Relation-Extraction"><a href="#Joint-Relation-Extraction" class="headerlink" title="Joint Relation Extraction"></a>Joint Relation Extraction</h3><h4 id="extraction-then-classify"><a href="#extraction-then-classify" class="headerlink" title="extraction-then-classify"></a>extraction-then-classify</h4><p>这种先提取后分类的方法虽然有了joint loss，但是仍然需要不同的独立组件来分别进行实体提取和关系分类。</p><h4 id="extraction-then-label"><a href="#extraction-then-label" class="headerlink" title="extraction-then-label"></a>extraction-then-label</h4><p>本文提出的方法，对于HE和TER采用相同的HBT。</p><h2 id="Framework-Method"><a href="#Framework-Method" class="headerlink" title="Framework/Method"></a>Framework/Method</h2><h3 id="Tagging-Scheme"><a href="#Tagging-Scheme" class="headerlink" title="Tagging Scheme"></a>Tagging Scheme</h3><blockquote><p>Sequence Labeling Task有很多不同的tagging schemes，例如BIO，BIEOS，span-based等等。本论文采用的是span-based的tagging schemes。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6mle4qehj218n0k1420.jpg" alt="An example of our tagging scheme"></p><p>如上图所示。首先看看 head-entity extraction，它被分解成两个sequence labeling subtasks。第一个subtask用于识别the start of head-entities，第二个subtask用于识别the end of head-entities。</p><p>接着，对于每一个head-entity，TER extraction同样被分解为两个sequence labeling subtasks来识别start和end。</p><p>这种tagging scheme的优势在于<strong>对于有m个head-entities的句子，整个任务被解构为2 + 2m个序列标记子任务，前2个为HE标记子任务，另外2m为TER标记子任务</strong>。比以往的论文有优势。</p><h3 id="Hierarchical-Boundary-Tagger"><a href="#Hierarchical-Boundary-Tagger" class="headerlink" title="Hierarchical Boundary Tagger"></a>Hierarchical Boundary Tagger</h3><blockquote><p>所谓层次的含义是start tagger和end tagger，它们两个是有先后顺序的。</p></blockquote><p>上节介绍了tagging scheme，这是模型的输出，那么怎么得到这种输出呢？论文中使用了一个unified <strong>hierarchical boundary tagger（HBT）</strong>。由于HE和TER采用的都是HBT，所以下面的讲解不进行区分。</p><p>对于从一个句子$S$抽取一个标签为$l$的目标$t$，其概率可以描述为如下：<br>$$<br>p(t,l|S) = p(s_t^l|S)p(e_t^l|s_t^l,S) \tag{1}<br>$$</p><p>这样一种decomposition strategy说明：在抽取目标的时候，start的抽取结果会影响到end的抽取结果。这也就是使用HBT的motivation。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6nm3z8hfj218n0hwjv5.jpg" alt="Figure 2. Model Overview"></p><p>假设现在进行start tag，那么标注单词$x_i$的公式如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6npnscsfj20jv04874m.jpg"></p><p>其中$\mathbf{a}_i$是一个笼统的概念——auxiliary vector。对于HE extraction，它是上图中的$g$，代表a global representation learned from the entire sentence。对于TER extraction，它是$g$，$h^h$和$P_i^{ht}$的组合。</p><p>同理，在进行end tag的时候，公式如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6nw2udo8j20jy04fgm0.jpg"></p><p>start tag（公式2-4）和end tag（公式5-7）的区别在于：</p><ol><li><p>我们用$\mathbf{h}_i^{sta}$代替了$\mathbf{h}_i$，这样就能够在进行end tag的时候嵌入start tag的信息；</p></li><li><p>增加了一个$\mathbf{p}_i^{se}$作为一个单词${word}_i$距离最近的一个start position的距离：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6o3ct5hcj20jp02mmx5.jpg"></p><p>嵌入这种信息，是因为我们希望模型能够<strong>限制提取的实体的长度，并学习到结束位置不可能在开始位置之前。</strong></p></li></ol><p><strong>训练时（training）</strong>，采用cross entropy来作为损失函数：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6sxyzkr0j20jk02nglm.jpg"></p><p><strong>在推断阶段（inference）</strong>，解码方式采用的是——**<code>multi-span decoding algorithm</code>**：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6t12e9n0j20jt0ntq6f.jpg" alt="Multi-span decoding"></p><h3 id="EXTRACTION-SYSTEM"><a href="#EXTRACTION-SYSTEM" class="headerlink" title="EXTRACTION SYSTEM"></a>EXTRACTION SYSTEM</h3><p>上面是对模型的创新之处——<code>tagging scheme</code>和<code>hierarchical boundary tagger</code>两个方面讲起。接下来，将从Figure 2中揭示的模型整体结构来看看。</p><h4 id="Shared-Encoder"><a href="#Shared-Encoder" class="headerlink" title="Shared Encoder"></a>Shared Encoder</h4><p>共享编码层，这是joint方法的常见操作：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6tf691nnj20gi01eq2s.jpg"></p><h4 id="HE-Extractor"><a href="#HE-Extractor" class="headerlink" title="HE Extractor"></a>HE Extractor</h4><p>HE Extractor用于识别候选的head-entities。计算公式如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6teqpbqej20gt01bweb.jpg"></p><p>其中$\mathbf{R}<em>{HE} = { (h_j , type</em>{h_j}) }_{j=1}^{m}$。</p><h4 id="TER-Extractor"><a href="#TER-Extractor" class="headerlink" title="TER Extractor"></a>TER Extractor</h4><p>TER Extractor是要以Head Entity为前提的，因此我们的模型输入应该要能够包含Head Entity的信息最好，那么怎么包含呢？论文提出了两方面的信息：</p><ol><li>加入head entity的特征；</li><li>加入${word}_i$到head entity的距离信息；</li></ol><p>综上，得到下面的公式：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6tjklq4lj20jp01odfp.jpg"></p><p>其中$\mathbf{h}^h = [\mathbf{h}<em>{s_h};\mathbf{h}</em>{e_h};]$。$\mathbf{p}_i^{ht}$就是position embedding（负数embedding怎么查找呀？一个head entity要是一个span怎么计算距离呢？）。</p><p>接下来就是：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6trgcyyrj20jo01odfp.jpg"></p><p>其中，</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6u6333o0j208u01qq2r.jpg"></p><p>最后，我们将head entity和tail entity，relation结合在一起，就得到了最后的关系抽取三元组——${ (h, rel_o,t_o) }_{o=1}^{z}$。<font color="red"><strong>需要特别注意的是，在训练的过程中，$h$使用的是gold head-entity，只是在inference阶段，才会用HE模块得到的head entity。</strong></font></p><h4 id="Training-of-Joint-Extractor"><a href="#Training-of-Joint-Extractor" class="headerlink" title="Training of Joint Extractor"></a>Training of Joint Extractor</h4><p>一直困扰我的就是joint model是怎么训练的，看了这个论文，我大概知道了，原来也没有那么复杂。</p><p>问题主要出在TER extractor的输入上，<strong>论文中说到，虽然HE extractor抽取了所有的head entity，但是TER extraction的输入只有一个head entity！</strong>如果想要遍历所有的head entity，那么需要以head entity为单位生成新的数据：</p><blockquote><p>To share input utterance across tasks and train them jointly, for each training instance, we randomly select one head-entity from gold head-entity set as the specified input of the TER extractor. We can also repeat each sentence many times to ensure all triplets are utilized,<br>but the experimental results show that this is not beneficial.</p></blockquote><p>综上所述，joint loss如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6uemjl27j20jq01bt8j.jpg"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验结果：</p><p><img src="https://img-blog.csdnimg.cn/2020101515554083.png"></p><h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/yubowen-ph/JointER">https://github.com/yubowen-ph/JointER</a></p><h2 id="Thinking-Further-Work"><a href="#Thinking-Further-Work" class="headerlink" title="Thinking/Further Work"></a>Thinking/Further Work</h2><p>论文：</p><ol><li>Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu, ‘Joint extraction of entities and relations based on a novel tagging scheme’, in Proc. of ACL, pp. 1227–1236, (2017).</li><li>Changzhi Sun, Yuanbin Wu, Man Lan, Shiliang Sun, Wenting Wang, Kuang-Chih Lee, and Kewen Wu, ‘Extracting entities and relations with joint minimum risk training’, in Proc. of EMNLP, pp. 2256–2265, (2018).</li><li>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi, ‘Bidirectional attention flow for machine comprehension’, arXiv preprint arXiv:1611.01603, (2016).</li><li><a href="https://kexue.fm/archives/6671">https://kexue.fm/archives/6671</a></li><li><a href="https://blog.csdn.net/qq_38556984/article/details/109092842">https://blog.csdn.net/qq_38556984/article/details/109092842</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联合抽取 </tag>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shallow vs Deep Copying of Python Objects</title>
      <link href="/2021/04/02/shallow-vs-deep-copying-of-python-objects/"/>
      <url>/2021/04/02/shallow-vs-deep-copying-of-python-objects/</url>
      
        <content type="html"><![CDATA[<p>在做NLP实验，进行数据预处理的时候，用到了字典对象之间的assignment，调试的时候发现数据不太对，通过查阅资料发现，原来Python中的assignment也是另有玄机，除此之外，还有shallow copy和deep copy两种高级玩法。</p><h2 id="Assignment"><a href="#Assignment" class="headerlink" title="Assignment"></a>Assignment</h2><p>Python是一门<strong>高度面向对象</strong>的语言，事实上，Python中每一个data都是一个object（int，float也不例外）。</p><p>object分为mutable和immutable两种：</p><ul><li>immutable object就是int，float之类的，这类object是不能改变的；</li><li>mutable object是dict，list这类的，这类object是可以改变的。</li></ul><p><code>n = 300</code></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6ae61rfcj20ho056dfl.jpg" alt="Variable Assignment"></p><p>对于immutable object，它的assignment和C++，Java的操作是一样的，就不再多说。</p><p>但是对于mutable object，它的assignment就是C++中常说的传地址操作，例如<code>a=dict();b=a</code>，那么此后<code>a</code>和<code>b</code>就是同一个东西的两个不同名字罢了，对它们的操作会影响到同一个mutable object。</p><p>综上，对于assignment操作，一定要清楚是操作在immutable object上的还是mutable object上的，否则会带来问题。</p><h2 id="Shallow-Copies"><a href="#Shallow-Copies" class="headerlink" title="Shallow Copies"></a>Shallow Copies</h2><p>通常使用<code>b = a.copy()</code>进行shallow copy。</p><p>shallow copy的示意图如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6andiirnj20ac078abn.jpg" alt="b = a.copy()"></p><p>如上图可见，对于一个mutable object，使用shallow copy会创建一个新的mutable object，但是这个mutable object里面的<strong>child mutable object依然还是指向原来的child mutable object</strong>（注意：只是mutable object，没有immutable object）</p><p>代码示例：</p><pre><code class="python">&gt;&gt;&gt; xs = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]&gt;&gt;&gt; ys = list(xs)  # Make a shallow copy&gt;&gt;&gt; xs[[1, 2, 3], [4, 5, 6], [7, 8, 9]]&gt;&gt;&gt; ys[[1, 2, 3], [4, 5, 6], [7, 8, 9]]&gt;&gt;&gt; xs.append(['new sublist'])&gt;&gt;&gt; xs[[1, 2, 3], [4, 5, 6], [7, 8, 9], ['new sublist']]&gt;&gt;&gt; ys[[1, 2, 3], [4, 5, 6], [7, 8, 9]]&gt;&gt;&gt; xs[1][0] = 'X'&gt;&gt;&gt; xs[[1, 2, 3], ['X', 5, 6], [7, 8, 9], ['new sublist']]&gt;&gt;&gt; ys[[1, 2, 3], ['X', 5, 6], [7, 8, 9]]</code></pre><h2 id="Deep-Copies"><a href="#Deep-Copies" class="headerlink" title="Deep Copies"></a>Deep Copies</h2><p>通常使用<code>b = copy.deepcopy(a)</code>进行deep copy。</p><p>deep copy的示意图如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp6au0q27kj20b307eq4p.jpg" alt="b = copy.deepcopy(a)"></p><p>这个没啥好说的啦，就是完全创建了一个新的对象，和之前的对象没有半毛钱关系的。</p><h2 id="Copy-Arbitrary-Python-Objects"><a href="#Copy-Arbitrary-Python-Objects" class="headerlink" title="Copy Arbitrary Python Objects"></a>Copy Arbitrary Python Objects</h2><p>对于一个任意的Python Object，对于它的shallow copy和deep copy，<strong>可以直接拿dict进行类比就好</strong>。</p><p>代码示例：</p><pre><code class="python">class Point:    def __init__(self, x, y):        self.x = x        self.y = y    def __repr__(self):        return f'Point({self.x!r}, {self.y!r})'&gt;&gt;&gt; a = Point(23, 42)&gt;&gt;&gt; b = copy.copy(a)&gt;&gt;&gt; aPoint(23, 42)&gt;&gt;&gt; bPoint(23, 42)&gt;&gt;&gt; a is bFalseclass Rectangle:    def __init__(self, topleft, bottomright):        self.topleft = topleft        self.bottomright = bottomright    def __repr__(self):        return (f'Rectangle({self.topleft!r}, '                f'{self.bottomright!r})')    rect = Rectangle(Point(0, 1), Point(5, 6))srect = copy.copy(rect)&gt;&gt;&gt; rectRectangle(Point(0, 1), Point(5, 6))&gt;&gt;&gt; srectRectangle(Point(0, 1), Point(5, 6))&gt;&gt;&gt; rect is srectFalse&gt;&gt;&gt; rect.topleft.x = 999&gt;&gt;&gt; rectRectangle(Point(999, 1), Point(5, 6))&gt;&gt;&gt; srectRectangle(Point(999, 1), Point(5, 6))&gt;&gt;&gt; drect = copy.deepcopy(srect)&gt;&gt;&gt; drect.topleft.x = 222&gt;&gt;&gt; drectRectangle(Point(222, 1), Point(5, 6))&gt;&gt;&gt; rectRectangle(Point(999, 1), Point(5, 6))&gt;&gt;&gt; srectRectangle(Point(999, 1), Point(5, 6))</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://realpython.com/python-variables/#variable-assignment">Variables in Python</a></li><li><a href="https://realpython.com/copying-python-objects/">Shallow vs Deep Copying of Python Objects</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python Collections Module</title>
      <link href="/2021/04/02/python-collections-module/"/>
      <url>/2021/04/02/python-collections-module/</url>
      
        <content type="html"><![CDATA[<p>Python语言在设计之初，使用了一个非常有用的特性叫做——<code>modular programming</code>（模块化编程）。基于这种语言特性，写程序就会像搭积木一样简单。而Python中实现模块化编程的工具有<code>functions</code>,<code>modules</code>,<code>package</code>。而今天要学习的collections就是一个非常重要的<code>module</code>。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在开始学习<code>collections</code>之前，先来了解一下<code>module</code>和<code>package</code>的概念（这是小杨一直想要了解的东西，这次一定要学会/(ㄒoㄒ)/~~）</p><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p>简单理解就是一个<code>module_name.py</code>文件，可以被其他文件通过<code>import</code>进行使用。</p><pre><code class="python"># import the libraryimport math#Using it for taking the logmath.log(10)2.302585092994046</code></pre><p>Python语言本身就自带了成千上万个module了，可以通过<a href="https://docs.python.org/3/py-modindex.html">这里</a>查看。其中，每个module都有两个built-in函数——<code>dir()</code>和<code>help()</code>。</p><ul><li><p><code>dir()</code>的作用是返回这个module里面包含的所有方法的方法名。</p><p><code>print(dir(math))</code></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp5lg2r2t0j20n802kmxo.jpg"></p></li><li><p><code>help()</code>用于显示某个function的含义</p><p><code>help(math.factorial)</code></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp5lhie7flj20h603mt8v.jpg"></p></li></ul><h3 id="Package"><a href="#Package" class="headerlink" title="Package"></a>Package</h3><p>package就是一堆“打包起来”的相关的module。例如Numpy和Scipy就是非常常见的机器学习package，它们里面有成千上万个modules。下图是Scipy中包含的sub-packages：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp5lkqbpeej20bv0a6my3.jpg"></p><h2 id="Collections-Module"><a href="#Collections-Module" class="headerlink" title="Collections Module"></a>Collections Module</h2><p>Collections是一个built-in Python module，用于对Python built-in containers（比如dict，list，set，tuple）进行扩展。</p><p>一些有用的data structures如下：</p><h3 id="1-namedtuple"><a href="#1-namedtuple" class="headerlink" title="1. namedtuple"></a>1. namedtuple</h3><p>人如其名，命名tuple。给tuple的每个index指定一个名字，联想一下数据库中的tuple~</p><pre><code class="python">from collections import namedtuplefruit = namedtuple('fruit','number variety color')guava = fruit(number=2,variety='HoneyCrisp',color='green')apple = fruit(number=5,variety='Granny Smith',color='red')</code></pre><h3 id="2-Counter"><a href="#2-Counter" class="headerlink" title="2. Counter"></a>2. Counter</h3><p>用到再说</p><h3 id="3-defaultdict"><a href="#3-defaultdict" class="headerlink" title="3. defaultdict"></a>3. defaultdict</h3><p>The point of difference is that <code>defaultdict</code> takes the first argument (default_factory) as a default data type for the dictionary.</p><p>在传统dict下执行下面的代码：</p><pre><code class="python">d = {}print(d['A'])</code></pre><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp5mhjg42tj20hj03kdg0.jpg"></p><p>使用defaultdict：</p><pre><code class="python">from collections import defaultdictd = defaultdict(object)print(d['A'])&lt;object object at 0x7fc9bed4cb00&gt;</code></pre><p>发现了吗？当访问不存在的key时，dict会报错，但是defaultdict会自动创建一个默认类型的空对象。</p><h3 id="4-OrderedDict"><a href="#4-OrderedDict" class="headerlink" title="4. OrderedDict"></a>4. OrderedDict</h3><p>相比较传统的dict，OrderedDict的特点体现在一个<code>Ordered</code>上，这里的Ordered是指：它能够记得key插入的顺序，当你访问它的key时，它不会乱序返回，而是按照插入时的次序返回。</p><ul><li><strong>regular dictionary</strong></li></ul><pre><code class="python">d = {'banana': 3, 'apple': 4, 'pear': 1, 'orange': 2}</code></pre><ul><li><strong>dictionary sorted by key</strong></li></ul><pre><code class="python">OrderedDict(sorted(d.items(), key=lambda t: t[0]))OrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])</code></pre><ul><li><strong>dictionary sorted by value</strong></li></ul><pre><code class="python">OrderedDict(sorted(d.items(), key=lambda t: t[1]))OrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])</code></pre><ul><li><strong>dictionary sorted by the length of the key string</strong></li></ul><pre><code class="python">OrderedDict(sorted(d.items(), key=lambda t: len(t[0])))OrderedDict([('pear', 1), ('apple', 4), ('banana', 3), ('orange', 2)])</code></pre><blockquote><p><em>A point to note here is that in Python 3.6, the regular dictionaries are</em> <strong>insertion ordered i.e</strong> <em>dictionaries remember the order of items inserted. Read the discussion</em> <a href="https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6"><em>here</em></a><em>.</em></p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>加油(ง •_•)ง</p><p>学到一个新的词组——general purpose，通用的。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://towardsdatascience.com/pythons-collections-module-high-performance-container-data-types-cb4187afb5fc">Python’s Collections Module — High-performance container data types.</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习中的logits是什么？</title>
      <link href="/2021/04/02/shen-du-xue-xi-zhong-de-logits-shi-shi-me/"/>
      <url>/2021/04/02/shen-du-xue-xi-zhong-de-logits-shi-shi-me/</url>
      
        <content type="html"><![CDATA[<p>在深度学习编码的过程中，常常会遇见一些变量名叫做<code>logits</code>，这个<code>logits</code>到底指代了一个什么东西呢？查阅资料之后，我在Google的<a href="https://developers.google.com/machine-learning/glossary">machine learning</a>文档中找到了定义：</p><blockquote><h2 id="Logits"><a href="#Logits" class="headerlink" title="Logits"></a>Logits</h2><p>The vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. If the model is solving a <a href="https://developers.google.com/machine-learning/glossary#multi-class"><strong>multi-class classification</strong></a> problem, logits typically become an input to the <a href="https://developers.google.com/machine-learning/glossary#softmax"><strong>softmax</strong></a> function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class.</p><p>In addition, logits sometimes refer to the element-wise inverse of the <a href="https://developers.google.com/machine-learning/glossary#sigmoid_function"><strong>sigmoid function</strong></a>. For more information, see <a href="https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits">tf.nn.sigmoid_cross_entropy_with_logits</a>.</p></blockquote><p>代码实例如下：</p><p>暂时没有实例。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch里的CrossEntropyLoss详解</title>
      <link href="/2021/04/02/pytorch-li-de-crossentropyloss-xiang-jie/"/>
      <url>/2021/04/02/pytorch-li-de-crossentropyloss-xiang-jie/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在使用Pytorch时经常碰见这些函数<code>cross_entropy</code>，<code>CrossEntropyLoss</code>, <code>log_softmax</code>, <code>LogSoftmax</code>。看得我头大，所以整理本文以备日后查阅。</p></blockquote><p>首先要知道上面提到的这些函数一部分是来自于<strong>torch.nn</strong>,而另一部分则来自于<strong>torch.nn.functional</strong>(常缩写为<strong>F</strong>）。二者函数的区别可参见 <a href="https://www.zhihu.com/question/66782101">知乎:torch.nn和funtional函数区别是什么？</a></p><p>下面是对与cross entropy有关的函数做的总结：</p><table><thead><tr><th align="left">torch.nn</th><th align="left">torch.nn.functional (F)</th></tr></thead><tbody><tr><td align="left">CrossEntropyLoss</td><td align="left">cross_entropy</td></tr><tr><td align="left">LogSoftmax</td><td align="left">log_softmax</td></tr><tr><td align="left">NLLLoss</td><td align="left">nll_loss</td></tr></tbody></table><p>下面将主要介绍torch.nn.functional中的函数为主,torch.nn中对应的函数其实就是对F里的函数进行包装以便管理变量等操作。</p><p>在介绍<code>cross_entropy</code>之前先介绍两个基本函数：</p><h2 id="log-softmax"><a href="#log-softmax" class="headerlink" title="log_softmax"></a>log_softmax</h2><p>这个很好理解，其实就是<strong>log</strong>和<strong>softmax</strong>合并在一起，同时执行。</p><h2 id="nll-loss"><a href="#nll-loss" class="headerlink" title="nll_loss"></a>nll_loss</h2><p>该函数的全称是<strong>negative log likelihood loss</strong>，函数表达式为<br>$$<br>f(x,class)=−x[class]f(x,class)=−x[class]<br>$$<br>例如假设$x=[1,2,3],class=2$,那么$f(x,class)=−x[2]=−3$。</p><h2 id="cross-entropy"><a href="#cross-entropy" class="headerlink" title="cross_entropy"></a>cross_entropy</h2><p>交叉熵的计算公式为：<br>$$<br>cross_entropy=-\sum_{k=1}^{N}\left(p_{k} * \log q_{k}\right)<br>$$<br>其中$p$表示真实值，在这个公式中是one-hot形式；$q$是预测值，在这里假设已经是经过softmax后的结果了。</p><p>仔细观察可以知道，因为$p$的元素不是0就是1，而且又是乘法，所以很自然地我们如果知道1所对应的index，那么就不用做其他无意义的运算了。所以在PyTorch代码中<code>target</code>不是以one-hot形式表示的，而是直接用scalar表示。所以交叉熵的公式(<strong>m表示真实类别</strong>)可变形为：<br>$$<br>cross_entropy=-\sum_{k=1}^{N}\left(p_{k} * \log q_{k}\right)=-log , q_m<br>$$<br>仔细看看，是不是就是等同于<strong>log_softmax</strong>和<strong>nll_loss</strong>两个步骤。</p><p>所以Pytorch中的<strong>F.cross_entropy</strong>会自动调用上面介绍的<strong>log_softmax</strong>和<strong>nll_loss</strong>来计算交叉熵,其计算方式如下:<br>$$<br>\operatorname{loss}(x, \text {class})=-\log \left(\frac{\exp (x[\operatorname{class}])}{\sum_{j} \exp (x[j])}\right)<br>$$<br>代码示例：</p><pre><code class="python">input = torch.randn(3, 5, requires_grad=True)target = torch.randint(5, (3,), dtype=torch.int64)loss = F.cross_entropy(input, target)loss.backward()</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Focal Loss——从直觉到实现</title>
      <link href="/2021/03/29/focal-loss-cong-zhi-jue-dao-shi-xian/"/>
      <url>/2021/03/29/focal-loss-cong-zhi-jue-dao-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="Focal-Loss——从直觉到实现"><a href="#Focal-Loss——从直觉到实现" class="headerlink" title="Focal Loss——从直觉到实现"></a>Focal Loss——从直觉到实现</h1><h2 id="问题（Why？）"><a href="#问题（Why？）" class="headerlink" title="问题（Why？）"></a>问题（Why？）</h2><p>做机器学习分类问题，难免遇到<strong>Biased-Data-Problem</strong>，例如</p><ul><li>CV的目标检测问题: 绝大多数检测框里都是 backgroud</li><li>NLP的异常文本检测: 绝大多数文本都是 normal</li></ul><p>对此，以下套路可以缓解：</p><ul><li>升/降采样, 或者调整样本权重</li><li><em>换个更鲁棒的loss函数</em> ，或者加正则</li><li>集成模型: Bagging，RandomForest …</li><li>利于外部先验知识：<strong>预训练 + 微调</strong></li><li><strong>多任务联合学习</strong>（multi-task，joint learning）</li><li>… （以上概念纯属经验总结，既不完备也不互斥）</li></ul><p>今天要聊的就是一种针对该问题精心设计的loss函数——<code>Focal Loss</code>。</p><h2 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h2><p>先来回顾一下常用的 <code>BinaryCrossEntropyLoss</code> 公式如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp0vuq52sdj20gm0790tc.jpg"></p><p>不难看出，CE是个“笨学生”。</p><p>考前复习的时候，<strong>他不会划重点，对所有知识点 “一视同仁”</strong>。</p><p>如果教科书上有100道例题，包括: 90道加减乘除 + 10道 三角函数。CE同学就会吭哧吭哧的“平均用力”反复练习这100道例题，结果可想而知——他会精通那90道个位数加减乘除题目，然后其他题目基本靠蒙。那10道他不会的题，往往还是分值高的压轴题。</p><p>嗯，大概就是这么个症状。</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>机智如你，想必已经有办法了 —— <strong>给他指个方向，别再“平均用力”就好了</strong></p><h3 id="方法一、分科复习"><a href="#方法一、分科复习" class="headerlink" title="方法一、分科复习"></a>方法一、分科复习</h3><blockquote><p>每个【科目】的难度是不同的； 你要花 30%的精力在四则运算，70%的精力在三角函数。 — 老师告诉CE同学 第一个技巧</p></blockquote><p>对应到公式中，就是针对每个类别赋予不同的权重，即下述$\alpha_{t}$：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp0vz4se45j20by02w3yi.jpg"></p><p>这是个简单粗暴有效的办法。</p><h3 id="方法二、刷题战术"><a href="#方法二、刷题战术" class="headerlink" title="方法二、刷题战术"></a>方法二、刷题战术</h3><blockquote><p>每道【题目】的难度是不同的； 你要根据以往刷类似题时候的正确率来合理分配精力。<br>— 老师告诉CE同学 第二个技巧</p></blockquote><p>观察CE中的$p_{t}$，它反映了模型对这个样本的识别能力（即 “这个知识点掌握得有多好”）；显然，对于$p_t$越大的样本，我们越要打压它对loss的贡献。</p><p>FL是这么定义的：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp0w32679fj20b202zglm.jpg"></p><p>这里有个超参数$\gamma$; 直观来看，$\gamma$越大，打压越重。如下图所示：</p><ul><li>横轴是$p_t$, 纵轴是$FL(p_t)$</li><li>总体来说，所有曲线都是单调下降的，即 “掌握越好的知识点越省力”</li><li>当 $\gamma = 0$ 时，FL退化成CE，即蓝色线条</li><li>当 $\gamma$ 很大时，线条逐步压低到绿色位置，即各样本对于总loss的贡献受到打压；中间靠右区段承压尤其明显</li></ul><h3 id="方法三、综合上述两者"><a href="#方法三、综合上述两者" class="headerlink" title="方法三、综合上述两者"></a>方法三、综合上述两者</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gp0w7wqmgtj20f302waa7.jpg"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><h4 id="Keras实现"><a href="#Keras实现" class="headerlink" title="Keras实现"></a>Keras实现</h4><pre><code class="python">from keras import backend as Kdef focal_loss(alpha=0.75, gamma=2.0):    """ 参考 https://blog.csdn.net/u011583927/article/details/90716942 """    def focal_loss_fixed(y_true, y_pred):        # y_true 是个一阶向量, 下式按照加号分为左右两部分        # 注意到 y_true的取值只能是 0或者1 (假设二分类问题)，可以视为“掩码”        # 加号左边的 y_true*alpha 表示将 y_true中等于1的槽位置为标量 alpha        # 加号右边的 (ones-y_true)*(1-alpha) 则是将等于0的槽位置为 1-alpha        ones = K.ones_like(y_true)        alpha_t = y_true*alpha + (ones-y_true)*(1-alpha)        # 类似上面，y_true仍然视为 0/1 掩码        # 第1部分 `y_true*y_pred` 表示 将 y_true中为1的槽位置为 y_pred对应槽位的值        # 第2部分 `(ones-y_true)*(ones-y_pred)` 表示 将 y_true中为0的槽位置为 (1-y_pred)对应槽位的值        # 第3部分 K.epsilon() 避免后面 log(0) 溢出        p_t = y_true*y_pred + (ones-y_true)*(ones-y_pred) + K.epsilon()        # 就是公式的字面意思        focal_loss = -alpha_t * K.pow((ones-p_t),gamma) * K.log(p_t)    return focal_loss_fixedmodel = ...model.compile(..., loss=focal_loss(gamma=3, alpha=0.5))</code></pre><h4 id="PyTorch二分类"><a href="#PyTorch二分类" class="headerlink" title="PyTorch二分类"></a>PyTorch二分类</h4><pre><code class="python">class BCEFocalLoss(torch.nn.Module):    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):        super(BCEFocalLoss, self).__init__()        self.gamma = gamma        self.alpha = alpha        self.reduction = reduction    def forward(self, predict, target):        pt = torch.sigmoid(predict) # sigmoide获取概率        #在原始ce上增加动态权重因子，注意alpha的写法，下面多类时不能这样使用        loss = - self.alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - (1 - self.alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)        if self.reduction == 'mean':            loss = torch.mean(loss)        elif self.reduction == 'sum':            loss = torch.sum(loss)        return loss</code></pre><h4 id="PyTorch多分类"><a href="#PyTorch多分类" class="headerlink" title="PyTorch多分类"></a>PyTorch多分类</h4><pre><code class="python">class MultiCEFocalLoss(torch.nn.Module):    def __init__(self, class_num, gamma=2, alpha=None, reduction='mean'):        super(MultiCEFocalLoss, self).__init__()        if alpha is None:            self.alpha = Variable(torch.ones(class_num, 1))        else:            self.alpha = alpha        self.gamma = gamma        self.reduction = reduction    def forward(self, predict, target):        pt = F.softmax(predict, dim=1) # softmmax获取预测概率        class_mask = F.one_hot(target, 5) #获取target的one hot编码        ids = target.view(-1, 1)         alpha = self.alpha[ids.data.view(-1)] # 注意，这里的alpha是给定的一个list(tensor#),里面的元素分别是每一个类的权重因子        probs = (pt * class_mask).sum(1).view(-1, 1) # 利用onehot作为mask，提取对应的pt        log_p = probs.log()# 同样，原始ce上增加一个动态权重衰减因子        loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p        if self.reduction == 'mean':            loss = loss.mean()        elif self.reduction == 'sum':            loss = loss.sum()        return loss</code></pre><p><strong>onehot</strong></p><p>也可以用下面三行代码自己实现onehot</p><pre><code class="python">ids = target.view(-1, 1)onehot =torch.zeros_like(P)onehot.scatter_(1, ids.data, 1.)</code></pre><h2 id="调参经验"><a href="#调参经验" class="headerlink" title="调参经验"></a>调参经验</h2><ul><li>$\alpha \in (0,1)$反映了“方法一、分科复习”时，各科目的难度比率；<ul><li>二分类场景下，类似于正例的<code>sample_weight</code>概念，可以按照样本占比，适度加权</li><li>e.g. 设有5条正例、95条负例，则建议 $\alpha = 0.95$</li><li>取 $\alpha = 0.5$ 相当于关掉该功能</li></ul></li><li>$\gamma \in [0,+\infty)$ 反映了 “方法二、刷题战术”时，对于难度的区分程度<ul><li>取 $\gamma = 0$ 相当于关掉该功能; 即不考虑难度区别，一视同仁</li><li>$\gamma$ 越大，则越重视难度，即专注于比较困难的样本。建议在 $(0.5,10.0)$ 范围尝试</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>机器学习分类问题中，各类别样本数差距悬殊是很常见的情况；这会干扰模型效果</li><li>通过将CrossEntropyLoss替换为综合版的FocalLoss，可以有效缓解上述问题</li><li>具体思路就是引入两个额外的变量来区分对待每个样本<ul><li>$\alpha_t$根据类别加权</li><li>$(1-p_t)^{\gamma}$根据难度加权</li></ul></li><li>代码实现很简单、调参也不复杂，详见上文</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/103623160">Focal Loss — 从直觉到实现</a></li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1708.02002.pdf">Focal Loss for Dense Object Detection</a></li><li><a href="https://link.zhihu.com/?target=https://towardsdatascience.com/demystifying-focal-loss-i-a-more-focused-version-of-cross-entropy-loss-f49e4b044213">Demystifying Focal Loss I: A More Focused Cross Entropy Loss</a></li><li><a href="https://zhuanlan.zhihu.com/p/49981234">Focal loss论文详解</a></li><li><a href="https://link.zhihu.com/?target=https://blog.csdn.net/MrR1ght/article/details/93649259">Focal loss 原理及keras实</a></li><li><a href="https://zhuanlan.zhihu.com/p/308290543">PyTorch代码实现</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 损失函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Extensively Matching for Few-shot Learning Event Detection</title>
      <link href="/2021/03/27/extensively-matching-for-few-shot-learning-event-detection/"/>
      <url>/2021/03/27/extensively-matching-for-few-shot-learning-event-detection/</url>
      
        <content type="html"><![CDATA[<h1 id="Extensively-Matching-for-Few-shot-Learning-Event-Detection"><a href="#Extensively-Matching-for-Few-shot-Learning-Event-Detection" class="headerlink" title="Extensively Matching for Few-shot Learning Event Detection"></a>Extensively Matching for Few-shot Learning Event Detection</h1><blockquote><p>2020 ACL《用于小样本学习事件检测的广泛匹配》<a href="https://www.aclweb.org/anthology/2020.nuse-1.5.pdf">(Extensively Matching for Few-shot Learning Event Detection)</a> 的阅读笔记</p></blockquote><h2 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h2><ol><li>目前典型的事件检测方法是基于特征工程的传统监督学习和神经网络，但是监督学习模型在处理未知类别的事件时效率较差，通常使用标注、再训练的方法，其代价较大。</li><li>One potential problem of prior FSL methods is that the model relies solely on training signals<br>between query instance and the support set. Thus, the matching information between samples in the support set has not been exploited yet.这句话的意思其实就是说以往的few-shot只依赖于下文中提到的$L_{query}$，没有引入$\hat {L}<em>{intra}$和$\hat {L}</em>{inter}$。</li></ol><h2 id="2-Contributions"><a href="#2-Contributions" class="headerlink" title="2. Contributions"></a>2. Contributions</h2><ol><li><p>第一次将ED问题定义为一个few-shot learning问题；</p></li><li><p>对Loss Function进行增强，引入了两种matching information：</p><ol><li>matching information between query instance and the support set；</li><li>matching information between the samples in the support themselves；</li></ol><p>文章中把这两种matching information叫做<strong>training signals</strong></p></li><li><p>提出的两个<strong>training signals</strong>效果显著，能够用于任何<code>metric-based FSL models</code>。</p></li></ol><h2 id="3-Terminology"><a href="#3-Terminology" class="headerlink" title="3. Terminology"></a>3. Terminology</h2><h3 id="Few-shot-learning"><a href="#Few-shot-learning" class="headerlink" title="Few shot learning"></a>Few shot learning</h3><p>In FSL, a trained model rapidly learns a new concept from a few examples while keeping great generalization from observed examples. Hence, if we need to extend event detection into a new<br>domain, a few examples are needed to activate the system in the new domain without retraining the<br>model. By formulating ED as FSL, we can significantly reduce the annotation cost and training cost<br>while maintaining highly accurate results.</p><p>How to do few shot learning ?</p><p><strong>In a few shot learning iteration</strong>, the model is given <strong>a support set and a query instance</strong>. The support set consists of examples from a small set of classes. <strong>A model needs to predict the label of the query instance in accordance with the set of classes appeared in the support set.</strong> </p><h2 id="4-Models"><a href="#4-Models" class="headerlink" title="4. Models"></a>4. Models</h2><h3 id="Event-Detection-as-Few-shot-Learning"><a href="#Event-Detection-as-Few-shot-Learning" class="headerlink" title="Event Detection as Few-shot Learning"></a>Event Detection as Few-shot Learning</h3><p>通常FSL模型都会采用 N-way K-shot的范式预测query instance。作者在这里增加了1个新的类别$NULL$，从而变成了<strong>（N+1）-way K-shot</strong>的求解范式。</p><p>支撑集（Support Set）中添加NULL类 <strong>N+1-way K-shot</strong>，如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1goyiwun9abj20d203k3yl.jpg"></p><ul><li>$(t_{1},…,t_{N})$: positive labels</li><li>$t_{null}$: a special label for non-event</li></ul><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><h4 id="Instance-Encoder"><a href="#Instance-Encoder" class="headerlink" title="Instance Encoder"></a>Instance Encoder</h4><p>就是对句子s里面的每个word进行word embedding。</p><p>随后对整个句子s，采用一些神经网络（CNN，LSTM，GCN），从句子s的word中，得到句子s的表示。</p><h4 id="Prototype-Encoder"><a href="#Prototype-Encoder" class="headerlink" title="Prototype Encoder"></a>Prototype Encoder</h4><p><strong>This module computes a representative vector, called prototype.</strong></p><p>有两种方法获得，一种方法是暴力平均，另外一种方法是加权平均（需要用到注意力机制）。</p><h4 id="Classification-Module"><a href="#Classification-Module" class="headerlink" title="Classification Module"></a>Classification Module</h4><p>计算公式如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1goysd24vtcj20e002gglo.jpg"></p><p>其中的函数$d()$可以有四种选择：</p><ol><li>Cosine similarity with averaging prototype as <strong>Matching</strong> network</li><li>Euclidean distance with averaging prototype as <strong>Proto</strong> network</li><li>Euclidean distance with weighted sum prototype as <strong>Proto+Att</strong> network</li><li>Learnable distance function with averaging prototype as <strong>Relation</strong> network</li></ol><h3 id="Training-Objectives"><a href="#Training-Objectives" class="headerlink" title="Training Objectives"></a>Training Objectives</h3><p>利用<strong>查询实例和支撑集之间</strong>与<strong>支撑集内样本自身之间</strong>的匹配信息来训练ED模型，可以显著减少标注和训练代价，同时维持高准确率。具体的方法是通过在损失函数中添加辅助参数来抑制学习过程。</p><ul><li><p>最大似然估计值<br>$$<br>L_{query}(x,S)=-logP(y=t|x,S) \tag{1}<br>$$<br>where $x$,$t$,$S$ are query instance,ground true label,and support set</p></li><li><p>Intra-cluster matching</p><p>相同类之间的向量是类似的，因此最小化它们的间距<br>$$<br>L_{intra}=\sum\limits_{i=1}^{N}\sum\limits_{k=1}^{K}\sum\limits_{j=k+1}^{K}mse(v_{i}^{j},v_{i}^{k}) \tag{2}<br>$$</p></li><li><p>Inter-cluster information</p><p>最大化类之间的距离<br>$$<br>L_{inter}=1-\sum\limits_{i=1}^{N}\sum\limits_{j=i+1}^{N}cosine(c_{i},c_{j}) \tag{3}<br>$$</p></li><li><p>损失函数</p><p>由（1）、（2）、（3）</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1goyix5ygbqj209801nt8j.jpg"></p></li></ul><h2 id="三、实验"><a href="#三、实验" class="headerlink" title="三、实验"></a>三、实验</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1goyiqicnbdj20no0dv0uk.jpg"></p><ul><li>表1显示：<ul><li>5+1-Way 5-shot的表现总是优于10+1-Way 10-shot，因为后者中需要被分类的类的数量是前者的2倍之多</li><li>Proto和Proto+Att模型的表现均最好</li><li>在10+1-Way 10-shot中Proto+Att的表现略好于Proto</li></ul></li><li>表2显示：<ul><li>使用给出的损失函数后，所有的神经网络模型中F都明显提高了</li></ul></li></ul><h2 id="四、消融实验"><a href="#四、消融实验" class="headerlink" title="四、消融实验"></a>四、消融实验</h2><p><img src="https://cdn.jsdelivr.net/gh/ZHEvent/ZHEvent.github.io@master/images/blog/few-shot-learning-event-detection-2.jpg"></p><p>上表显示了各个模型中未加入损失函数、只加入Inter、只加入Intra和同时加入Inter和Intra的结果，表明这两个损失函数对于结果都有明显的提升，且缺失任何一个，都会对结果造成较大精度损失。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few shot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>刷爆3路榜单，信息抽取冠军方案分享：嵌套NER+关系抽取+实体标准化</title>
      <link href="/2021/03/25/shua-bao-3-lu-bang-dan-xin-xi-chou-qu-guan-jun-fang-an-fen-xiang-qian-tao-ner-guan-xi-chou-qu-shi-ti-biao-zhun-hua/"/>
      <url>/2021/03/25/shua-bao-3-lu-bang-dan-xin-xi-chou-qu-guan-jun-fang-an-fen-xiang-qian-tao-ner-guan-xi-chou-qu-shi-ti-biao-zhun-hua/</url>
      
        <content type="html"><![CDATA[<p>本篇文章，JayJay并不想只是围绕竞赛本身谈策略，而是想和大家一起交流：无论在竞赛还是落地中，<strong>信息抽取任务的稳定提升策略有哪些</strong>？总的来看就是两点：</p><ol><li><strong>构建一个强大的baseline，这取决于标注框架的选择</strong>；</li><li><strong>套路化的辅助策略，稳定迭代并提升</strong>；</li></ol><p>为更好地展示关键内容，本文以QA形式探讨了以下问题：</p><blockquote><p><strong>Q1:如何构建强大的baseline？万能的4种标注框架供你选择！</strong><br><strong>Q2:如何解决复杂NER问题：嵌套/非连续/类型易混淆？</strong><br><strong>Q3:升级的NER竞赛：如何解决嵌套实体抽取？</strong><br><strong>Q4:贴合真实场景的NER竞赛：如何解决不完全标注NER？</strong><br><strong>Q5:关系抽取一片红海，如何魔改标注框架？如何突破SOTA：暴漏偏差/独立编码/pipeline？</strong><br><strong>Q6:如何登顶关系抽取冠军宝座：强大的标注策略+词汇增强/对抗训练/远程监督/假阴性降噪/交替训练？</strong><br><strong>Q7:NER的最后一步：负样本为王，实体标准化！</strong></p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-e04970f10dabf8f24e59d4e4ef673fe8_720w.jpg" alt="思维导图"></p><blockquote><p>相关代码后续会在 <strong>DeepIE: <a href="https://link.zhihu.com/?target=https://github.com/loujie0822/DeepIE">https://github.com/loujie0822/DeepIE</a></strong> 开源，尽情关注～</p></blockquote><h2 id="Q1-如何构建强大的baseline？万能的4种标注框架供你选择！"><a href="#Q1-如何构建强大的baseline？万能的4种标注框架供你选择！" class="headerlink" title="Q1:如何构建强大的baseline？万能的4种标注框架供你选择！"></a>Q1:如何构建强大的baseline？万能的4种标注框架供你选择！</h2><p>谈到标注框架，NLPer首先想到的就是序列标注，而如今我们面临早已不是一个简单抽取问题，序列标注已经无法“胜任”了：例如，在医疗抽取任务中，我们常常会遇到<strong>嵌套、非连续、类型混淆、信息块重叠、关系重叠</strong>等复杂抽取问题。</p><p>因此，掌握标注框架（解码方式）是解决信息抽取问题的第一步，也是构建强大baseline的关键一步：试想一下，如果你的标注框架都不能<strong>完备解码</strong>（gold输入，输出指标也应该达到或接近100%），不能cover绝大部分case情况，又何谈下一步优化提升呢？</p><p>JayJay这里归纳了4种“<strong>易于上手</strong>”的标注框架：</p><h3 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h3><p>每个序列位置都被标注为一个标签，比如按照BILOU标注，我们常用<strong>MLP或CRF解码</strong>。</p><h3 id="指针标注"><a href="#指针标注" class="headerlink" title="指针标注"></a>指针标注</h3><p>对每个span的start和end进行标记，对于多片段抽取问题转化为N个2分类（N为序列长度），如果涉及多类别可以转化为层叠式指针标注（C个指针网络，C为类别总数）。事实上，<strong>指针标注已经成为统一实体、关系、事件抽取的一个“大杀器”</strong>。</p><h3 id="多头标注"><a href="#多头标注" class="headerlink" title="多头标注"></a>多头标注</h3><p>对每个token pair进行标记，其实就是构建一个 <img src="https://www.zhihu.com/equation?tex=+%5Ctimes+%5Ctimes+" alt="[公式]"> 的分类矩阵，可以用于实体或关系抽取。其重点就是如何强有力的表征构建分类矩阵。事实上，<strong>多头标注成为了众多实体和关系抽取SOTA的首选利器</strong>！（PS：多头标注是JayJay自己叫的，单纯是为了纪念多头选择机制的关系抽取论文[<a href="https://zhuanlan.zhihu.com/p/326302618#ref_1">1]</a>）</p><h3 id="片段排列"><a href="#片段排列" class="headerlink" title="片段排列"></a>片段排列</h3><p>源于Span-level NER[<a href="https://zhuanlan.zhihu.com/p/326302618#ref_2">2]</a>的思想，枚举所有可能的span进行分类，同序列长度进行解耦，可以更加灵活地处理复杂抽取和低资源问题。事实上，<strong>片段排列的思想已经被Google</strong>[<a href="https://zhuanlan.zhihu.com/p/326302618#ref_3">3]</a><strong>推崇并统一了信息抽取各个子任务</strong>。</p><p>掌握上述4种标注框架后，我们就可以根据具体抽取任务、灵活地应用于实体、关系、事件抽取等场景中了（PS：对于一些生成式的标注框架，JayJay感觉不够稳定，就不再单独介绍了）。</p><h2 id="Q2-如何解决复杂NER问题：嵌套-非连续-类型易混淆？"><a href="#Q2-如何解决复杂NER问题：嵌套-非连续-类型易混淆？" class="headerlink" title="Q2:如何解决复杂NER问题：嵌套/非连续/类型易混淆？"></a>Q2:如何解决复杂NER问题：嵌套/非连续/类型易混淆？</h2><p>在实际业务场景中的NER问题可能与你想的不太一样，比如下图中的复杂NER问题你遇到过吗？</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowhqgkm41j20mk05vaat.jpg" alt="undefined"></p><p>本文这里给出了上述复杂NER问题的简单解决方案：</p><ul><li><strong>嵌套NER</strong>：例如在span「呼吸中枢受累」中，存在两个实体嵌套：「症状：呼吸中枢受累」和「部位：呼吸中枢」。我们在Q3中具体介绍解决方案。</li><li><strong>非连续NER</strong>：例如在span「尿道、膀胱、肾绞痛」中存在三个非连续实体「尿道痛」、「膀胱痛」、「肾绞痛」。这里给出3种解决方案：<ul><li>继续当作序列标注任务：拓展BIO标签；</li><li>转化为一个属性/关系抽取问题：由于病历文本趋向模板化，所以用规则提取更加便捷。</li><li>模仿句法解析器的做法，设置shift-reduce parser，具体可参见ACL20的这篇paper。</li></ul></li><li><strong>类型易混淆NER</strong>：例如对于部位实体「左肺上叶」，其归属于「病理」还是「影像」模块呢？对于「纵隔」部位，是属于「肿瘤」还是「淋巴结」部位呢？这里给出2种解决方案：<ul><li><strong>事件论元抽取</strong>：对于医疗领域，不同于通常的论元抽取，因为电子病历一般不存在<strong>信息块重叠</strong>（事件类型交叉重叠）问题，因此可以先进行<strong>事件段落抽取</strong>，再将「左肺上叶」部位实体归属到当前的事件段落中。</li><li><strong>两阶段NER</strong>：在同一事件类型中，第一阶段可以确定<strong>实体span边界</strong>（例如找到部位实体「纵隔」的边界），第二阶段再结合上下文信息进行<strong>实体typing</strong>（例如对「纵隔」进行性质判断），这样做指标通常会提高哦～</li></ul></li></ul><h2 id="Q3-升级的NER竞赛：如何解决嵌套实体抽取？"><a href="#Q3-升级的NER竞赛：如何解决嵌套实体抽取？" class="headerlink" title="Q3:升级的NER竞赛：如何解决嵌套实体抽取？"></a>Q3:升级的NER竞赛：如何解决嵌套实体抽取？</h2><p>既然实际业务中的NER问题是复杂的，NER竞赛也不应该循规蹈矩了～这不，CHIP2020的中文医学实体抽取评测就是一个嵌套实体抽取问题，数据集包含504种常见的儿科疾病、7,085种身体部位、12,907种临床表现等九大类医学实体。</p><p>对于嵌套实体抽取这个任务，我们直接套用<strong>Q1</strong>中的4种万能标注框架就可以解决了：</p><ul><li><strong>序列标注</strong>：<ul><li><strong>多标签分类</strong>。如下图（a）所示，将多分类转化为多标签分类，即使用sigmoid设定阈值进行解码；这种方式的学习难度较大，也会容易导致label之间依赖关系的缺失；</li><li><strong>合并标签层</strong>。如下图（b）所示，依然采用CRF，但设置多个标签层，对于每一个token给出其所有的label，然后将所有标签层合并。这种方式指数级增加了标签，对于多层嵌套，稀疏问题较为棘手；</li></ul></li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowhzx6eajj20lq07rwfg.jpg"></p><ul><li><strong>指针标注</strong>：<ul><li><strong>层叠式指针标注</strong>：即设置C个指针网络，如下图（c）所示。</li><li><strong>MRC-QA + 层叠式指针标注</strong>：构建query问题指代所要抽取的实体类型，同时也引入了先验语义知识，如下图（d）所示。在文献中就对不同实体类型构建query，并采取指针标注，此外也构建了$N \times N$矩阵来判断span是否构成一个实体mention。</li></ul></li></ul><p><img src="https://pic4.zhimg.com/80/v2-bb7070a4f8131576ea5eb1374d8210eb_720w.jpg"></p><ul><li><strong>多头标注</strong><ul><li>构建一个$N \times N \times C$的Span矩阵，如下图（e）所示，Span{呼}{枢}=1，代表「呼吸中枢」是一个部位实体；Span{呼}{累}=2，代表「呼吸中枢受累」是一个症状实体；对于多头标注的一个重点就是如何<strong>构造Span矩阵</strong>、以及<strong>解决0-1标签稀疏问题</strong>。</li><li><strong>嵌套实体的2篇SOTA之作</strong>： ACL20的《Named Entity Recognition as Dependency Parsing》采取Biaffine机制构造Span矩阵；EMNLP20的<strong>HIT</strong>[<a href="https://zhuanlan.zhihu.com/p/326302618#ref_4">4]</a>则通过Biaffine机制专门捕获边界信息，并采取传统的序列标注任务强化嵌套结构的内部信息交互，同时采取focal loss来解决0-1标签不平衡问题。</li></ul></li></ul><p><img src="https://pic2.zhimg.com/80/v2-a1677bec536477c83e93a14f2c452ee9_720w.jpg"></p><ul><li><p><strong>片段排列</strong></p><ul><li><p>十分直接，如下图（f）所示。对于含T个token的文本，理论上共有$N = T(T+1)/2$种片段排列。如果文本过长，会产生大量的负样本，在实际中需要限制span长度并合理削减负样本。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowih1f01sj20am0dgmxn.jpg" alt="undefined"></p></li></ul></li></ul><p>在CHIP20嵌套实体评测中，我们对比了不同标注策略下的效果（如下图），可以发现：多头标注效果最佳！</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowin28938j20k006xjrv.jpg" alt="undefined"></p><h2 id="Q4-贴合真实场景的NER竞赛：如何解决不完全标注NER？"><a href="#Q4-贴合真实场景的NER竞赛：如何解决不完全标注NER？" class="headerlink" title="Q4:贴合真实场景的NER竞赛：如何解决不完全标注NER？"></a>Q4:贴合真实场景的NER竞赛：如何解决不完全标注NER？</h2><p>标注资源少、如何降低标注量一直是真实工业场景中必须面对的问题，不同于分类任务，大规模的实体标注数据集的构建成本更高。BERT的出现本身就是一种降低标注量的方式，此外，文本增强等方式也可降低标注（PS：NER等序列标注任务的数据增强方式可能要独立适配会更好，采用常见的增强方式效果提升不明显～）。</p><p>那么，有没有一种仅仅通过积累的实体词典、来匹配标注数据的方式，这样可以不用大规模的进行人工标注了。这种方式，可以统称为「<strong>不完全标注NER问题</strong>」：这种方式最为突出一点就是漏标情况严重，而<strong>NER序列标注的方式对噪声（漏标）十分敏感</strong>。（事实上，人工标注中也会存在漏标等情况）</p><p>CHIP20的评测六-中药说明书实体识别挑战(<a href="https://link.zhihu.com/?target=http://cips-chip.org.cn/2020/eval6">http://cips-chip.org.cn/2020/eval6</a>)就是对这一问题的评测。由于这个评测正在答辩环节中，JayJay也进入最后答辩了，具体方案等成绩揭晓后再与大家分享吧～下面，我们来看看学术界都有哪些解决方案：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowipbx4xuj21cm0qw77q.jpg" alt="undefined"></p><p>需要特别介绍的是一篇来自ICLR2021投稿的《Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition》，就是采用我们上述提到的「片段排列」标注方式，摒弃传统的CRF序列标注、与序列长度解耦，转化为一个对span的分类问题，这样更适合对负样本实体的采样；这样模型建模不会像对序列标注中的漏标过于敏感，也更好控制。</p><h2 id="Q5-关系抽取一片红海，如何魔改标注框架？如何突破SOTA：暴漏偏差-独立编码-pipeline？"><a href="#Q5-关系抽取一片红海，如何魔改标注框架？如何突破SOTA：暴漏偏差-独立编码-pipeline？" class="headerlink" title="Q5:关系抽取一片红海，如何魔改标注框架？如何突破SOTA：暴漏偏差/独立编码/pipeline？"></a>Q5:关系抽取一片红海，如何魔改标注框架？如何突破SOTA：暴漏偏差/独立编码/pipeline？</h2><p>2020年以来，关系抽取SOTA就换了好几个，JayJay常常感叹：关系抽取也太卷了吧～不过仔细阅读后，发现这些SOTA其实绝大多数还是围绕“标注框架”进行魔改，只要我们掌握<strong>Q1</strong>中提到的4种万能标注，登顶SOTA也不是不可能！</p><blockquote><p>本文所提到的「关系抽取」就是实体关系抽取，不同于「关系分类」。</p></blockquote><p>关系抽取范式主要有两大类：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowiqdqc0qj21860f2mz8.jpg" alt="关系抽取解决范式"></p><p>JayJay也有一段时间痴迷于各种联合抽取的joint魔改模型，如果大家有兴趣可以在知乎上直接搜索阅读JayJay的这篇文章《<strong>nlp中的实体关系抽取方法总结</strong>》。由于篇幅限制，这里简单给出一个总结图：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowitwd4ofj20k00a7wf7.jpg" alt="关系抽取——Joint范式"></p><p>结合上图，我们可以发现未来突破SOTA的方向可能是：</p><ol><li><strong>打破Joint好于Pipeline的刻板印象</strong>：Pipeline是否一定不如Joint，我们不能一概而论，特别是看过“女神的新SOTA”上一篇推文<a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/xwljKL3FjY-Nw-Zll4x3pQ">《</a><a href="https://zhuanlan.zhihu.com/p/274938894">JayLou娄杰：反直觉！陈丹琦用pipeline方式刷新关系抽取SOTA</a><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/xwljKL3FjY-Nw-Zll4x3pQ">》</a>之后。</li><li><strong>共享编码可能过于直接了</strong>：使用单独的编码器确实可以学习独立的特定任务特征，对于实体和关系确实需要特定的特征编码，在构建joint模型时如果只是简单的强行共享编码，真的可能会适得其反。这表明：针对一项任务提取的特征可能与针对另一项任务提取的特征一致或冲突，从而使学习模型混乱。所以，接下来怎么更好地去设计既可以共享、又可以任务独立的特征吧。</li><li><strong>解决暴漏偏差，迫在眉睫</strong>：最近COLING2020的一篇paper为了缓解这个问题，提出了一种单阶段的联合提取模型<strong>TPLinker</strong>[<a href="https://zhuanlan.zhihu.com/p/326302618#ref_5">5]</a>，其不包含任何相互依赖的抽取步骤，因此避免了在训练时依赖于gold的情况，从而实现了训练和测试的一致性。</li></ol><h2 id="Q6-如何登顶关系抽取冠军宝座：强大的标注策略-词汇增强-对抗训练-远程监督-假阴性降噪-交替训练？"><a href="#Q6-如何登顶关系抽取冠军宝座：强大的标注策略-词汇增强-对抗训练-远程监督-假阴性降噪-交替训练？" class="headerlink" title="Q6:如何登顶关系抽取冠军宝座：强大的标注策略+词汇增强/对抗训练/远程监督/假阴性降噪/交替训练？"></a>Q6:如何登顶关系抽取冠军宝座：强大的标注策略+词汇增强/对抗训练/远程监督/假阴性降噪/交替训练？</h2><p>废话不说，下面直接来介绍CHIP20中的关系抽取评测。这个评测任务来源于中文医学信息抽取数据集CMeIE(<a href="https://link.zhihu.com/?target=http://cmekg.pcl.ac.cn/">http://cmekg.pcl.ac.cn/</a>)，是目前最大的中文医学关系数据集，共包含近7.5万三元组数据，2.8万疾病语句和53种定义好的schema，共44种关系，如下图所示（图片来自于腾讯天衍实验室）：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowiz6g6swj213n0dt41j.jpg" alt="undefined"></p><p>这个关系评测任务是一个SPO抽取问题：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowizjbq7zj20mk072aaz.jpg" alt="undefined"></p><p>看到这个任务介绍后，如何快速构建强大的baseline呢？可以直接套用<strong>Q1</strong>给出的4种通用框架：</p><p>**策略1：基于主语感知的层叠式指针网络(指针标注)**，抽取过程：先抽取主语subject，再抽取谓语predicate和宾语object，主要参考自ACL20的CasRel[<a href="https://zhuanlan.zhihu.com/p/326302618#ref_6">6]</a>，JayJay做了以下几点改进（网络架构如下图所示）：</p><ol><li>没有随机选择主语(subject)，⽽是遍历所有不同主语的标注样本构建训练集。</li><li>对subject的感知表征，引入conditional LayerNorm进⾏。</li><li>对于医疗⽂本中，中英⽂和特殊标点同时出现的特殊情况，改进bert的分词器，以更好提取英⽂专有名词等。</li></ol><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gowj1tnjl6j20n60mi0v7.jpg" alt="undefined"></p><p>**策略2：多头选择机制(多头标注)**，是基于文献[<a href="https://zhuanlan.zhihu.com/p/326302618#ref_1">1]</a>的改进，最重要的就是关系分类器的构造，即是实体pair的一个线性分类器，每个实体pair只选取当前实体span的最后⼀个字符进⾏关系预测，如下图：</p><p>综上，我们将「融合BERT与多头选择」作为一个较强的baseline。有了baseline后，又该通过哪些辅助策略稳步迭代提升呢？其实，信息抽取类的竞赛套路都差不多，比如：</p><ul><li><strong>词汇增强</strong>：即引入词汇信息，并适配于所对应的标注策略；这种词汇增强的方式常见于NER问题中，具体可参见JayJay之前的推文<a href="https://zhuanlan.zhihu.com/p/142615620">《</a><a href="https://zhuanlan.zhihu.com/p/142615620">JayLou娄杰：中文NER的正确打开方式: 词汇增强方法总结 (从Lattice LSTM到FLAT)</a><a href="https://zhuanlan.zhihu.com/p/142615620">》</a>。这种方式的关键在于如何引入具体的知识库信息（实体信息）；</li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 信息抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>l1和l2正则化的区别</title>
      <link href="/2021/03/22/l1-he-l2-zheng-ze-hua-de-qu-bie/"/>
      <url>/2021/03/22/l1-he-l2-zheng-ze-hua-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<p><strong>L1正则化</strong>和<strong>L2正则化</strong>是<strong>控制模型复杂度</strong>和<strong>限制过拟合</strong>的常用方法。L1正则化和L2正则化之间有一些有趣的比较。我发现这些视觉上的对比和它们的解释很容易理解。</p><h2 id="1-为什么需要正则化？"><a href="#1-为什么需要正则化？" class="headerlink" title="1. 为什么需要正则化？"></a>1. 为什么需要正则化？</h2><p>首先，让我们以线性回归为例。假设<code>Y</code>和一大堆其他<code>Factor</code>之间的关系不清楚，因子1、因子2、因子3……，我们不知道这些因素中哪些会影响Y，但我们想预测Y，我们决定使用线性回归来近似Y。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got2ezrwv0j20np01nt8m.jpg" alt="undefined"></p><h2 id="2-L1-vs-L2"><a href="#2-L1-vs-L2" class="headerlink" title="2. L1 vs. L2"></a>2. L1 vs. L2</h2><h2 id="3-Difference-between-L1-and-L2-on-restrict-model-behavior"><a href="#3-Difference-between-L1-and-L2-on-restrict-model-behavior" class="headerlink" title="3. Difference between L1 and L2 on restrict model behavior"></a>3. Difference between L1 and L2 on restrict model behavior</h2>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Animated RNN, LSTM and GRU</title>
      <link href="/2021/03/22/animated-rnn-lstm-and-gru/"/>
      <url>/2021/03/22/animated-rnn-lstm-and-gru/</url>
      
        <content type="html"><![CDATA[<p>RNN是</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got0gy97hxj20ob0hajuo.jpg" alt="Fig. 0: Legend for animations"></p><h2 id="Vanilla-RNN"><a href="#Vanilla-RNN" class="headerlink" title="Vanilla RNN"></a>Vanilla RNN</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got0jibouqg20ob0i275x.gif" alt="Fig. 1: Animated vanilla RNN cell"></p><ul><li>$t$——time step</li><li>$X$——input</li><li>$h$——hidden state</li><li>length of $X$ —size/dimension of input</li><li>length of $h$ — no. of hidden units. Note that different libraries call them differently, but they mean the same:<ul><li>Keras — <code>state_size</code> <em>,</em><code>units</code></li><li>PyTorch — <code>hidden_size</code></li><li>TensorFlow — <code>num_units</code></li></ul></li></ul><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>下面两张图是LSTM中一个cell的内部结构。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got1oa5fr2g20ob0g347u.gif" alt="Animated LSTM cell"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got0va6ttlj21q10nbag0.jpg" alt="The repeating module in an LSTM contains four interacting layers."></p><p>上图中各个符号的含义：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got1zd5xguj20qe04x3yw.jpg" alt="符号含义"></p><ul><li><code>Neural Network Layer</code>表示里面具有需要学习的参数。</li><li><code>Pointwise Operation</code>表示单纯的向量操作，比如vector addition。需要仔细观察的是，在上图中，有一个<code>tanh</code>函数属于Neural Network Layer，而另一个<code>tanh</code>函数属于Pointwise Operation，区别可以在Input Gate Layer和Output Gate Layer部分的数学公式知晓。</li></ul><p>LSTM一共有3个gate来控制cell state。</p><h3 id="Step-1-Forget-Gate-Layer"><a href="#Step-1-Forget-Gate-Layer" class="headerlink" title="Step 1: Forget Gate Layer"></a>Step 1: Forget Gate Layer</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got121x5ojj21eq0fogn3.jpg" alt="Forget Gate Layer"></p><h3 id="Step-2-Input-Gate-Layer"><a href="#Step-2-Input-Gate-Layer" class="headerlink" title="Step 2: Input Gate Layer"></a>Step 2: Input Gate Layer</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got1fh7rtwj21eq0fomz1.jpg" alt="undefined"></p><p>接下来对Cell State进行更新：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got1gswwfkj21eq0fo0ub.jpg" alt="undefined"></p><h3 id="Step-3-Output-Gate-Layer"><a href="#Step-3-Output-Gate-Layer" class="headerlink" title="Step 3: Output Gate Layer"></a>Step 3: Output Gate Layer</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got1iqh6bgj21eq0fomz1.jpg" alt="undefined"></p><h2 id="GRU（Gated-Recurrent-Unit-）"><a href="#GRU（Gated-Recurrent-Unit-）" class="headerlink" title="GRU（Gated Recurrent Unit ）"></a>GRU（Gated Recurrent Unit ）</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got1qt4fpgg20ob0h67d3.gif" alt="Fig. 3: Animated GRU cell"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1got1k2w46fj21eq0fognw.jpg" alt="undefined"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
            <tag> LSTM </tag>
            
            <tag> RNN </tag>
            
            <tag> GRU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Q&amp;A——Transformer，BERT</title>
      <link href="/2021/02/24/q-a-transformer-bert/"/>
      <url>/2021/02/24/q-a-transformer-bert/</url>
      
        <content type="html"><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><blockquote><ul><li>代码实战：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li><li>超级好的动画演示：<a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li><li><a href="https://zhuanlan.zhihu.com/p/60821628">碎碎念：Transformer的细枝末节</a></li></ul></blockquote><h3 id="画Transformer的结构图"><a href="#画Transformer的结构图" class="headerlink" title="画Transformer的结构图"></a>画Transformer的结构图</h3><p><img src="https://pic3.zhimg.com/80/v2-47a5a577603efb220d80167c4c291806_720w.jpg" alt="Transformer结构图"></p><h3 id="简述Transformer原理"><a href="#简述Transformer原理" class="headerlink" title="简述Transformer原理"></a>简述Transformer原理</h3><ul><li>Positional Encoding</li><li>Multi-Head Attention</li><li>Residual &amp; Layer Normalization</li></ul><h3 id="Scaled-Dot-Product-Attention为什么要scaled？（两点）"><a href="#Scaled-Dot-Product-Attention为什么要scaled？（两点）" class="headerlink" title="Scaled Dot-Product Attention为什么要scaled？（两点）"></a>Scaled Dot-Product Attention为什么要scaled？（两点）</h3><blockquote><p>论文中解释是：<strong>向量的点积结果会很大</strong>，将softmax函数push到梯度很小的区域，scaled会缓解这种现象。怎么理解将sotfmax函数push到梯度很小区域？还有为什么scaled是维度的根号，不是其他的数？</p></blockquote><p>谢邀。非常有意义的问题，我思考了好久，按照描述中的两个问题分点回答一下。</p><p><strong>Question 1: 为什么比较大的输入会使得softmax的梯度变得很小？</strong></p><p>对于一个输入向量 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%5Cin%5Cmathbb%7BR%7D%5Ed" alt="[公式]"> ，softmax函数将其映射/归一化到一个分布 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7By%7D%7D%5Cin%5Cmathbb%7BR%7D%5Ed" alt="[公式]"> 。在这个过程中，softmax先用一个自然底数 <img src="https://www.zhihu.com/equation?tex=e" alt="[公式]"> 将输入中的<strong>元素间差距先“拉大”</strong>，然后归一化为一个分布。假设某个输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D" alt="[公式]"> 中最大的的元素下标是 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> ，<strong>如果输入的数量级变大（每个元素都很大），那么</strong><img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_k" alt="[公式]"><strong>会非常接近1。</strong></p><p>我们可以用一个小例子来看看 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D" alt="[公式]"> 的数量级对输入最大元素对应的预测概率<img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_k" alt="[公式]">的影响。假定输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D=%5Ba,+a,+2a%5D%5E%5Ctop" alt="[公式]"> ），我们来看不同量级的 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> 产生的<img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3" alt="[公式]">有什么区别。</p><ul><li><img src="https://www.zhihu.com/equation?tex=a=1+" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3=0.5761168847658291" alt="[公式]"> ;</li><li><img src="https://www.zhihu.com/equation?tex=a=10++" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3=0.999909208384341" alt="[公式]">;</li><li><img src="https://www.zhihu.com/equation?tex=a=100++" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3%5Capprox+1.0" alt="[公式]"> (计算机精度限制)。</li></ul><p>我们不妨把 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> 在不同取值下，对应的的<img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_3" alt="[公式]">全部绘制出来。代码如下：</p><pre><code class="python">from math import expfrom matplotlib import pyplot as pltimport numpy as np f = lambda x: exp(x * 2) / (exp(x) + exp(x) + exp(x * 2))x = np.linspace(0, 100, 100)y_3 = [f(x_i) for x_i in x]plt.plot(x, y_3)plt.show()</code></pre><p>得到的图如下所示：</p><p><img src="https://pic1.zhimg.com/50/v2-1d7ef7b8fb59a925470dc52218b43117_hd.jpg?source=1940ef5c"></p><p>可以看到，数量级对softmax得到的分布影响非常大。<strong>在数量级较大时，softmax将几乎全部的概率分布都分配给了最大值对应的标签</strong>。</p><p>然后我们来看softmax的梯度。不妨简记softmax函数为 <img src="https://www.zhihu.com/equation?tex=g(%5Ccdot)" alt="[公式]"> ，softmax得到的分布向量 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7By%7D%7D=g(%5Cmathbf%7Bx%7D)" alt="[公式]"> 对输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D" alt="[公式]"> 的梯度为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+g(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%5Cmathbf%7Bx%7D%7D=%5Cmathrm%7Bdiag%7D(%5Chat%7B%5Cmathbf%7By%7D%7D)-%5Chat%7B%5Cmathbf%7By%7D%7D%5Chat%7B%5Cmathbf%7By%7D%7D%5E%5Ctop%5Cquad%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes+d%7D" alt="[公式]"></p><p>把这个矩阵展开：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+g(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%5Cmathbf%7Bx%7D%7D=%5Cleft%5B%5Cbegin%7Bmatrix%7D+%5Chat%7By%7D_1+&amp;+0&amp;%5Ccdots+&amp;0%5C%5C+0+&amp;+%5Chat%7By%7D_2+&amp;+%5Ccdots+&amp;+0%5C%5C+%5Cvdots+&amp;+%5Cvdots+&amp;+%5Cddots+&amp;+%5Cvdots%5C%5C+0+&amp;+0+&amp;+%5Ccdots+&amp;+%5Chat%7By%7D_d+%5Cend%7Bmatrix%7D%5Cright%5D+-+%5Cleft%5B%5Cbegin%7Bmatrix%7D+%5Chat%7By%7D_1%5E2+&amp;+%5Chat%7By%7D_1%5Chat%7By%7D_2&amp;%5Ccdots+&amp;%5Chat%7By%7D_1%5Chat%7By%7D_d%5C%5C+%5Chat%7By%7D_2%5Chat%7By%7D_1+&amp;+%5Chat%7By%7D_2%5E2+&amp;+%5Ccdots+&amp;+%5Chat%7By%7D_2%5Chat%7By%7D_d%5C%5C+%5Cvdots+&amp;+%5Cvdots+&amp;+%5Cddots+&amp;+%5Cvdots%5C%5C+%5Chat%7By%7D_d%5Chat%7By%7D_1+&amp;+%5Chat%7By%7D_d%5Chat%7By%7D_2+&amp;+%5Ccdots+&amp;+%5Chat%7By%7D_d%5E2+%5Cend%7Bmatrix%7D%5Cright%5D" alt="[公式]"></p><p>根据前面的讨论，当输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D" alt="[公式]"> 的元素均较大时，softmax会把大部分概率分布分配给最大的元素，假设我们的输入数量级很大，最大的元素是 <img src="https://www.zhihu.com/equation?tex=x_1" alt="[公式]"> ，那么就将产生一个接近one-hot的向量 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7By%7D%7D%5Capprox%5B1,+0,+%5Ccdots,+0%5D%5E%5Ctop" alt="[公式]">  ,此时上面的矩阵变为如下形式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+g(%5Cmathbf%7Bx%7D)%7D%7B%5Cpartial%5Cmathbf%7Bx%7D%7D%5Capprox%5Cleft%5B%5Cbegin%7Bmatrix%7D+1+&amp;+0&amp;%5Ccdots+&amp;0%5C%5C+0+&amp;+0+&amp;+%5Ccdots+&amp;+0%5C%5C+%5Cvdots+&amp;+%5Cvdots+&amp;+%5Cddots+&amp;+%5Cvdots%5C%5C+0+&amp;+0+&amp;+%5Ccdots+&amp;+0+%5Cend%7Bmatrix%7D%5Cright%5D+-+%5Cleft%5B%5Cbegin%7Bmatrix%7D+1+&amp;+0&amp;%5Ccdots+&amp;0%5C%5C+0+&amp;+0+&amp;+%5Ccdots+&amp;+0%5C%5C+%5Cvdots+&amp;+%5Cvdots+&amp;+%5Cddots+&amp;+%5Cvdots%5C%5C+0+&amp;+0+&amp;+%5Ccdots+&amp;+0+%5Cend%7Bmatrix%7D%5Cright%5D=%5Cmathbf%7B0%7D" alt="[公式]"></p><p>也就是说，<strong>在输入的数量级很大时，梯度消失为0，造成参数更新困难。</strong></p><blockquote><p>注： softmax的梯度可以自行推导，网络上也有很多推导可以参考。</p></blockquote><hr><p><strong>Question 2. 维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？</strong></p><p>针对为什么维度会影响点积的大小，在论文的脚注中其实给出了一点解释：</p><p><img src="https://pic4.zhimg.com/50/v2-493286fbea075e160bf3bac214d2ac60_hd.jpg?source=1940ef5c" alt="img"></p><p>假设向量 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积 <img src="https://www.zhihu.com/equation?tex=q%5Ccdot+k" alt="[公式]"> 的均值是0，方差是 <img src="https://www.zhihu.com/equation?tex=d_k" alt="[公式]"> 。这里我给出一点更详细的推导：</p><p>对 <img src="https://www.zhihu.com/equation?tex=%5Cforall+i+=+1,+%5Ccdots,+d_k" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=q_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=k_i" alt="[公式]"> 都是随机变量，为了方便书写，不妨记 <img src="https://www.zhihu.com/equation?tex=X=q_i" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=Y=k_i" alt="[公式]"> 。这样有： <img src="https://www.zhihu.com/equation?tex=D(X)=D(Y)=1" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=E(X)=E(Y)=0+" alt="[公式]"> 。</p><p>则:</p><ol><li><img src="https://www.zhihu.com/equation?tex=E(XY)=E(X)E(Y)=0%5Ctimes+0=0" alt="[公式]"></li><li></li></ol><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7DD(XY)&amp;=E(X%5E2%5Ccdot+Y%5E2)-%5BE(XY)%5D%5E2%5C%5C+&amp;=E(X%5E2)E(Y%5E2)-%5BE(X)E(Y)%5D%5E2%5C%5C+&amp;=E(X%5E2-0%5E2)E(Y%5E2-0%5E2)-%5BE(X)E(Y)%5D%5E2%5C%5C+&amp;=E(X%5E2-%5BE(X)%5D%5E2)E(Y%5E2-%5BE(Y)%5D%5E2)-%5BE(X)E(Y)%5D%5E2%5C%5C+&amp;=D(X)D(Y)-%5BE(X)E(Y)%5D%5E2%5C%5C+&amp;=1%5Ctimes+1+-+(0%5Ctimes+0)%5E2%5C%5C+&amp;=1+%5Cend%7Balign%7D" alt="[公式]"></p><p>这样<img src="https://www.zhihu.com/equation?tex=%5Cforall+i+=+1,+%5Ccdots,+d_k" alt="[公式]"> ，  <img src="https://www.zhihu.com/equation?tex=q_i%5Ccdot+k_i" alt="[公式]">的均值是0，方差是1，又由期望和方差的性质， 对相互独立的分量 <img src="https://www.zhihu.com/equation?tex=Z_i" alt="[公式]">，有</p><p><img src="https://www.zhihu.com/equation?tex=E(%5Csum_i+Z_i)=%5Csum_i+E(Z_i)" alt="[公式]">，</p><p>以及</p><p><img src="https://www.zhihu.com/equation?tex=D(%5Csum_i+Z_i)=%5Csum_i+D(Z_i)" alt="[公式]"> ，</p><p>所以有 <img src="https://www.zhihu.com/equation?tex=q%5Ccdot+k" alt="[公式]"> 的均值 <img src="https://www.zhihu.com/equation?tex=E(q%5Ccdot+k)=0" alt="[公式]"> ，方差 <img src="https://www.zhihu.com/equation?tex=D(q%5Ccdot+k)=d_k" alt="[公式]"> 。<strong>方差越大也就说明，点积的数量级越大（以越大的概率取大值）</strong>。那么一个自然的做法就是把方差稳定到1，做法是将点积除以 <img src="https://www.zhihu.com/equation?tex=%5Csqrt+d_k" alt="[公式]"> ，这样有：</p><p><img src="https://www.zhihu.com/equation?tex=D(%5Cfrac%7Bq%5Ccdot+k%7D%7B%5Csqrt+d_k%7D)=%5Cfrac%7Bd_k%7D%7B(%5Csqrt+d_k)%5E2%7D=1" alt="[公式]"></p><p><strong>将方差控制为1，也就有效地控制了前面提到的梯度消失的问题</strong>。</p><p>可以参考一下。水平有限，如果有误请指出。</p><p>作者：LinT<br>链接：<a href="https://www.zhihu.com/question/339723385/answer/782509914">https://www.zhihu.com/question/339723385/answer/782509914</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h3 id="attention和self-attention的区别"><a href="#attention和self-attention的区别" class="headerlink" title="attention和self-attention的区别"></a>attention和self-attention的区别</h3><h3 id="为什么self-attention可以替代seq2seq"><a href="#为什么self-attention可以替代seq2seq" class="headerlink" title="为什么self-attention可以替代seq2seq"></a>为什么self-attention可以替代seq2seq</h3><p>关于为何self-attention可以代替seq2seq，论文中提到了三点：</p><ol><li><strong>Complexity per layer</strong>（$O(n^2 d)$）</li><li><strong>Sequential Operations</strong>（i.e. can be parallelized, speed-up in the training time）（$O(1)$）</li><li><strong>Maximum Path Length</strong>（it can capture longer dependencies in a sentence.）（$O(1)$）</li></ol><h3 id="transformer中句子的encoder表示的是什么"><a href="#transformer中句子的encoder表示的是什么" class="headerlink" title="transformer中句子的encoder表示的是什么"></a>transformer中句子的encoder表示的是什么</h3><p>表示的是句子中每个token的Embedding。</p><h3 id="怎么加入词序信息"><a href="#怎么加入词序信息" class="headerlink" title="怎么加入词序信息"></a>怎么加入词序信息</h3><p>在Word Embedding层后面加上了<code>Positional Encoding</code>。</p><p>Ideally, the following criteria should be satisfied:</p><ul><li><strong>唯一性</strong>。It should output a unique encoding for each time-step (word’s position in a sentence)</li><li><strong>距离不变性</strong>。Distance between any two time-steps should be consistent across sentences with different lengths.</li><li><strong>扩展性</strong>。Our model should generalize to longer sentences without any efforts. Its values should be bounded.</li><li><strong>确定性</strong>。It must be deterministic.</li></ul><p>在Transformer中，选择的<code>Positional Encoding</code>是不同频率的<code>sine</code>和<code>consine</code>函数：</p><blockquote><p>参考文献：</p><ul><li><a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#proposed-method">Transformer Architecture: The Positional Encoding</a></li><li><a href="https://www.zhihu.com/question/350116316/answer/864616018">https://www.zhihu.com/question/350116316/answer/864616018</a></li></ul></blockquote><h3 id="为什么选择sinusoidal"><a href="#为什么选择sinusoidal" class="headerlink" title="为什么选择sinusoidal"></a>为什么选择sinusoidal</h3><blockquote><p>it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p></blockquote><blockquote><p>参考文献：</p><ul><li><a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#proposed-method">Transformer Architecture: The Positional Encoding</a></li><li><a href="https://www.zhihu.com/question/350116316/answer/864616018">https://www.zhihu.com/question/350116316/answer/864616018</a></li><li><a href="https://www.zhihu.com/question/347678607/answer/864217252">https://www.zhihu.com/question/347678607/answer/864217252</a></li></ul></blockquote><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h3 id="BERT的具体网络结构，以及训练过程，bert为什么火，它在什么的基础上改进了些什么？"><a href="#BERT的具体网络结构，以及训练过程，bert为什么火，它在什么的基础上改进了些什么？" class="headerlink" title="BERT的具体网络结构，以及训练过程，bert为什么火，它在什么的基础上改进了些什么？"></a>BERT的具体网络结构，以及训练过程，bert为什么火，它在什么的基础上改进了些什么？</h3><p>BERT是用Transformer的Encoder侧的网络，作为一个文本编码器，使用大规模数据进行预训练，预训练使用两个loss：</p><ul><li>一个是<strong>Masked LM</strong>，遮蔽掉源端的一些字（可能会被问到mask的具体做法，<strong>15%概率mask词</strong>，这其中<strong>80%用[mask]替换，10%随机替换一个其他字，10%不替换</strong>，<strong>至于为什么这么做，那就得问问BERT的作者了</strong>），然后根据上下文去预测这些字；</li><li>一个是<strong>Next Sentence Prediction</strong>，判断两个句子是否在文章中互为上下句，然后使用了大规模的语料去预训练。</li></ul><p>在它之前是GPT，GPT是一个<strong>单向语言模型</strong>的预训练过程（<strong>它和GPT的区别就是bert为啥叫双向 bi-directional</strong>），更适用于文本生成，通过前文去预测当前的字。下图为transformer的结构，bert的网络结构则用了左边的encoder。</p><p><img src="https://pic3.zhimg.com/80/v2-47a5a577603efb220d80167c4c291806_720w.jpg" alt="Transformer结构图"></p><h3 id="为什么BERT有3个嵌入层，它们都是如何实现的"><a href="#为什么BERT有3个嵌入层，它们都是如何实现的" class="headerlink" title="为什么BERT有3个嵌入层，它们都是如何实现的"></a><a href="https://www.cnblogs.com/d0main/p/10447853.html">为什么BERT有3个嵌入层，它们都是如何实现的</a></h3><p>本文将阐述BERT中嵌入层的实现细节，包括token embeddings、segment embeddings, 和position embeddings.</p><p>下面这幅来自原论文的图清晰地展示了BERT中每一个嵌入层的作用：</p><p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190227235341575-1614948318.png"></p><p> 和大多数NLP深度学习模型一样，BERT将输入文本中的每一个词（token)送入token embedding层从而将每一个词转换成向量形式。但不同于其他模型的是，BERT又多了两个嵌入层，即segment embeddings和 position embeddings。在阅读完本文之后，你就会明白为何要多加这两个嵌入层了。</p><h4 id="Token-Embeddings"><a href="#Token-Embeddings" class="headerlink" title="Token Embeddings"></a>Token Embeddings</h4><p><strong>作用</strong></p><p>正如前面提到的，token embedding 层是要将各个词转换成固定维度的向量。在BERT中，每个词会被转换成768维的向量表示。</p><p><strong>实现</strong></p><p> 假设输入文本是 “I like strawberries”。下面这个图展示了 Token Embeddings 层的实现过程:</p><p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190227235724883-2118042882.png" alt="img"></p><p> 输入文本在送入token embeddings 层之前要先进行tokenization处理。此外，两个特殊的token会被插入到tokenization的结果的开头 ([CLS])和结尾 ([SEP]) 。它们视为后面的分类任务和划分句子对服务的。</p><p> tokenization使用的方法是WordPiece tokenization. 这是一个数据驱动式的tokenization方法，旨在权衡词典大小和oov词的个数。这种方法把例子中的“strawberries”切分成了“straw” 和“berries”。这种方法的详细内容不在本文的范围内。有兴趣的读者可以参阅 <a href="https://arxiv.org/pdf/1609.08144.pdf">Wu et al. (2016)</a> 和 <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf">Schuster &amp; Nakajima (2012)</a>。使用WordPiece tokenization让BERT在处理英文文本的时候仅需要存储30,522 个词，而且很少遇到oov的词。</p><p> Token Embeddings 层会将每一个wordpiece token转换成768维的向量。这样，例子中的6个token就被转换成了一个(6, 768) 的矩阵或者是(1, 6, 768)的张量（如果考虑batch_size的话）。</p><h4 id="Segment-Embeddings"><a href="#Segment-Embeddings" class="headerlink" title="Segment Embeddings"></a>Segment Embeddings</h4><p><strong>作用</strong></p><p> BERT 能够处理对输入句子对的分类任务。这类任务就像判断两个文本是否是语义相似的。句子对中的两个句子被简单的拼接在一起后送入到模型中。那BERT如何去区分一个句子对中的两个句子呢？答案就是segment embeddings.</p><p><strong>实现</strong></p><p>假设有这样一对句子 (“I like cats”, “I like dogs”)。下面的图成仙了segment embeddings如何帮助BERT区分两个句子:</p><p><img src="https://img2018.cnblogs.com/blog/1135245/201902/1135245-20190227235737504-479706108.png"></p><p> Segment Embeddings 层只有两种向量表示。前一个向量是把0赋给第一个句子中的各个token, 后一个向量是把1赋给第二个句子中的各个token。如果输入仅仅只有一个句子，那么它的segment embedding就是全0。</p><h4 id="Position-Embeddings"><a href="#Position-Embeddings" class="headerlink" title="Position Embeddings"></a>Position Embeddings</h4><p><strong>作用</strong></p><p>BERT包含这一串Transformers (<a href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al</a>. 2017)，而且一般认为，Transformers无法编码输入的序列的顺序性。 <a href="https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a">博客</a>更加详细的解释了这一问题。总的来说，加入position embeddings会让BERT理解下面这种情况：</p><blockquote><p>I think, therefore I am</p></blockquote><p>第一个 “I” 和第二个 “I”应该有着不同的向量表示。</p><p><strong>实现</strong></p><p> <strong>BERT能够处理最长512个token的输入序列。</strong>论文作者通过让BERT在各个位置上学习一个向量表示来讲序列顺序的信息编码进来。这意味着Position Embeddings layer 实际上就是一个大小为 (512, 768) 的lookup表，表的第一行是代表第一个序列的第一个位置，第二行代表序列的第二个位置，以此类推。因此，如果有这样两个句子“Hello world” 和“Hi there”, “Hello” 和“Hi”会由完全相同的position embeddings，因为他们都是句子的第一个词。同理，“world” 和“there”也会有相同的position embedding。</p><h4 id="合成表示"><a href="#合成表示" class="headerlink" title="合成表示"></a>合成表示</h4><p>我们已经介绍了长度为n的输入序列将获得的三种不同的向量表示，分别是：</p><ul><li>Token Embeddings， (1, n, 768) ，词的向量表示</li><li>Segment Embeddings， (1, n, 768)，辅助BERT区别句子对中的两个句子的向量表示</li><li>Position Embeddings ，(1, n, 768) ，让BERT学习到输入的顺序属性</li></ul><p> 这些表示会被按元素相加，得到一个大小为(1, n, 768)的合成表示。这一表示就是BERT编码层的输入了。</p><h3 id="BERT为什么使用position-embedding而不是position-encoding"><a href="#BERT为什么使用position-embedding而不是position-encoding" class="headerlink" title="BERT为什么使用position embedding而不是position encoding?"></a>BERT为什么使用position embedding而不是position encoding?</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1go5wvs4iyyj20mz0kbdj3.jpg"></p><h3 id="讲讲multi-head-attention的具体结构"><a href="#讲讲multi-head-attention的具体结构" class="headerlink" title="讲讲multi-head attention的具体结构"></a>讲讲multi-head attention的具体结构</h3><p><code>BERT-BASE</code>由12层transformer layer（encoder端）构成，首先token embedding , postion embedding（可能会被问到有哪几种position embedding的方式，bert是使用的哪种）, segment embedding做加和作为网络输入，每层由一个multi-head attention, 一个feed forward 以及两层layerNorm构成，一般会被问到multi-head attention的结构，具体可以描述为：</p><p>step 1：</p><p>一个768的hidden向量，被映射成query， key， value。 然后三个向量分别切分成12个小的64维的向量，每一组小向量之间做attention。不妨假设batch_size为32，seqlen为512，隐层维度为768，12个head。</p><p>hidden(32 x 512 x 768) -&gt; query(32 x 512 x 768) -&gt; 32 x 12 x 512 x 64</p><p>hidden(32 x 512 x 768) -&gt; key(32 x 512 x 768) -&gt; 32 x 12 x 512 x 64</p><p>hidden(32 x 512 x 768) -&gt; val(32 x 512 x 768) -&gt; 32 x 12 x 512 x 64</p><p>step 2：</p><p>然后query和key之间做attention，得到一个32 x 12 x 512 x 512的权重矩阵，然后根据这个权重矩阵加权value中切分好的向量，得到一个32 x 12 x 512 x 64 的向量，拉平输出为768向量。</p><p>32 x 12 x 512 x 64(query_hidden) * 32 x 12 x 64 x 512(key_hidden) -&gt; 32 x 12 x 512 x 512</p><p>32 x 12 x 64 x 512(value_hidden) * 32 x 12 x 512 x 512 (权重矩阵) -&gt; 32 x 12 x 512 x 64</p><p>然后再还原成 -&gt; 32 x 512 x 768</p><p>简言之是12个头，每个头都是一个64维度分别去与其他的所有位置的hidden embedding做attention然后再合并还原。</p><h3 id="BERT-采用哪种Normalization结构，LayerNorm和BatchNorm区别，LayerNorm结构有参数吗，参数的作用？"><a href="#BERT-采用哪种Normalization结构，LayerNorm和BatchNorm区别，LayerNorm结构有参数吗，参数的作用？" class="headerlink" title="BERT 采用哪种Normalization结构，LayerNorm和BatchNorm区别，LayerNorm结构有参数吗，参数的作用？"></a>BERT 采用哪种Normalization结构，LayerNorm和BatchNorm区别，LayerNorm结构有参数吗，参数的作用？</h3><p>采用LayerNorm结构，和BatchNorm的区别主要是<strong>做Normalization的维度不同</strong>；</p><p><code>BatchNorm</code>针对一个batch里面的所有样本进行规范化，针对单个神经元进行，比如batch里面有64个样本，那么规范化输入的这64个样本各自经过这个神经元后的值（64维）。</p><p><code>LayerNorm</code>则是针对单个样本，不依赖于其他数据，常被用于小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域，就BERT来说就是对每层输出的隐层向量（768维）做规范化。</p><p>图像领域用BN比较多的原因是因为每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起规范化。</p><pre><code class="python">class BertLayerNorm(nn.Module):    def __init__(self, hidden_size, eps=1e-5):        super(BertLayerNorm, self).__init__()        self.weight = nn.Parameter(torch.ones(hidden_size))        self.bias = nn.Parameter(torch.zeros(hidden_size))        self.variance_epsilon = eps    def forward(self, x):        u = x.mean(-1, keepdim=True)        s = (x - u).pow(2).mean(-1, keepdim=True)        x = (x - u) / torch.sqrt(s + self.variance_epsilon)        return self.weight * x + self.bias</code></pre><p>贴一个LayerNorm的实现，可以看到module中有<code>weight</code>和<code>bias</code>参数，以Sigmoid激活函数为例，批量归一化之后数据整体处于函数的非饱和区域， 只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数w和b ，也就是上面的weight和bias，简而言之，规范化后的隐层表示将输入数据限制到了一个全局统一的确定范围，为了保证模型的表达能力不因为规范化而下降，引入了<img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D" alt="[公式]">是<strong>再平移参数</strong>，<img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bw%7D" alt="[公式]">是<strong>再缩放参数</strong>。（过激活函数前规范化，之后还原）</p><p>我认为BN效果不好的原因，<strong>主要还是NLP数据与CV数据特性的差别对训练过程产生了影响，使得训练中batch的统计量不稳定。</strong></p><h3 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h3>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is the difference between FP16 and FP32 when doing deep learning?</title>
      <link href="/2021/02/22/what-is-the-difference-between-fp16-and-fp32-when-doing-deep-learning/"/>
      <url>/2021/02/22/what-is-the-difference-between-fp16-and-fp32-when-doing-deep-learning/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning">https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning</a></p></blockquote><p>This is a well-timed question, as we just added FP16 support to <a href="https://github.com/uber/horovod">Horovod</a> last Friday. So naturally, I’m itching to talk more about it!</p><p>The value proposition when using FP16 for training a deep neural network is <strong>significantly faster training</strong> times without “any” loss in performance (<strong>some restrictions apply</strong>).</p><p>Specifically, FP16 will:</p><ul><li><strong>Reduce memory</strong> by cutting the size of your tensors in half.</li><li><strong>Reduce training time</strong> by speeding up computations on the GPU (reducing arithmetic bandwidth) and (in the distributed case) reducing network bandwidth.</li></ul><p>Theoretically, you’ll be able to train bigger models faster.</p><p>Sounds great, right? But how does it work, and why might it not <em>always</em> work?</p><hr><p>FP16 here refers to <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">half-precision floating points</a> (16-bit), as opposed to the standard 32-bit floating point, or FP32.</p><p>Traditionally, when training a neural network, you would use 32-bit floating points to represent the <strong>weights</strong> in your network. There are a number of reasons for that:</p><ul><li>32-bit floating points have enough <em>range</em> to represent numbers of <em>magnitude</em> both smaller (10^-45) and larger (10^38) than you’d need for most applications.</li><li>32-bit floats have enough <em>precision</em> such that we can <em>distinguish</em> numbers of varying magnitudes from one another.</li><li>Virtually all hardware (GPUs, CPUs) and APIs support 32-bit floating point instructions natively, and efficiently.</li><li>Very rarely in computing is floating point math a major bottleneck, and if it is, there’s rarely away to get around it (because we need that precision).</li></ul><p><img src="https://qph.fs.quoracdn.net/main-qimg-749cc641eb4d5dafd085e8c23f8826aa.webp" alt="img"></p><p>In a 32-bit floating point, you reserve 8 bits for the exponent (the “magnitude”) and 23 bits for the mantissa (the “precision”).</p><p>But, as it turns out, for most deep learning use cases, we <em>don’t</em> actually need all that precision. And indeed, we rarely need all that much magnitude either.</p><p>NVIDIA did a great analysis on mixed precision training in discussing the half-precision support available in their Volta series of GPUs[<a href="https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning#HDPqZ">1]</a> . Their conclusion was that most weights and gradients tend to fall well within the 16-bit representable range, and of the gradients that did not (mostly small activation gradients in some networks), simply scaling up the gradient was sufficient to achieve convergence.</p><p>So for most cases, all those extra bits are just wasteful. With FP16, we can reduce the number of bits in half, reducing the exponent from 8 bits to 5, and the mantissa from 23 bits to 10.</p><p><img src="https://qph.fs.quoracdn.net/main-qimg-68317a1ee8958729a57e35dbc7aa2501" alt="img"></p><p>But it’s not without risks. The representable range for FP16 is <em>very</em> small in comparison to FP32: 10^-8 to 65504! What that means is that we risk <strong>underflow</strong> (attempting to represent numbers so small they clamp to zero) and <strong>overflow</strong> (numbers so large they become NaN, not a number). With underflow, our network never learns anything, and with overflow, it learns garbage. Both are bad.</p><p>One exciting alternative that addresses this issue is <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a>. Though bfloat16 is also a 16-bit floating point representation, it uses its bits a <em>bit</em> differently:</p><p><img src="https://qph.fs.quoracdn.net/main-qimg-65b4c66b94efd478d62ff7812c96127b" alt="img"></p><p>The size of its exponent is the same size as FP32, meaning it can represent the same magnitudes, but with <em>much less</em> precision, effectively eliminating the underflow and overflow problem, but at the cost of not being able to distinguish numbers of similar magnitudes from one another.</p><p>But it turns out: that’s okay! When thinking about gradients and weights in particular, the <strong>magnitude</strong> and <strong>direction</strong> end up being by far the most significant factors, the precise digits being of comparatively little importance. Indeed, there’s even been recent work on quantizing gradients to a <strong>single bit</strong>[<a href="https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning#eLDiW">2]</a> !</p><p>Unfortunately, bfloat16 is not currently supported natively in most instruction sets, with the notable exception of Google’s TPUs, one of their major selling points!</p><hr><p>If you’re interested in comparing your training performance with FP16 vs FP32, I encourage you to check out Horovod’s new gradient compression feature:</p><pre><code>opt = hvd.DistributedOptimizer(opt, compression=hvd.Compression.fp16) </code></pre><p>And let me know how it goes!</p><p>Footnotes</p><p>[<a href="https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning#cite-HDPqZ">1] </a><a href="https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/">Mixed-Precision Training of Deep Neural Networks | NVIDIA Developer Blog</a></p><p>[<a href="https://www.quora.com/What-is-the-difference-between-FP16-and-FP32-when-doing-deep-learning#cite-eLDiW">2] </a><a href="https://arxiv.org/abs/1802.04434">Compressed Optimisation for Non-Convex Problems</a></p><p><a href="https://www.freepatentsonline.com/y2018/0322391.html">https://www.freepatentsonline.com/y2018/0322391.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习tricks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>warmup_proportion预热学习率的作用</title>
      <link href="/2021/02/22/warmup-proportion-yu-re-xue-xi-lu-de-zuo-yong/"/>
      <url>/2021/02/22/warmup-proportion-yu-re-xue-xi-lu-de-zuo-yong/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：EO_eaf6<br>链接：<a href="https://www.jianshu.com/p/19a4abfcd835">https://www.jianshu.com/p/19a4abfcd835</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>学习率（learning rate）是神经网络训练中最重要的超参数之一，针对学习率的优化方式很多，Warmup是其中的一种。</p><h2 id="什么是Warmup"><a href="#什么是Warmup" class="headerlink" title="什么是Warmup?"></a>什么是Warmup?</h2><p> Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。</p><h2 id="为什么使用Warmup"><a href="#为什么使用Warmup" class="headerlink" title="为什么使用Warmup?"></a>为什么使用Warmup?</h2><p> 由于刚开始训练时,模型的权重（weights）是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定（振荡），选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定，等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p><p>Example：Resnet论文中使用一个110层的ResNet在cifar10上训练时，先用0.01的学习率训练直到训练误差低于80%(大概训练了400个steps)，然后使用0.1的学习率进行训练。</p><h2 id="Warmup的改进"><a href="#Warmup的改进" class="headerlink" title="Warmup的改进"></a>Warmup的改进</h2><p>前面所述的Warmup是constant warmup，它的不足之处在于从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。于是18年Facebook提出了gradual warmup来解决这个问题，即从最初的小学习率开始，每个step增大一点点，直到达到最初设置的比较大的学习率时，采用最初设置的学习率进行训练。</p><ul><li><p><a href="https://stackoverflow.com/questions/55933867/what-does-learning-rate-warm-up-mean">https://stackoverflow.com/questions/55933867/what-does-learning-rate-warm-up-mean</a></p></li><li><p><a href="https://github.com/google-research/bert/issues/425">https://github.com/google-research/bert/issues/425</a></p></li><li><p><a href="https://arxiv.org/abs/1803.09820">https://arxiv.org/abs/1803.09820</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习tricks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> warmup_proportion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch中在反向传播前为什么要手动将梯度清零？</title>
      <link href="/2021/02/21/pytorch-zhong-zai-fan-xiang-chuan-bo-qian-wei-shi-me-yao-shou-dong-jiang-ti-du-qing-ling/"/>
      <url>/2021/02/21/pytorch-zhong-zai-fan-xiang-chuan-bo-qian-wei-shi-me-yao-shou-dong-jiang-ti-du-qing-ling/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作者：Pascal<br>链接：<a href="https://www.zhihu.com/question/303070254/answer/573037166">https://www.zhihu.com/question/303070254/answer/573037166</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p>这种模式可以让梯度玩出更多花样，比如说<strong>梯度累加（gradient accumulation）</strong>。gradient_accumulation_steps通过累计梯度来解决本地显存不足问题。</p><p>传统的训练函数，一个batch是这么训练的：</p><pre><code class="python">for i,(images,target) in enumerate(train_loader):    # 1. input output    images = images.cuda(non_blocking=True)    target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)    outputs = model(images)    loss = criterion(outputs,target)    # 2. backward    optimizer.zero_grad()   # reset gradient    loss.backward()    optimizer.step()</code></pre><ol><li>获取loss：输入图像和标签，通过infer计算得到预测值，计算损失函数；</li><li>optimizer.zero_grad() 清空过往梯度；</li><li>loss.backward() 反向传播，计算当前梯度；</li><li>optimizer.step() 根据梯度更新网络参数</li></ol><p>简单的说就是进来一个batch的数据，计算一次梯度，更新一次网络</p><p>使用梯度累加是这么写的：</p><pre><code class="python">for i,(images,target) in enumerate(train_loader):    # 1. input output    images = images.cuda(non_blocking=True)    target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)    outputs = model(images)    loss = criterion(outputs,target)    # 2.1 loss regularization    loss = loss/accumulation_steps       # 2.2 back propagation    loss.backward()    # 3. update parameters of net    if((i+1)%accumulation_steps)==0:        # optimizer the net        optimizer.step()        # update parameters of net        optimizer.zero_grad()   # reset gradient</code></pre><p>获取loss：输入图像和标签，通过infer计算得到预测值，计算损失函数；</p><p>loss.backward() 反向传播，计算当前梯度；</p><p>多次循环步骤1-2，不清空梯度，使梯度累加在已有梯度上；</p><p>梯度累加了一定次数后，先optimizer.step() 根据累计的梯度更新网络参数，然后optimizer.zero_grad() 清空过往梯度，为下一波梯度累加做准备；</p><p>总结来说：梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空，不断累加，累加一定次数后，根据累加的梯度更新网络参数，然后清空梯度，进行下一次循环。</p><p>一定条件下，batchsize越大训练效果越好，梯度累加则实现了batchsize的变相扩大，如果accumulation_steps为8，则batchsize ‘变相’ 扩大了8倍，<strong>是我们这种乞丐实验室解决显存受限的一个不错的trick，使用时需要注意，学习率也要适当放大。</strong></p><p>更新1：关于BN是否有影响，之前有人是这么说的：</p><blockquote><p>As far as I know, batch norm statistics get updated on each forward pass, so no problem if you don’t do .backward() every time.</p></blockquote><p>BN的估算是在forward阶段就已经完成的，并不冲突，只是accumulation_steps=8和真实的batchsize放大八倍相比，效果自然是差一些，毕竟八倍Batchsize的BN估算出来的均值和方差肯定更精准一些。</p><p>更新2：根据 <a href="http://www.zhihu.com/people/ebdb20dc00a4f2864105ae5b1e1d2358">@李韶华</a>的分享，可以适当调低BN自己的momentum参数</p><blockquote><p>bn自己有个momentum参数：x_new_running = (1 - momentum) * x_running + momentum * x_new_observed. momentum越接近0，老的running stats记得越久，所以可以得到更长序列的统计信息</p></blockquote><p>我简单看了下PyTorch 1.0的源码：<a href="https://link.zhihu.com/?target=https://github.com/pytorch/pytorch/blob/162ad945902e8fc9420cbd0ed432252bd7de673a/torch/nn/modules/batchnorm.py%23L24">https://github.com/pytorch/pytorch/blob/162ad945902e8fc9420cbd0ed432252bd7de673a/torch/nn/modules/batchnorm.py#L24</a>，BN类里面momentum这个属性默认为0.1，可以尝试调节下。</p><p>今天仍然还是在复现论文，编码。</p><p>上午：调bug，发现程序运行的贼慢，调了半天发现人家的代码跟实验室的GPU环境不太搭，看来得按图索骥，重新写。</p><p>下午，代码：照抄+理解了一部分代码，目前还没有动到要自己修改的代码部分。难顶。</p><p>光复现了，没有跟上论文阅读，知识面太窄。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习tricks </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 梯度累加 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跑实验中遇到的Bug</title>
      <link href="/2021/02/21/pao-shi-yan-zhong-yu-dao-de-bug/"/>
      <url>/2021/02/21/pao-shi-yan-zhong-yu-dao-de-bug/</url>
      
        <content type="html"><![CDATA[<h2 id="model-to-device-十分缓慢"><a href="#model-to-device-十分缓慢" class="headerlink" title="model.to(device)十分缓慢"></a>model.to(device)十分缓慢</h2><p>跑<code>eeqa</code>这个项目，运行到<code>model.to(device)</code>这行代码的时候，十分缓慢。按理来说，以往的项目<code>model</code>很快就能加载到GPU上的。</p><p>后来经过谷歌搜索之后，发现可能的原因是：<code>eeqa</code>这个项目使用的是<code>torch==1.2.0</code>，其对应的<code>CUDA==9.2</code>。而在实验室的服务器上<code>CUDA==11.1</code>。</p><p>我将<code>torch</code>版本修改为<code>1.7.0</code>（其对应的<code>CUDA==11.1</code>）之后，重新运行<code>model.to(device)</code>，能够迅速执行完成。</p><p>综上，坑的地方就是GPU版本的<code>torch</code>与实验室的服务器上<code>CUDA==11.1</code>上进行匹配。</p><p><strong>To Reproduce</strong></p><pre><code># takes seconds with CUDA 10.0 and minutes with CUDA 10.1torch.zeros(25000, 300, device=torch.device("cuda"))</code></pre><ul><li><a href="https://github.com/pytorch/pytorch/issues/27807">https://github.com/pytorch/pytorch/issues/27807</a></li></ul><p>上午调代码发现昨天的bug可以修复，不用重写代码了</p><p>下午把剩下的代码看完了，跑了实验，发现跑出来的结果和论文差别很大，不知道是不是原作者给的源代码有错。得发邮件问问了。</p><p>晚上，出去散步2小时，补了一些白天不懂的代码知识。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2021/02/20/linux-chang-yong-ming-ling/"/>
      <url>/2021/02/20/linux-chang-yong-ming-ling/</url>
      
        <content type="html"><![CDATA[<h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>经常需要将windows下的脚本里面的换行符转变成linux下的换行符，使用下面指令：</p><pre><code class="bash">sed -e 's/.$//' %s &gt; tmp;cat tmp &gt; %s</code></pre><pre><code class="shell">--train_file ../proc/data/ace-event/processed-data/default-settings/json/train_convert.json --dev_file ../proc/data/ace-event/processed-data/default-settings/json/dev_convert.json --test_file ../proc/data/ace-event/processed-data/default-settings/json/test_convert.json --train_batch_size 8 --eval_batch_size 8 --eval_per_epoch 20 --num_train_epochs 6 --output_dir trigger_qa_output --model_dir trigger_qa_output/epoch0-step0 --learning_rate 4e-5 --nth_query 5 --warmup_proportion 0.1 --do_train --do_eval --do_lower_case --model bert-base-uncased --gpu_ids 6,7,8,9</code></pre><p>sed -e ‘s/.$//‘ ./code/script_trigger_qa.sh &gt; tmp;cat tmp &gt; ./code/script_trigger_qa.sh</p><p>sed -e ‘s/.$//‘ ./code/script_args_qa_thresh.sh &gt; tmp;cat tmp &gt; ./code/script_args_qa_thresh.sh;rm tmp</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ACE2005数据集介绍</title>
      <link href="/2021/02/20/ace2005-shu-ju-ji-jie-shao/"/>
      <url>/2021/02/20/ace2005-shu-ju-ji-jie-shao/</url>
      
        <content type="html"><![CDATA[<h1 id="ACE2005数据理解"><a href="#ACE2005数据理解" class="headerlink" title="ACE2005数据理解"></a>ACE2005数据理解</h1><p><a href="https://catalog.ldc.upenn.edu/LDC2006T06">ACE2005数据集</a>,共包涵来自weblogs, broadcast news, newsgroups, broadcast conversation等六类资源的含三种语言(Mandarin Chinese, Standard Arabic, English)的语料,最初被用于2005 Automatic Content Extraction(ACE)的评测。该语料包含了由Linguistic Data Consortium(LDC)提供支持进行标注的多种类型实体、关系和事件；目前，ACE2005数据较多被用于事件抽取任务中。</p><h2 id="标注说明"><a href="#标注说明" class="headerlink" title="标注说明"></a>标注说明</h2><p>根据<a href="https://catalog.ldc.upenn.edu/docs/LDC2006T06/">README</a>,ACE2005的数据标注过程如下所示：</p><ul><li>首先分别进行1P和DUAL两轮标注，标注结果分别存储于对应语料的fp1和fp2目录下；</li><li>对以上两轮标注的结果进行裁决，将裁决后的标注结果存储于对应语料的adj目录下；</li><li>对于English语料，对adj/目录下标注结果再进行一步处理，将结果存储于timex2norm/目录下。</li></ul><h2 id="目录架构"><a href="#目录架构" class="headerlink" title="目录架构"></a>目录架构</h2><ul><li><a href="https://catalog.ldc.upenn.edu/docs/LDC2006T06/">官方README</a></li></ul><pre><code class="text">─Arabic              # 阿拉伯语语料库│  ├─bn│  │  ├─adj│  │  ├─altAdj│  │  ├─fp1│  │  └─fp2│  ├─nw│  │  ├─adj│  │  ├─altAdj│  │  ├─fp1│  │  └─fp2│  └─wl│      ├─adj│      ├─fp1│      └─fp2├─Chinese             # 中文语料│  ├─bn│  │  ├─adj│  │  ├─fp1│  │  └─fp2│  ├─nw│  │  ├─adj│  │  ├─fp1│  │  └─fp2│  └─wl│      ├─adj│      ├─fp1│      └─fp2├─dtd               # 数据说明文件  └─English           # 英文语料    ├─bc    │  ├─adj    │  ├─fp1    │  ├─fp2    │  └─timex2norm    ├─bn    │  ├─adj    │  ├─fp1    │  ├─fp2    │  └─timex2norm    ├─cts    │  ├─adj    │  ├─fp1    │  ├─fp2    │  └─timex2norm    ├─nw    │  ├─adj    │  ├─fp1    │  ├─fp2    │  └─timex2norm    ├─un    │  ├─adj    │  ├─fp1    │  ├─fp2    │  └─timex2norm    └─wl        ├─adj        ├─fp1        ├─fp2        └─timex2norm</code></pre><h2 id="文件解读"><a href="#文件解读" class="headerlink" title="文件解读"></a>文件解读</h2><p><strong>以English/bn/CNN_ENG_20030630_085848.18为例。</strong></p><p>根据<a href="https://catalog.ldc.upenn.edu/docs/LDC2006T06/">官方README</a>中第六部分，每份语料由如下所示的5个文件组成。</p><pre><code class="text">Source Text (.sgm) Files      - These files contain the source text files in an SGM format.  These files use the UNIX-style end of lines.  All .sgm files are  in UTF-8. ACE Program Format (APF) (.apf.xml) Files    - These files are in the official ACE annotation file format.  See      section 8 for more details. AG (.ag.xml) Files    - These are annotation files created with the LDC's annotation      toolkit.  These files have been convetered to the corresponding      .apf.xml files. ID table (.tab) Files    - These files store mapping tables between the IDs used in the      ag.xml files and their corresponding apf.xml files. AIF (.aif.xml) Files    - These are annotation files created with MITRE's Callisto      annotation tool.  Applies only to Arabic data produced by Valorem.</code></pre><p>以/English/bn/CNN_ENG_20030630_085848.18为例(<a href="https://catalog.ldc.upenn.edu/desc/addenda/LDC2006T06.txt">官网给出的样例数据也是CNN_ENG_20030630_085848.18</a>)，进行具体的解读:</p><h3 id="CNN-ENG-20030630-085848-18-sgm"><a href="#CNN-ENG-20030630-085848-18-sgm" class="headerlink" title="CNN_ENG_20030630_085848.18.sgm"></a>CNN_ENG_20030630_085848.18.sgm</h3><p>sgm文件即是数据源文件，给出了数据原文。其中，关于各个、等标签的含义，可见<strong>dtd/ace_source_sgml.v1.0.2.dtd</strong>。</p><pre><code class="text">&lt;DOC&gt;&lt;DOCID&gt; CNN_ENG_20030630_085848.18 &lt;/DOCID&gt;&lt;DOCTYPE SOURCE="broadcast news"&gt; NEWS STORY &lt;/DOCTYPE&gt;&lt;DATETIME&gt; 2003-06-30 09:23:30 &lt;/DATETIME&gt;&lt;BODY&gt;&lt;TEXT&gt;&lt;TURN&gt;a wildfire in california forced hundreds of people from their homes.the fire, near the historic state park started yesterday when atrailer, hauled by a pickup, ignited on the golden state freeway. thefire consumed more than 500 acres is only about 35% contained. noinjuries have been reported thankfully hat this time.&lt;/TURN&gt;&lt;/TEXT&gt;&lt;/BODY&gt;&lt;ENDTIME&gt; 2003-06-30 09:23:54 &lt;/ENDTIME&gt;&lt;/DOC&gt;</code></pre><h3 id="CNN-ENG-20030630-085848-18-apf-xml"><a href="#CNN-ENG-20030630-085848-18-apf-xml" class="headerlink" title="CNN_ENG_20030630_085848.18.apf.xml"></a>CNN_ENG_20030630_085848.18.apf.xml</h3><p>.apf.xml文件是<strong>ACE标注过实体、关系、事件等要素后以XML格式呈现的文本</strong>，具体可见<a href="https://catalog.ldc.upenn.edu/desc/addenda/LDC2006T06.txt">官网给出的样例</a>, .apf.xml文件的说明文档是<strong>dtd/ace_source_sgml.apf.v5.1.1.dtd</strong>。</p><p>根据<strong>dtd/ace_source_sgml.apf.v5.1.1.dtd</strong>, apf.xml文件中标注的要素包括</p><h4 id="ENTITY"><a href="#ENTITY" class="headerlink" title="ENTITY"></a>ENTITY</h4><ul><li><p>entity包含4个必须具备的属性:<strong>ID,TYPE,SUBTYPE和CLASS</strong></p></li><li><p>entity属性中的TYPE共有<strong>7类</strong>，分别是PER、ORG、LOC、GPE、FAC、VEH和WEA；每一类下都有若干对应的子类，具体可见<strong>dtd/ace_source_sgml.apf.v5.1.1.dtd</strong>文档；</p></li><li><p>entity可能包含的元素有<strong>entity mention</strong>、<strong>entity_attributes</strong>,<strong>external_link</strong>，具体可见文档</p><ul><li>entity_mention包含<strong>head和extent</strong>两个元素，ID和TYPNAM|NOM|PRO)两个必备的属性，以及LDCTYPE(NAM|NOM|BAR|PRO|WHQ|HLS|PTV|APP|ARC|EAP|NAMPRE|NOMPRE)、ROLE(PER|ORG|LOC|GPE)、METONYMY_MENTION (TRUE|FALSE)和LDCATR(TRUE|FALSE)四个非必需的属性。</li></ul></li></ul><p>LDCTYPE及其子类什么意思???METONYMY_MENTION (TRUE|FALSE)和LDCATR(TRUE|FALSE)什么意思???</p><ul><li>其他两个元素具体见文档说明</li></ul><h4 id="VALUE"><a href="#VALUE" class="headerlink" title="VALUE"></a><strong>VALUE</strong></h4><ul><li>value包含3个必须具备的属性:<strong>ID,TYPE和SUBTYPE</strong></li><li>value的TYPE共有5类，分别是Numeric、Contact-Info、Crime、Job-Title和Sentence；每一类下都有若干对应的子类，具体可见<strong>dtd/ace_source_sgml.apf.v5.1.1.dtd</strong>文档；</li><li>value包含的元素是<strong>value mention</strong>, 其必备属性是ID</li></ul><h4 id="timex2"><a href="#timex2" class="headerlink" title="timex2"></a><strong>timex2</strong></h4><ul><li><p>timex2的必备属性是ID,其他可选属性包括VAL、MOD(BEFORE|AFTER|ON_OR_BEFORE|…|APPROX)、ANCHOR_VAL、ANCHOR_DIR(WITHIN|…|BEFORE|AFTER)、SET(YES)、NON_SPECIFIC(YES)和COMMENT</p></li><li><p>timex2还包括<strong>timex2 mention</strong>, 其必备属性是ID</p><ul><li>timex2 mention的元素是<strong>extent</strong></li></ul><p>ldc_scope, char_span.seq_char是什么???</p></li></ul><h4 id="RELATION"><a href="#RELATION" class="headerlink" title="RELATION"></a>RELATION</h4><ul><li><p>relation包含2个必须具备的属性:<strong>ID和TYPE</strong>，其他可选属性包括SUBTYPE、MODALITY和TENSE</p></li><li><p>relation可能包含包含3个元素，分别是relation_argument,relation_argument+,和relation_mention*</p></li><li><p>Some Explaination</p><pre><code>METONYMY relations mark cross-type metonymies, and will not have relation mentions or values for MODALITY and TENSE.  For these reasons, we use "relation_mention*" instead of "relation_mention+", and "#IMPLIED" for MODALITY and TENSE. </code></pre></li></ul><h3 id="EVENT"><a href="#EVENT" class="headerlink" title="EVENT"></a>EVENT</h3><ul><li>relation包含6个必须具备的属性,分别是TYPE (Life|…|Justice)、SUBTYPE(Be-Born|…|Appeal)、MODALITY (Asserted|Other)、POLARITY (Positive|Negative)、GENERICITY(Generic|Specific)、TENSE(Past|…|Unspecified)</li><li>event可能包含包含2个元素，分别是event_argument*,event_mention+</li><li>注意，apf.xml文件中Event标注部分的<strong>anchor</strong>是event trigger.</li></ul><h3 id="CNN-ENG-20030630-085848-18-ag-xml"><a href="#CNN-ENG-20030630-085848-18-ag-xml" class="headerlink" title="CNN_ENG_20030630_085848.18.ag.xml"></a>CNN_ENG_20030630_085848.18.ag.xml</h3><p>.ag.xml文件是<strong>ACE ToolKit标注后的问题，根据ag.xml转化得到pdf.xml</strong>，, .ag.xml文件的说明文档是<strong>dtd/ace_source_sgml.ag-1.1.dtd</strong>。</p><h3 id="CNN-ENG-20030630-085848-18-tab"><a href="#CNN-ENG-20030630-085848-18-tab" class="headerlink" title="CNN_ENG_20030630_085848.18.tab"></a>CNN_ENG_20030630_085848.18.tab</h3><p>.tab文件存储了ag.xml文件中ID于apf.xml中标注结果的映射</p><p>以上是基本的数据理解，具体再根据读论文的情况进行补充。</p><p>如有不当与缺失之处，欢迎阅读此文的朋友一起交流。</p><p>/home/yc21/software/anaconda3/envs/eeqa/bin/python</p><pre><code>bash ./scripts/data/ace-event/collect_ace_event.sh ~/dataset/ACE2005/raw/ace_2005_td_v7</code></pre><p>sed -e ‘s/.$//‘</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Event Extraction by Answering (Almost) Natural Questions</title>
      <link href="/2021/02/18/event-extraction-by-answering-almost-natural-questions/"/>
      <url>/2021/02/18/event-extraction-by-answering-almost-natural-questions/</url>
      
        <content type="html"><![CDATA[<h2 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h2><ol><li>目前常见的argument extraction依赖于NER，一般都是先进行NER，NER之后的结果再进行argument extraction，这样会导致出现error propagation。因为一旦NER出错，argument extraction就会出错；</li><li><strong>忽略了不同论元角色之间的语义相似性</strong>。例如，在ACE2005数据集中，CONFLICT-&gt;ATTACK事件和JUSTICE-&gt;EXECUTE事件中的论元角色<code>PERSON</code>均指human being (who) is affected by an action.（某人被袭击了）。论文指出，如果不考虑论元角色之间的相似性会给模型的识别效果造成影响，尤其是对少样本（few shot）数据而言。</li><li>当前使用QA的方式进行NLP任务非常流行。</li></ol><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li><p>首次提出使用Question Answering/MRC的方式来解决事件抽取问题；</p></li><li><p>使用QA的机制，能够克服在Background中提到的几个问题：</p><p>（1）一种End to End的方式，<strong>不需要进行NER</strong>；</p><p>（2）能够实现迁移学习，对于不同的事件之间相同的role，<strong>能够共享信息</strong>；</p></li><li><p>对于<strong>抽取某些event arguments能够实现zero-shot</strong>；</p></li></ol><h2 id="Terminology-Related-Works"><a href="#Terminology-Related-Works" class="headerlink" title="Terminology/Related Works"></a>Terminology/Related Works</h2><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="Framework-Overview"><a href="#Framework-Overview" class="headerlink" title="Framework Overview"></a>Framework Overview</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gnrptinnhoj21020g8q5u.jpg"></p><p>如上图所示即为模型运作的流程图。很明显的看出有两大工作需要完成：</p><ol><li>Question Generation；</li><li>Question Answering，即<code>BERT_QA_Trigger</code>和<code>BERT_QA_Arg</code>；</li></ol><h3 id="Question-Generation-Strategies"><a href="#Question-Generation-Strategies" class="headerlink" title="Question Generation Strategies"></a>Question Generation Strategies</h3><p>分为trigger question generation和argument question generation两个部分。</p><p>对于trigger question generation，使用下面四种固定形式的question：</p><ol><li>what is the trigger?</li><li>trigger</li><li>action</li><li>verb</li></ol><p>对于argument question generation，有下面4种可选的question：</p><ol><li>Template 1：Role Name</li><li>Template 2：Type + Role</li><li>Template 3：Incorporating Annotation Guidelines</li><li>在Template 1/2/3后面加上“in <trigger>”</trigger></li></ol><p><strong>这4类问题模板层层递进，以question的形式给模型引入更多、更精确的<strong><strong>先验（语义）信息，</strong></strong>让模型更清楚自己需要找什么。</strong>而这也是QA/MRC模型的优势。</p><h3 id="Question-Answering-Models"><a href="#Question-Answering-Models" class="headerlink" title="Question Answering Models"></a>Question Answering Models</h3><p>对于$BERT_{Tr}$之后的结果，我们书写成$\bf E$，对于$BERT_{Arg}$之后的结果，我们书写成$\bf A1$。</p><h4 id="BERT-QA-Trigger"><a href="#BERT-QA-Trigger" class="headerlink" title="BERT_QA_Trigger"></a>BERT_QA_Trigger</h4><p>经过BERT之后进行一层线性层。<br>$$<br>P_{tr} = softmax(\bf{EW}_{tr} \in \mathbb{R}^{T} \times N)<br>$$</p><h4 id="BERT-QA-Arg"><a href="#BERT-QA-Arg" class="headerlink" title="BERT_QA_Arg"></a>BERT_QA_Arg</h4><p>$$<br>P_{s}(i) = softmax(a_{i}W_{s})<br>$$</p><p>$$<br>P_{e}(i)=softmax(a_iW_e)<br>$$</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>minimize the negative log-likelihood loss</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>预训练模型超全知识点梳理与面试必备高频FAQ</title>
      <link href="/2021/02/14/yu-xun-lian-mo-xing-chao-quan-zhi-shi-dian-shu-li-yu-mian-shi-bi-bei-gao-pin-faq/"/>
      <url>/2021/02/14/yu-xun-lian-mo-xing-chao-quan-zhi-shi-dian-shu-li-yu-mian-shi-bi-bei-gao-pin-faq/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文章地址——<a href="https://zhuanlan.zhihu.com/p/115014536">预训练模型超全知识点梳理与面试必备高频FAQ</a></p></blockquote><p><em><strong>预训练模型</strong></em>（Pre-trained Models,PTMs）的出现将NLP带入了一个全新时代。2020年3月18日，邱锡鹏老师发表了关于NLP预训练模型的综述<a href="https://arxiv.org/abs/2003.08271v2">《Pre-trained Models for Natural Language Processing: A Survey》</a>，这是一篇全面的综述，系统地对PTMs进行了归纳分类。</p><p>本文以此篇综述论文为主要参考，通过借鉴不同的归纳方法进行总结，同时也整合了专栏之前已经介绍过的《nlp中的词向量对比》和《nlp中的预训练语言模型总结》两篇文章，以QA形式对PTMs进行全面总结归纳。</p><p>获取<strong>总结图片下载</strong>以及<strong>单模型精读</strong>请到 <a href="https://link.zhihu.com/?target=https://github.com/loujie0822/Pre-trained-Models">github:NLP预训练模型的全面总结</a>，希望为大家的学习工作提供一些帮助。</p><blockquote><p>笔者注：本文总结与原综述论文也有一些不同之处（详见文末），如有错误或不当之处请指正。很多总结归纳的点不太好拿捏，大家多给意见～</p></blockquote><p><img src="https://pic3.zhimg.com/80/v2-0ace60ca3d843fc9b69c6965731f288e_720w.jpg"></p><h2 id="一、为什么要进行预训练？（Why？）"><a href="#一、为什么要进行预训练？（Why？）" class="headerlink" title="一、为什么要进行预训练？（Why？）"></a>一、为什么要进行预训练？（Why？）</h2><p>深度学习时代，为了充分训练深层模型参数并防止过拟合，通常需要更多标注数据喂养。在NLP领域，标注数据更是一个昂贵资源。PTMs从大量无标注数据中进行预训练使许多NLP任务获得显著的性能提升。总的来看，预训练模型PTMs的优势包括：</p><ol><li>在<strong>庞大的无标注数据</strong>上进行预训练可以获取<strong>更通用的语言表示</strong>，并有利于下游任务；</li><li>为模型提供了一个更好的初始化参数，在目标任务上具备更好的<strong>泛化性能</strong>、并加速收敛；</li><li>是一种有效的正则化手段，避免在小数据集上<strong>过拟合</strong>（一个随机初始化的深层模型容易对小数据集过拟合）；</li></ol><h2 id="二、什么是词嵌入和分布式表示？PTMs与分布式表示的关系？What"><a href="#二、什么是词嵌入和分布式表示？PTMs与分布式表示的关系？What" class="headerlink" title="二、什么是词嵌入和分布式表示？PTMs与分布式表示的关系？What"></a>二、什么是词嵌入和分布式表示？PTMs与分布式表示的关系？What</h2><p>词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量，这也是分布式表示：向量的每一维度都没有实际意义，而整体代表一个具体概念。</p><p>分布式表示相较于传统的独热编码（one-hot）表示具备更强的表示能力，而独热编码存在维度灾难和语义鸿沟（不能进行相似度计算）等问题。传统的分布式表示方法，如矩阵分解（SVD/LSA）、LDA等均是根据全局语料进行训练，是机器学习时代的产物。</p><p>PTMs也属于分布式表示的范畴，本文的PTMs主要介绍深度学习时代、自NNLM以来的 “modern” 词嵌入。</p><h2 id="三、PTMs有哪两大范式？对比不同的预训练编码器？How"><a href="#三、PTMs有哪两大范式？对比不同的预训练编码器？How" class="headerlink" title="三、PTMs有哪两大范式？对比不同的预训练编码器？How"></a>三、PTMs有哪两大范式？对比不同的预训练编码器？How</h2><p>PTMs的发展经历从浅层的词嵌入到深层编码两个阶段，按照这两个主要的发展阶段，我们归纳出PTMs两大范式：「浅层词嵌入」和「预训练编码器」。</p><p><strong>1、浅层词嵌入（</strong> Non-Contextual Embeddings<strong>）</strong></p><p>浅层词嵌入，这一类PTMs范式是我们通常所说的“词向量”，其主要特点是学习到的是上下文独立的静态词嵌入，其主要代表为NNLM、word2vec（CBOW、Skip-Gram）、Glove等。这一类词嵌入通常采取浅层网络进行训练，而应用于下游任务时，整个模型的其余部分仍需要从头开始学习。因此，对于这一范式的PTMs没有必要采取深层神经网络进行训练，采取浅层网络加速训练也可以产生好的词嵌入。</p><p>浅层词嵌入的主要缺陷为：</p><ul><li>词嵌入与上下文无关，每个单词的嵌入向量始终是相同，因此不能解决一词多义的问题。</li><li>通常会出现OOV问题，为了解决这个问题，相关文献提出了字符级表示或sub-word表示，如CharCNN 、FastText 和 Byte-Pair Encoding。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-59b1971adb8c0df0804fbc6f9f108b3f_720w.jpg" alt="图1: 常见的3种浅层词嵌入对比：NNLM、word2vec、Glove"></p><p>图1给出了三种常见的浅层词嵌入之间的对比，Glove可以被看作是更换了目标函数和权重函数的全局word2vec。此外，相关文献也提出了句子和文档级别的嵌入方式，如 Skip-thought 、Context2Vec 等。</p><p><strong>2、预训练编码器（</strong>Contextual Embeddings<strong>）</strong></p><p>第二类PTMs范式为预训练编码器，主要目的是通过一个预训练的编码器能够输出上下文相关的词向量，解决一词多义的问题。这一类预训练编码器输出的向量称之为「上下文相关的词嵌入」。</p><p><img src="https://pic2.zhimg.com/80/v2-2ed682fd131934fa9ddf6b483abc4de9_720w.jpg" alt="图2: NLP编码器对比"></p><p>图2给出了NLP各种编码器间的对比。PTMs中预训练编码器通常采用LSTM和Transformer（Transformer-XL），其中Transformer又依据其attention-mask方式分为Transformer-Encoder和Transformer-Decoder两部分。此外，Transformer也可看作是一种图神经网络GNN。</p><p>这一类「预训练编码器」范式的PTMs主要代表有<strong>ELMO</strong>、<strong>GPT-1</strong>、<strong>BERT</strong>、<strong>XLNet</strong>等。</p><h2 id="四、PTMs按照任务类型如何分类？"><a href="#四、PTMs按照任务类型如何分类？" class="headerlink" title="四、PTMs按照任务类型如何分类？"></a>四、PTMs按照任务类型如何分类？</h2><p>PTMs按照任务类型可分为2大类：<strong>监督学习</strong> 和 <strong>无监督学习/自监督学习</strong>。</p><p>监督学习在NLP-PTMs中的主要代表就是<strong>CoVe</strong>，CoVe作为机器翻译的encoder部分可以应用于多种NLP下游任务。除了CoVe外，NLP中的绝大多数PTMs属于自监督学习。</p><p>自监督学习是无监督学习的一种方法，自监督学习主要是利用辅助任务从大规模的无监督数据中挖掘自身的监督信息，通过这种构造的监督信息对网络进行训练，从而可以学习到对下游任务有价值的表征。因此，从“构造监督信息”这个角度来看，自监督也可看作是监督学习和无监督学习的一种融合。严格地讲，从是否由人工标注来看，自监督学习属于无监督学习的范畴。</p><p>综合各种自监督学习的分类方式，笔者将NLP-PTMs在自监督学习中分为两种类型：基于上下文（Context Based）和基于对比（Contrastive Based）。</p><p><strong>1、基于上下文（Context Based）</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新手入门 Kaggle NLP类比赛总结</title>
      <link href="/2021/02/12/xin-shou-ru-men-kaggle-nlp-lei-bi-sai-zong-jie/"/>
      <url>/2021/02/12/xin-shou-ru-men-kaggle-nlp-lei-bi-sai-zong-jie/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文地址：<a href="https://zhuanlan.zhihu.com/p/109992475">新手入门 Kaggle NLP类比赛总结</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 比赛 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Summary of The Models</title>
      <link href="/2021/02/04/summary-of-the-models/"/>
      <url>/2021/02/04/summary-of-the-models/</url>
      
        <content type="html"><![CDATA[<p>This is a summary of the models available in 🤗 Transformers. It assumes you’re familiar with the original <a href="https://arxiv.org/abs/1706.03762">transformer model</a>. For a gentle introduction check the <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">annotated transformer</a>. Here we focus on the high-level differences between the models. You can check them more in detail in their respective documentation. Also check out the <a href="https://huggingface.co/transformers/pretrained_models.html">pretrained model page</a> to see the checkpoints available for each type of model and all <a href="https://huggingface.co/models">the community models</a>.</p><p>Each one of the models in the library falls into one of the following categories:</p><ul><li><a href="https://huggingface.co/transformers/model_summary.html#autoregressive-models">Autoregressive models</a></li><li><a href="https://huggingface.co/transformers/model_summary.html#autoencoding-models">Autoencoding models</a></li><li><a href="https://huggingface.co/transformers/model_summary.html#seq-to-seq-models">Sequence-to-sequence models</a></li><li><a href="https://huggingface.co/transformers/model_summary.html#multimodal-models">Multimodal models</a></li><li><a href="https://huggingface.co/transformers/model_summary.html#retrieval-based-models">Retrieval-based models</a></li></ul><h2 id="Autoregressive-models"><a href="#Autoregressive-models" class="headerlink" title="Autoregressive models"></a>Autoregressive models</h2><p>Autoregressive models are pretrained on the classic language modeling task: <strong>guess the next token having read all the previous ones.</strong> They correspond to the decoder of the original transformer model, and a mask is used on top of the full sentence so that the attention heads can only see what was before in the text, and not what’s after. Although those models can be fine-tuned and achieve great results on many tasks, the most natural application is text generation. A typical example of such models is GPT.</p><h3 id="Original-GPT"><a href="#Original-GPT" class="headerlink" title="Original GPT"></a>Original GPT</h3><h3 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h3><h3 id="CTRL"><a href="#CTRL" class="headerlink" title="CTRL"></a>CTRL</h3><h3 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h3><h3 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h3><h3 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h3><h2 id="Autoencoding-models"><a href="#Autoencoding-models" class="headerlink" title="Autoencoding models"></a>Autoencoding models</h2><p>Autoencoding models <strong>are pretrained by corrupting the input tokens in some way and trying to reconstruct the original sentence</strong>. They correspond to the encoder of the original transformer model in the sense that they get access to the full inputs without any mask. Those models usually build a bidirectional representation of the whole sentence. They can be fine-tuned and achieve great results on many tasks such as text generation, but their most natural application is sentence classification or token classification. A typical example of such models is BERT.</p><p>Note that <strong>the only difference between autoregressive models and autoencoding models is in the way the model is pretrained.</strong> Therefore, the same architecture can be used for both autoregressive and autoencoding models. When a given model has been used for both types of pretraining, we have put it in the category corresponding to the article where it was first introduced.</p><h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><h3 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h3><h3 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h3><h3 id="DistilBERT"><a href="#DistilBERT" class="headerlink" title="DistilBERT"></a>DistilBERT</h3><h3 id="XLM"><a href="#XLM" class="headerlink" title="XLM"></a>XLM</h3><h3 id="XLM-RoBERTa"><a href="#XLM-RoBERTa" class="headerlink" title="XLM-RoBERTa"></a>XLM-RoBERTa</h3><h3 id="FlauBERT"><a href="#FlauBERT" class="headerlink" title="FlauBERT"></a>FlauBERT</h3><h3 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h3><h3 id="Funnel-Transformer"><a href="#Funnel-Transformer" class="headerlink" title="Funnel Transformer"></a>Funnel Transformer</h3><h3 id="Longformer"><a href="#Longformer" class="headerlink" title="Longformer"></a>Longformer</h3><h2 id="Sequence-to-sequence-models"><a href="#Sequence-to-sequence-models" class="headerlink" title="Sequence-to-sequence models"></a>Sequence-to-sequence models</h2><p>Sequence-to-sequence models use both the encoder and the decoder of the original transformer, either for translation tasks or by transforming other tasks to sequence-to-sequence problems. They can be fine-tuned to many tasks but their most natural applications are translation, summarization and question answering. The original transformer model is an example of such a model (only for translation), T5 is an example that can be fine-tuned on other tasks.</p><h3 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h3><h3 id="Pegasus"><a href="#Pegasus" class="headerlink" title="Pegasus"></a>Pegasus</h3><h3 id="MarianMT"><a href="#MarianMT" class="headerlink" title="MarianMT"></a>MarianMT</h3><h3 id="T5"><a href="#T5" class="headerlink" title="T5"></a>T5</h3><h3 id="MT5"><a href="#MT5" class="headerlink" title="MT5"></a>MT5</h3><h3 id="MBart"><a href="#MBart" class="headerlink" title="MBart"></a>MBart</h3><h3 id="ProphetNet"><a href="#ProphetNet" class="headerlink" title="ProphetNet"></a>ProphetNet</h3><h3 id="XLM-ProphetNet"><a href="#XLM-ProphetNet" class="headerlink" title="XLM-ProphetNet"></a>XLM-ProphetNet</h3><h2 id="Multimodal-models"><a href="#Multimodal-models" class="headerlink" title="Multimodal models"></a>Multimodal models</h2><p>Multimodal models mix text inputs with other kinds (e.g. images) and are more specific to a given task.</p><h3 id="MMBT"><a href="#MMBT" class="headerlink" title="MMBT"></a>MMBT</h3><h2 id="Retrieval-based-models"><a href="#Retrieval-based-models" class="headerlink" title="Retrieval-based models"></a>Retrieval-based models</h2><p>Some models use documents retrieval during (pre)training and inference for open-domain question answering, for example.</p><h3 id="DPR"><a href="#DPR" class="headerlink" title="DPR"></a>DPR</h3><h3 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h3>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Summary of The Tasks</title>
      <link href="/2021/02/02/summary-of-the-tasks/"/>
      <url>/2021/02/02/summary-of-the-tasks/</url>
      
        <content type="html"><![CDATA[<p>This page shows the most frequent use-cases when using the library. The models available allow for many different configurations and a great versatility in use-cases. The most simple ones are presented here, showcasing usage for tasks such as question answering, sequence classification, named entity recognition and others.</p><p>These examples leverage auto-models, which are classes that will instantiate a model according to a given checkpoint, automatically selecting the correct model architecture. Please check the <a href="https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel"><code>AutoModel</code></a> documentation for more information. Feel free to modify the code to be more specific and adapt it to your specific use-case.</p><p>In order for a model to perform well on a task, it must be loaded from a checkpoint <strong>corresponding to that task.</strong> These checkpoints are usually pre-trained on a large corpus of data and fine-tuned on a specific task. This means the following:</p><ul><li><strong>Not all models were fine-tuned on all tasks.</strong> If you want to fine-tune a model on a specific task, you can leverage one of the <code>run_$TASK.py</code> scripts in the <a href="https://github.com/huggingface/transformers/tree/master/examples">examples</a> directory.</li><li><strong>Fine-tuned models were fine-tuned on a specific dataset.</strong> This dataset may or may not overlap with your use-case and domain. As mentioned previously, you may leverage the <a href="https://github.com/huggingface/transformers/tree/master/examples">examples</a> scripts to fine-tune your model, or you may create your own training script.</li></ul><p>In order to do an inference on a task, several mechanisms are made available by the library:</p><ul><li><strong>Pipelines</strong>: very easy-to-use abstractions, which require as little as two lines of code.</li><li><strong>Direct model use</strong>: Less abstractions, but more flexibility and power via a direct access to a tokenizer (PyTorch/TensorFlow) and full inference capacity.</li></ul><h2 id="Sequence-Classification"><a href="#Sequence-Classification" class="headerlink" title="Sequence Classification"></a>Sequence Classification</h2><blockquote><p><strong>文本序列到类别</strong></p></blockquote><p>Sequence classification is the task of classifying sequences according to a given number of classes. </p><h2 id="Extractive-Question-Answering"><a href="#Extractive-Question-Answering" class="headerlink" title="Extractive Question Answering"></a>Extractive Question Answering</h2><blockquote><p><strong>文本序列到类别</strong></p></blockquote><p>Extractive Question Answering is the task of extracting an answer from a text given a question. </p><h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><blockquote><p><strong>文本序列到文本序列</strong></p></blockquote><p>Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular transformer-based models are trained using a variant of language modeling, e.g. BERT with masked language modeling, GPT-2 with causal language modeling.</p><p>Language modeling can be useful outside of pretraining as well, for example to shift the model distribution to be domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or on scientific papers e.g. <a href="https://huggingface.co/lysandre/arxiv-nlp">LysandreJik/arxiv-nlp</a>.</p><h3 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h3><p>Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for downstream tasks requiring bi-directional context, such as SQuAD (question answering, see <a href="https://arxiv.org/abs/1910.13461">Lewis, Lui, Goyal et al.</a>, part 4.2).</p><h3 id="Causal-Language-Modeling"><a href="#Causal-Language-Modeling" class="headerlink" title="Causal Language Modeling"></a>Causal Language Modeling</h3><p>Causal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting for generation tasks.</p><p>Usually, the next token is predicted by sampling from the logits of the last hidden state the model produces from the input sequence.</p><h3 id="Text-Generation"><a href="#Text-Generation" class="headerlink" title="Text Generation"></a>Text Generation</h3><p>In text generation (<em>a.k.a</em> <em>open-ended text generation</em>) the goal is to create a coherent portion of text that is a continuation from the given context. </p><h2 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h2><blockquote><p><strong>文本序列到类别</strong></p></blockquote><p>Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example, identifying a token as a person, an organisation or a location. An example of a named entity recognition dataset is the CoNLL-2003 dataset, which is entirely based on that task. If you would like to fine-tune a model on an NER task, you may leverage the <a href="https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_ner.py">run_ner.py</a> (PyTorch), <a href="https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_pl_ner.py">run_pl_ner.py</a> (leveraging pytorch-lightning) or the <a href="https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_tf_ner.py">run_tf_ner.py</a> (TensorFlow) scripts.</p><h2 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h2><blockquote><p><strong>文本序列到文本序列</strong></p></blockquote><p>Summarization is the task of summarizing a document or an article into a shorter text.</p><p>An example of a summarization dataset is the CNN / Daily Mail dataset, which consists of long news articles and was created for the task of summarization. If you would like to fine-tune a model on a summarization task, various approaches are described in this <a href="https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md">document</a>.</p><h2 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h2><blockquote><p><strong>文本序列到文本序列</strong></p></blockquote><p>Translation is the task of translating a text from one language to another.</p><p>An example of a translation dataset is the WMT English to German dataset, which has sentences in English as the input data and the corresponding sentences in German as the target data. If you would like to fine-tune a model on a translation task, various approaches are described in this <a href="https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md">document</a>.</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Glossary of 🤗 Transformers</title>
      <link href="/2021/02/01/glossary-of-hugging-face-transformers/"/>
      <url>/2021/02/01/glossary-of-hugging-face-transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="General-terms"><a href="#General-terms" class="headerlink" title="General terms"></a>General terms</h2><ul><li><code>autoencoding models</code>: see MLM</li><li><code>autoregressive models</code>: see CLM</li><li><code>CLM</code>: causal language modeling, a pretraining task where the model reads the texts in order and has to predict the next word. It’s usually done by reading the whole sentence but using a mask inside the model to hide the future tokens at a certain timestep.</li><li><code>MLM</code>: masked language modeling, a pretraining task where the model sees a corrupted version of the texts, usually done by masking some tokens randomly, and has to predict the original text.</li><li><code>multimodal</code>: a task that combines texts with another kind of inputs (for instance images).</li><li><code>NLG</code>: natural language generation, all tasks related to generating text ( for instance talk with transformers, translation)</li><li><code>NLP</code>: natural language processing, a generic way to say “deal with texts”.</li><li><code>NLU</code>: natural language understanding, all tasks related to understanding what is in a text (for instance classifying the whole text, individual words)</li><li><code>pretrained model</code>: a model that has been pretrained on some data (for instance all of Wikipedia). Pretraining methods involve <strong>a self-supervised objective</strong>, which can be reading the text and trying to predict the next word (see CLM) or masking some words and trying to predict them (see MLM).</li><li><code>RNN</code>: recurrent neural network, a type of model that uses a loop over a layer to process texts.</li><li><code>seq2seq or sequence-to-sequence</code>: models that generate a new sequence from an input, like translation models, or summarization models (such as <a href="https://huggingface.co/transformers/model_doc/bart.html">Bart</a> or <a href="https://huggingface.co/transformers/model_doc/t5.html">T5</a>).</li><li><code>token</code>: a part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a punctuation symbol.</li></ul><h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🤗 Transformers</title>
      <link href="/2021/01/30/hugging-face-transformers/"/>
      <url>/2021/01/30/hugging-face-transformers/</url>
      
        <content type="html"><![CDATA[<h2 id="Huggingface🤗"><a href="#Huggingface🤗" class="headerlink" title="Huggingface🤗"></a>Huggingface🤗</h2><p><a href="https://huggingface.co/">Huggingface（抱抱脸）</a>就是推出🤗 Transformers的公司，其总部位于纽约，是一家专注于自然语言处理、人工智能和分布式系统的创业公司。他们所提供的聊天机器人技术一直颇受欢迎，但更出名的是他们在NLP开源社区上的贡献。Huggingface一直致力于自然语言处理NLP技术的平民化（democratize），希望每个人都能用上最先进（SOTA, state-of-the-art）的NLP技术，而非困窘于训练资源的匮乏。</p><p>除了官网，他们在<a href="https://link.zhihu.com/?target=http://medium.com">http://medium.com</a>上也时常有高质量的分享，这里是他们的<a href="https://medium.com/huggingface/about">medium</a>主页。</p><h2 id="Transformer结构和BERTology家族"><a href="#Transformer结构和BERTology家族" class="headerlink" title="Transformer结构和BERTology家族"></a>Transformer结构和BERTology家族</h2><p>2020年，NLP最前沿的研究领域基本上已经被<strong>大型语言模型+迁移学习</strong>这一范式所垄断了。</p><p>2017年6月，研究人员提出了Transformer编码解码结构， 这一结构也成为了后续一系列工作的基石。2018年10月，基于Transformer，Google的研究人员发布了“全面超越人类”的BERT，一种融合了双向上下文信息预训练语言模型，该模型当时一举打破了11项纪录。从此之后，BERT的继任者们百花齐放，不断刷新各leaderboard最高成绩。现在，这些研究被称为BERTology，不完全的名单包括：Transformer-XL, XLNet, Albert, RoBERTa, DistilBERT, CTRL, …（<a href="https://zhuanlan.zhihu.com/p/105859953">文章介绍</a>）</p><p>BERTs模型虽然很香，但是用起来还是有一些障碍，比如：</p><ul><li>预训练需要大量的资源，一般研究者无法承担。以RoBERTa为例，它在160GB文本上利用1024块32GB显存的V100卡训练得到，如果换算成AWS上的云计算资源的话，但这一模型就需要10万美元的开销。</li><li>很多大机构的预训练模型被分享出来，但没有得到很好的组织和管理。</li><li>BERTology的各种模型虽然师出同源，但在模型细节和调用接口上还是有不少变种，用起来容易踩坑</li></ul><h2 id="Transformers名字的历史沿革"><a href="#Transformers名字的历史沿革" class="headerlink" title="Transformers名字的历史沿革"></a>Transformers名字的历史沿革</h2><p>为了让这些预训练语言模型使用起来更加方便，Huggingface在github上开源了<a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers">Transformers</a>。这个项目开源之后备受推崇，截止2020年5月，已经累积了26k的star和超过6.4k的fork。</p><p>Transformers最早的名字叫做<strong>pytorch-pretrained-bert</strong>，推出于google BERT之后。顾名思义，它是基于pytorch对BERT的一种实现。pytorch框架上手简单，BERT模型性能卓越，集合了两者优点的pytorch-pretrained-bert自然吸引了大批的追随者和贡献者。</p><p>其后，在社区的努力下，GPT、GPT-2、Transformer-XL、XLNET、XLM等一批模型也被相继引入，整个家族愈发壮大，这个库适时地更名为<strong>pytorch-transformers</strong>。</p><p>深度学习框架上一直存在着pytorch和tensorflow两大阵营，为了充分利用两个框架的优点，研究人员不得不经常在两者之间切换。<strong>Transformers</strong>的开发者们也敏锐地抓住了这一个痛点，加入了pytorch和TF2.0+的互操作性，模型之间可以方便地互相转换，算法的实现也是各自最native的味道。此举显然笼络了更多的自然语言研究人员和从业者投入麾下。而项目的名字，也顺理成章地改成了现在的Transformers。时至今日，Transformers已经在100+种人类语言上提供了32+种预训练语言模型。作为NLP的从业者，真的很难抵制住去一探究竟的诱惑。</p><h2 id="Transformers的设计理念"><a href="#Transformers的设计理念" class="headerlink" title="Transformers的设计理念"></a>Transformers的设计理念</h2><p>了解一下Transformers库的设计理念有助于更好地使用它。</p><ul><li><strong>分享及关爱</strong>。我们把各种模型和代码汇集到一处，从而使得更多人可以共享昂贵的训练资源。我们提供了简单一致的API，遵循经典的NLP Pipeline设计。</li><li><strong>性能优异且易于访问</strong>。一方面我们会尽可能让模型复现论文中的最优结果，一方面我们也尽力降低使用的门槛。我们的三大组件<code>configuration</code>, <code>tokenizer</code>和<code>model</code>都可以通过一致的from_pertrained()方法来实例化。</li><li><strong>注重可解释性和多样性</strong>。我们让使用者能够轻松访问到hidden states, attention weights和head importance这样的内部状态，从而更好地理解不同的模型是如何运作的。我们提供了类似GLUE, SuperGLuE, SQuAD这样的基准测试集的接入，方便对不同模型效果进行比较。</li><li><strong>推进传播最佳实践</strong>。在贴近原模型作者意图的基础上，我们的代码实现会尽可能地规范化，遵循业界的最佳实践，比如对于PyTorch和TensorFlow 2.0的完全兼容。</li><li><strong>从学界到业界</strong>。在工业级支持上，Transformers的模型支持TorchScript，一种PyTorch中创建可序列化可优化模型的方式，也能够和Tensorflow Extended框架相兼容。</li></ul><h2 id="Transformers的组件和模型架构"><a href="#Transformers的组件和模型架构" class="headerlink" title="Transformers的组件和模型架构"></a>Transformers的组件和模型架构</h2><p>Transformers提供了三个主要的组件。</p><ul><li><code>Configuration</code>配置类。存储<code>model</code>和<code>tokenizer</code>的参数，诸如词表大小，隐层维数，dropout rate等。配置类对深度学习框架是透明的。</li><li><code>Tokenizer</code>分词器类。<strong>每个模型都有对应的分词器</strong>，存储token到index的映射，负责每个模型特定的序列编码解码流程，比如BPE(Byte Pair Encoding)，SentencePiece等等。也可以方便地添加特殊token或者调整词表大小，如CLS、SEP等等。</li><li><code>Model</code>模型类。提供一个基类，实现模型的计算图和编码（Encoder）过程，实现前向传播过程，通过一系列self-attention层直到最后一个隐藏状态层。在最后一层基础上，根据不同的应用会再做些封装，比如XXXForSequenceClassification，XXXForMaskedLM这些派生类。</li></ul><p>Transformers的作者们还为以上组件提供了一系列<strong>Auto Classes</strong>，<strong>能够从一个短的别名（如bert-base-cased）里自动推测出来应该实例化哪种配置类、分词器类和模型类。</strong></p><p>Transformers提供两大类的模型架构：</p><ul><li>一类用于<strong>语言生成NLG</strong>任务，比如GPT、GPT-2、Transformer-XL、XLNet和XLM；</li><li>另一类主要用于<strong>语言理解NLU</strong>任务，如Bert、DistilBert、RoBERTa、XLM。</li></ul><h2 id="相关资源"><a href="#相关资源" class="headerlink" title="相关资源"></a>相关资源</h2><ul><li><a href="https://transformer.huggingface.co/">Write With Transformers</a>，这是Huggingface官方提供的一个web app，它展示了语言生成类模型的写作天赋，像GPT-2/XLNet等等。蛮有趣的一个应用，你可以体验一下和机器人一起创作的乐趣，当你才思枯竭的时候，只要按一下Tab键，就可以唤起AI来为你有模有样地写几段。</li><li><a href="https://github.com/huggingface/transfer-learning-conv-ai">transfer-learning-conv-ai</a>，迁移学习在对话AI上的SOTA应用，也是huggingface官方出品，后续我们有机会重点介绍一下。</li></ul><p>几个和Transformers有着类似使命的自然语言处理建模框架：</p><ul><li><a href="https://link.zhihu.com/?target=https://github.com/kaushaltrivedi/fast-bert">fast-bert</a>，Super easy library for BERT based NLP models</li><li><a href="https://link.zhihu.com/?target=https://github.com/deepset-ai/FARM">FARM</a>，Fast &amp; easy transfer learning for NLP. Harvesting language models for the industry</li><li><a href="https://link.zhihu.com/?target=https://github.com/facebookresearch/pytext">pytext</a>，A natural language modeling framework based on PyTorch</li><li><a href="https://link.zhihu.com/?target=https://github.com/explosion/spacy-transformers">spaCy</a>，spaCy pipelines for pre-trained BERT, XLNet and GPT-2</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>Huggineface官网，<a href="https://link.zhihu.com/?target=https://huggingface.co/">https://huggingface.co/</a></li><li>Arxiv上的官方白皮书，<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1910.03771">HuggingFace’s Transformers: State-of-the-art Natural Language Processing</a></li><li>Transformers文档，<a href="https://link.zhihu.com/?target=https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></li><li>Transformers github主页，<a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers/tree/master/examples">https://github.com/huggingface/transformers</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Unified MRC Framework for Named Entity Recognition</title>
      <link href="/2021/01/27/a-unified-mrc-framework-for-named-entity-recognition/"/>
      <url>/2021/01/27/a-unified-mrc-framework-for-named-entity-recognition/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文：<a href="https://arxiv.org/pdf/1910.11476.pdf">https://arxiv.org/pdf/1910.11476.pdf</a></p><p>源码：<a href="https://github.com/ShannonAI/mrc-for-flat-nested-ner">https://github.com/ShannonAI/mrc-for-flat-nested-ner</a></p><pre><code class="BibTex">@article{li2019unified, title={A Unified MRC Framework for Named Entity Recognition}, author={Li, Xiaoya and Feng, Jingrong and Meng, Yuxian and Han, Qinghong and Wu, Fei and Li, Jiwei}, journal={arXiv preprint arXiv:1910.11476}, year={2019}}</code></pre></blockquote><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ol><li>目前的NER任务分为nested NER和flat NER，两者通常是使用不同的model来解决的，很少有一个unified model来同时解决这个问题。（例如sequence labeling就只能解决flat NER）;</li><li>李宏毅老师在《人类语言处理》这门课有说，随着BERT的崛起，越来越多的NLP任务都可以使用MRC的框架来解决。</li></ol><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>提出了一个model：<code>BERT-MRC</code>，能够在这一个model里面同时解决nested NER和flat NER；</li><li><code>BERT-MRC</code>引入了MRC。这种基于query的问答方式，能够encode informative prior knowledge。</li></ol><h2 id="Terminologies"><a href="#Terminologies" class="headerlink" title="Terminologies"></a>Terminologies</h2><h3 id="Sequence-Labeling"><a href="#Sequence-Labeling" class="headerlink" title="Sequence Labeling"></a>Sequence Labeling</h3><p>输入一个Sequence，输出Sequence中每个token从属的那一个类别。</p><p>Sequence Labeling的任务包括Named Entity Recognition，POS Tagging等等。</p><h3 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h3><p>Traditionally，这个任务是用CRF求解的，例如之前在学习BiLSTM-CRF的时候讲到，对于一个Person实体，会有B-Person（Person的开头），I-Person（Person的中间），E-Person（Person的结束），S-Person（一个token就代表一个Person）。例如：</p><blockquote><p>Carl Young is so diligent!</p></blockquote><p>token Carl应该是B-Person，token Young应该是E-Person。</p><h3 id="Machine-Reading-Comprehension（MRC）"><a href="#Machine-Reading-Comprehension（MRC）" class="headerlink" title="Machine Reading Comprehension（MRC）"></a>Machine Reading Comprehension（MRC）</h3><p>这个任务很简单，就是给定一个问题，在text中找到问题的答案区间<code>[strat,end]</code>。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h3><p>根据原有的数据集，将其构建成许多的问答三元组：$(QUESTION,ANSWER,CONTEXT)$的形式，一个例子是$(q_y,x_{start,end},X)$。</p><p>关于Question的生成有很多的方式：</p><ol><li><strong>Position index of labels</strong>：a query is constructed using the index of a tag to , i.e.,“one”, “two”, “three”.</li><li><strong>Keyword</strong>：a query is the keyword describing the tag, e.g., the question query for tag ORG<br>is “organization”.</li><li><strong>Rule-based template filling</strong>：generates questions using templates. The query for tag ORG is “which organization is mentioned in the text”.</li><li><strong>Wikipedia</strong>：a query is constructed using its wikipedia definition. The query for tag ORG<br>is ”an organization is an entity comprising multiple people, such as an institution or an<br>association.”</li><li><strong>Synonyms</strong>：are words or phrases that mean exactly or nearly the same as the original keyword<br>extracted using the Oxford Dictionary. The query for tag ORG is “association”.</li><li><strong>Keyword + Synonyms</strong>：the concatenation of a keyword and its synonym.</li><li><strong>Annotation guideline notes</strong>：is the method we use in this paper. The query for tag ORG<br>is ”find organizations including companies,agencies and institutions”.</li></ol><h3 id="Model-Backbone"><a href="#Model-Backbone" class="headerlink" title="Model Backbone"></a>Model Backbone</h3><h4 id="1-BERT"><a href="#1-BERT" class="headerlink" title="1. BERT"></a>1. BERT</h4><p>Tokenization的输入是${[CLS],q_1,q_2,…,q_m,[SEP],x_1,x_2,…,x_n}$，最后只需要$X$的representation。</p><blockquote><p>$q_y$的representation不需要，它们的信息已经通过BERT的注意力机制嵌入到$X$的representation之中了。</p></blockquote><h4 id="2-Span-Selection"><a href="#2-Span-Selection" class="headerlink" title="2. Span Selection"></a>2. Span Selection</h4><p>得到$X$中每个token的representation之后，对每一个token representation都进行两个分类任务：</p><ul><li>Task 1：预测这个token会不会是$q_y$的start index；</li><li>Task 2：预测这个token会不会是$q_y$的end index。</li></ul><h5 id="start-index-classifier"><a href="#start-index-classifier" class="headerlink" title="start index classifier"></a>start index classifier</h5><p>由于Task 1和Task 2是一样的方法，所以只以Task 1为例。</p><p>对于一个token，将其输入到start index classifier中，得到$P_{is \ start \ index}$和$P_{is \ not \ start \ index}$。看两个概率哪个高，这个token就是哪个标签。<br>$$<br>P_{start} = softmax_{each \ row}(E \cdot T_{start} ) \in \mathbb {R}^{n \times 2}<br>$$</p><blockquote><p>这里使用softmax，而不是logistics~原因不详。记得区分logits和logistics的区别。</p></blockquote><h5 id="start-end-matching-classifier"><a href="#start-end-matching-classifier" class="headerlink" title="start-end matching classifier"></a>start-end matching classifier</h5><p>如果是flat NER，直接对每个start最近匹配end即可，但是在nested NER就不能这么做了。</p><p>对于所有被标记成start index和end index的token，它们之间两两配对，输入到start-end matching分类器中，预测它们是否彼此相互匹配。<br>$$<br>P_{i_{start},j_{end}} = sigmoid(m \cdot concat(E_{i_{start}},E_{j_{end}}))<br>$$</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>我们使用</p><ul><li><p>$Y_{start},Y_{end}$表示整个序列中，每个token的ground-truth labels。$P_{start},P_{end}$表示预测的结果。</p></li><li><p>$Y_{start,end}$表示整个序列中存在的span，$P_{start,end}$表示预测的结果。</p></li></ul><p>可以得到三个loss function：<br>$$<br>\mathcal {L_{start}} = CE(P_{start},Y_{start}) \<br>\mathcal {L_{end}} = CE(P_{end},Y_{end}) \<br>\mathcal {L_{span}} = CE(P_{start,end},Y_{start,end}) \<br>$$<br>总的loss function为：<br>$$<br>\mathcal{L} = \alpha \mathcal{L_{start}} + \beta \mathcal{L_{end}} + \gamma \mathcal{L_{span}}<br>$$<br>其中，$\alpha,\beta,\gamma$都是超参数。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><h2 id="更深的阅读"><a href="#更深的阅读" class="headerlink" title="更深的阅读"></a>更深的阅读</h2><ol><li>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions<br>for machine comprehension of text. arXiv preprint arXiv:1606.05250.</li><li>ELMO</li><li>Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019.<br>Entity-relation extraction as multi-turn question answering.In Proceedings of the 57th Conference of<br>the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume<br>1: Long Papers, pages 1340–1350.</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.bilibili.com/video/BV1wk4y127x4?from=search&amp;seid=14016735522804524512">Bert-MRC-A Unified MRC Framework for Named Entity Recognition论文讲解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> NLP </tag>
            
            <tag> MRC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is .bashrc file in Linux?</title>
      <link href="/2021/01/26/what-is-bashrc-file-in-linux/"/>
      <url>/2021/01/26/what-is-bashrc-file-in-linux/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>“graphical user interfaces make easy tasks easy, while command line interfaces make difficult tasks possible”</p></blockquote><p>The <strong>.bashrc</strong> file is a script file that’s executed when a user logs in. The file itself contains a series of configurations for the terminal session. This includes setting up or enabling: coloring, completion, shell history, command aliases, and more.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.journaldev.com/41479/bashrc-file-in-linux">What is .bashrc file in Linux?</a></li><li><a href="https://unix.stackexchange.com/questions/3467/what-does-rc-in-bashrc-stand-for">What does “rc” in .bashrc stand for?</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实验室服务器深度学习环境搭建</title>
      <link href="/2021/01/25/shi-yan-shi-fu-wu-qi-shen-du-xue-xi-huan-jing-da-jian/"/>
      <url>/2021/01/25/shi-yan-shi-fu-wu-qi-shen-du-xue-xi-huan-jing-da-jian/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="下载Anaconda"><a href="#下载Anaconda" class="headerlink" title="下载Anaconda"></a>下载Anaconda</h2><p>下载Anaconda，推荐到<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">清华镜像</a>里面下载，这样下载会比去Anaconda官网下载快很多。</p><p>首先检查Linux的版本：</p><pre><code class="shell"></code></pre>]]></content>
      
      
      <categories>
          
          <category> 实验室 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CRF Layer on the Top of BiLSTM(BiLSTM-CRF)</title>
      <link href="/2021/01/24/crf-layer-on-the-top-of-bilstm-bilstm-crf/"/>
      <url>/2021/01/24/crf-layer-on-the-top-of-bilstm-bilstm-crf/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原论文：</p><pre><code class="BibTex">@article{DBLP:journals/corr/LampleBSKD16,  author    = {Guillaume Lample and               Miguel Ballesteros and               Sandeep Subramanian and               Kazuya Kawakami and               Chris Dyer},  title     = {Neural Architectures for Named Entity Recognition},  journal   = {CoRR},  volume    = {abs/1603.01360},  year      = {2016},  url       = {http://arxiv.org/abs/1603.01360},  archivePrefix = {arXiv},  eprint    = {1603.01360},  timestamp = {Mon, 13 Aug 2018 16:47:39 +0200},  biburl    = {https://dblp.org/rec/journals/corr/LampleBSKD16.bib},  bibsource = {dblp computer science bibliography, https://dblp.org}}</code></pre></blockquote><blockquote><p>关于BiLSTM-CRF的详细讲解，可以通过<a href="https://createmomo.github.io/">https://createmomo.github.io/</a> 里面的文章进行系统学习，里面讲得非常非常好，给出了大量的实例。对CRF的train阶段（loss function）和test阶段（decode）都讲得非常详细。</p></blockquote><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>BiLSTM-CRF常见的应用场景如下：</p><ol><li>Named Entity Recognition</li></ol><h2 id="Model-Architecture⭐⭐⭐⭐⭐"><a href="#Model-Architecture⭐⭐⭐⭐⭐" class="headerlink" title="Model Architecture⭐⭐⭐⭐⭐"></a>Model Architecture⭐⭐⭐⭐⭐</h2><p>如果只是当一个掉包侠，那么一定要把握好模型的Architecture，知道输入和输出，以及需要调整的参数。</p><p>模型由BiLSTM和CRF两个部分构成，BiLSTM的输出作为CRF的输入。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmywrsgjtxj20bx0an74j.jpg"></p><p><code>BiLSTM</code>做的是多分类任务，其输出是某个word被标记为某个tag的概率（即<code>Emission Score</code>）。</p><p><code>CRF</code>吃进去这些<code>Emission Score</code>，然后在内部训练<code>Transition Score Matrix</code>，最后直接输出预测的tag序列。</p><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><h3 id="为什么需要CRF层？"><a href="#为什么需要CRF层？" class="headerlink" title="为什么需要CRF层？"></a>为什么需要CRF层？</h3><p>首先，如果不使用CRF层，只使用BiLSTM怎么做？</p><p>如下图所示，直接选择每个word的最大概率标签，然后将它们拼接在一起即可：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gotta5cdj7j20bx069mxo.jpg" alt="The BiLSTM model with out CRF layer output correct labels"></p><p>但是，我们并不能每次都能得到<strong>合理的结果</strong>，例如下图：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gottc0ztvaj20bx069aal.jpg" alt="The BiLSTM model with out CRF layer output some invalid label sequences"></p><p>而此时正是CRF的用武之地，**<code>CRF can automatically learn some constraints from the training dataset to make output predictions valid</code>**。</p><p>The constrains could be:</p><ul><li>The label of the first word in a sentence should start with “B-“ or “O”, not “I-“</li><li>“B-label1 I-label2 I-label3 I-…”, in this pattern, label1, label2, label3 … should be the same named entity label. For example, “B-Person I-Person” is valid, but “B-Person I-Organization” is invalid.</li><li>“O I-label” is invalid. The first label of one named entity should start with “B-“ not “I-“, in other words, the valid pattern should be “O B-label”</li><li>…</li></ul><h3 id="CRF的损失函数中使用了哪两种score？哪种score可以帮助CRF学习到constraints？"><a href="#CRF的损失函数中使用了哪两种score？哪种score可以帮助CRF学习到constraints？" class="headerlink" title="CRF的损失函数中使用了哪两种score？哪种score可以帮助CRF学习到constraints？"></a>CRF的损失函数中使用了哪两种score？哪种score可以帮助CRF学习到constraints？</h3><p>CRF的损失函数由两种类型的score构成——<code>Emission Score</code>和<code>Transition Score</code>，这两个score是CRF的核心概念。</p><h4 id="Emission-Score"><a href="#Emission-Score" class="headerlink" title="Emission Score"></a>Emission Score</h4><p>如下图红框部分，Emission Score就是BiLSTM的输出：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gottmzdsncj20fo0dnabt.jpg" alt="the emission scores come from the BiLSTM layer"></p><h4 id="Transition-Score"><a href="#Transition-Score" class="headerlink" title="Transition Score"></a>Transition Score</h4><p>我们使用$t_{y_iy_j}$来表示transition score，比如$t_{B-Person,I-Person} \ = \ 0.9$意味着标签转移$B-Person -&gt; I-Person$的score为0.9。以此类推，我们应该能够得到一个transition score matrix，如下图是一个示例：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gotttff5o7j20of0c1js0.jpg" alt="transition score matrix"></p><p>可以通过transition score matrix看出，CRF能够学习到许多有用的constraints：</p><ul><li>The label of the first word in a sentence should start with “B-“ or “O”, not “I-“ <strong>(The transtion scores from “START” to “I-Person or I-Organization” are very low.)</strong></li><li>“B-label1 I-label2 I-label3 I-…”, in this pattern, label1, label2, label3 … should be the same named entity label. For example, “B-Person I-Person” is valid, but “B-Person I-Organization” is invalid. <strong>(For example, the score from “B-Organization” to “I-Person” is only 0.0003 which is much lower than the others.)</strong></li><li>“O I-label” is invalid. The first label of one named entity should start with “B-“ not “I-“, in other words, the valid pattern should be “O B-label” <strong>(Again, for instance, the score tO,I−PersontO,I−Person is very small.)</strong></li><li>…</li></ul><h3 id="CRF的loss-function是什么？"><a href="#CRF的loss-function是什么？" class="headerlink" title="CRF的loss function是什么？"></a>CRF的loss function是什么？</h3><p>The CRF loss function is consist of the <strong>real path score</strong> and <strong>the total score of all the possible paths</strong>.The real path should have the highest score among those of all the possible paths.<br>$$<br>LossFunction =\frac {P_{RealPath}} {P_{total}} = \frac {P_{RealPath}} {P_1+P_2+…+P_N}<br>$$<br>Now, the questions are:</p><ol><li>How to define the score of a path?</li><li>How to calculate the total score of all possible paths?</li><li>When we calculate the total score, do we have to list all the possible paths? (The answer to this question is NO. )</li></ol><h3 id="如何计算Real-Path-Score？"><a href="#如何计算Real-Path-Score？" class="headerlink" title="如何计算Real Path Score？"></a>如何计算Real Path Score？</h3><p>首先是定义计算公式：$P_i = e^{S_i}$</p><p>Take the real path, <strong>“START B-Person I-Person O B-Organization O END”</strong>, we used before, for example:</p><ul><li>We have a sentence which has 5 words, $w_1,w_2,w_3,w_4,w_5$</li><li>We add two more extra words which denote the start and the end of a sentence, $w_0,w_6$</li><li>SiSi consists of 2 parts: $S_i=EmissionScore+TransitionScore$</li></ul><h4 id="Emission-Score-1"><a href="#Emission-Score-1" class="headerlink" title="Emission Score"></a>Emission Score</h4><p>$$<br>EmissionScore=x_{0,START}+x_{1,B−Person}+x_{2,I−Person}+x_{3,O}+x_{4,B−Organization}+x_{5,O}+x_{6,END}<br>$$</p><h4 id="Transition-Score-1"><a href="#Transition-Score-1" class="headerlink" title="Transition Score"></a>Transition Score</h4><p>$$<br>TransitionScore=t_{START,B-Person} + t_{B-Person,I-Person} + t_{I-Person,O} + t_{O,B-Organization} + t_{B_Organazation,O} + t_{O,END}<br>$$</p><p>其实上述的公式用可以表示为：<br>$$<br>P_i = e^{EmissionScore} \cdot e^{TransitionScore}<br>$$</p><h3 id="如何计算The-total-score-of-all-the-possible-paths？"><a href="#如何计算The-total-score-of-all-the-possible-paths？" class="headerlink" title="如何计算The total score of all the possible paths？"></a>如何计算The total score of all the possible paths？</h3><p>采用动态规划算法求解：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gotvyksolhj21on2dqqrp.jpg" alt="推导-2.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gotvyw75qqj21on2dq7jg.jpg" alt="推导-3.jpg"></p><h3 id="如何得到Transition-Score-Matrix"><a href="#如何得到Transition-Score-Matrix" class="headerlink" title="如何得到Transition Score Matrix?"></a>如何得到Transition Score Matrix?</h3><p><strong>它们都是超参数，需要通过机器学习，自己得到！</strong></p><h3 id="如何使用CRF进行inference？"><a href="#如何使用CRF进行inference？" class="headerlink" title="如何使用CRF进行inference？"></a>如何使用CRF进行inference？</h3><p>过程和计算The total score of all the possible paths类似，同样是动态规划，只不过是将previous更新时的<code>sum()</code>变成了<code>max()</code>操作。</p><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><p>使用<a href="https://pytorch-crf.readthedocs.io/en/stable/#"><code>torchcrf</code></a>包可以直接使用CRF Layer。</p><p><code>CRF</code> Module包含<code>forward()</code>和<code>decode()</code>两个重要的函数，分别对应<code>train</code>和<code>test</code>。</p><p><code>forward()</code>函数的输出是conditional log likelihood，至于为什么是log，这个是在我们推导loss function的时候定义的。而且需要注意的是，这里得到的是条件概率，而不是概率，这也是我们推导loss function的时候定义的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/">CRF Layer on the Top of BiLSTM - 1</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BiLSTM-CRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【李宏毅-深度学习】Structured Learning: Sequence Labeling</title>
      <link href="/2021/01/23/li-hong-yi-shen-du-xue-xi-structured-learning-sequence-labeling/"/>
      <url>/2021/01/23/li-hong-yi-shen-du-xue-xi-structured-learning-sequence-labeling/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="Sequence-Labeling"><a href="#Sequence-Labeling" class="headerlink" title="Sequence Labeling"></a>Sequence Labeling</h2><blockquote><p><strong>序列标注</strong>问题，输入一个Sequence，对应在其每个position输出一个class。</p></blockquote><p><img src="http://img-blog.csdnimg.cn/2020081811244361.png"></p><p>如上，RNN可以做这个问题。但是我们今天的方法与 RNN 有差别，具体差别将在下面讲述。</p><h3 id="POS-tagging"><a href="#POS-tagging" class="headerlink" title="POS tagging"></a>POS tagging</h3><p>今天以 POS tagging 为例，在英文上，这个问题甚至算是一个已经被解决的问题。<br><img src="https://img-blog.csdnimg.cn/20200818112710453.png"></p><h3 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h3><ul><li>Hidden Markov Model (HMM)</li><li>Conditional Random Field (CRF)</li><li>Structured Perceptron / SVM</li><li>Towards Deep Learning</li></ul><h2 id="Hidden-Markov-Model-HMM"><a href="#Hidden-Markov-Model-HMM" class="headerlink" title="Hidden Markov Model (HMM)"></a>Hidden Markov Model (HMM)</h2><p><img src="http://img-blog.csdnimg.cn/20200818112948306.png"><br>我们人类怎么产生一个句子呢？首先，在心中产生一个文法，接着匹配单词。这是 <strong>HMM 的假设</strong>。<br><img src="http://img-blog.csdnimg.cn/20200818113120841.png"><br>我们假设，我们脑中的文法，是<strong>马尔可夫链（Markov Chain）</strong>构成的。我们的某个文法，有自己出现的概率。这是第一步。<br><img src="https://img-blog.csdnimg.cn/20200818113213123.png"><br>接着，第二步，我们衡量各个单词在词义上的出现概率。<br><img src="https://img-blog.csdnimg.cn/20200818113602923.png"><br>最终，我们由条件概率公式，得到$P(x,y)$。<br><img src="https://img-blog.csdnimg.cn/2020081811370770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>总结一下，第一步是计算 <strong>Transition probability</strong> ，第二步是计算 <strong>Emission probability</strong> 。</p><h3 id="Estimating-the-probabilities"><a href="#Estimating-the-probabilities" class="headerlink" title="Estimating the probabilities"></a>Estimating the probabilities</h3><p><img src="https://img-blog.csdnimg.cn/20200818113853876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br><strong>HMM是基于计数的，依赖于我们的training data</strong>。我们由训练数据得到各个概率。</p><h3 id="How-to-do-POS-Tagging"><a href="#How-to-do-POS-Tagging" class="headerlink" title="How to do POS Tagging?"></a>How to do POS Tagging?</h3><p><img src="https://img-blog.csdnimg.cn/20200818113953206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上， x 是给定的，我们找出最可能的 y 即可。上面也说明了Hidden Markov Model名字的由来。<br><img src="https://img-blog.csdnimg.cn/20200818114104206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>但是，我们找 y 的过程，需要穷举 y 吗？可以使用 <strong>Viterbi Algorithm</strong> 。</p><h3 id="Drawbacks"><a href="#Drawbacks" class="headerlink" title="Drawbacks"></a>Drawbacks</h3><p><img src="https://img-blog.csdnimg.cn/20200818114607525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>HMM 存在哪些问题呢？如上，尽管我们通过数据训练得到了两种 probability 的数据，并且也得出$y_1$最有可能是$V$的结论。</p><p>但是，$N: D: a$这个序列其实是实实在在出现过的数据，而我们实际使用时，遇到了$N: : a$却无法把$D$填在其中。</p><p>因此，<strong>HMM 会“脑补”一些序列</strong>。这件事有好有坏：</p><ul><li>可以弥补 training data 的不足；</li><li>但是容易出现如上的问题。</li></ul><p>或许我们可以建立更复杂的模型弥补缺点；但是，我们还可以使用 CRF ，并且不用建立更复杂的模型。</p><h2 id="Conditional-Random-Field-（CRF）"><a href="#Conditional-Random-Field-（CRF）" class="headerlink" title="Conditional Random Field （CRF）"></a>Conditional Random Field （CRF）</h2><p><img src="https://img-blog.csdnimg.cn/20200818115106735.png"><br>其假设，$P(x,y)$与$exp(w \cdot \phi(x,y))$成正比。可以做出变换如上。</p><p>其实，CRF 与 HMM 的区别并不大。</p><blockquote><p>上述 $P(x) = \sum_{y’}P(x,y’)$ 使用的是全概率公式计算的。 </p></blockquote><h3 id="P-x-y-for-CRF"><a href="#P-x-y-for-CRF" class="headerlink" title="P(x, y) for CRF"></a>P(x, y) for CRF</h3><p><img src="https://img-blog.csdnimg.cn/20200818115228352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>在 HMM 中，对 P(x, y) 取 log 。<br><img src="https://img-blog.csdnimg.cn/20200818115340959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，把最后一项做一个转换。为什么可以做这个转换呢？我们下面举一个直观的例子。<br><img src="https://img-blog.csdnimg.cn/20200818115454666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，李老师举了一个很直观的例子（其实最后，前面的求和符号应该去掉）。</p><p>我们对所有的项都做这个转换。<br><img src="https://img-blog.csdnimg.cn/20200818115632613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>这样我们就相当于对两个向量求了内积。<strong>这样，HMM 与 CRF 其实就是一回事了。</strong></p><p>可以说，HMM是CRF的一种特殊的情况，CRF的w并不做任何限制，是一个需要通过训练得到的vector，而HMM是限制w的取值之后得到的结果。<br><img src="http://img-blog.csdnimg.cn/20200818115929567.png"><br>如上，我们在训练时，w是可正可负的，如果 w 是大于 0 的话，那由$P = e^w$转换，得到的$P$大于 1 ，则很奇怪。</p><p><strong>因此，我们在 CRF 中，不做等号，而作“正比”关系。</strong></p><h3 id="Feature-Vector——-phi-x-y"><a href="#Feature-Vector——-phi-x-y" class="headerlink" title="Feature Vector——$\phi(x,y)$"></a>Feature Vector——$\phi(x,y)$</h3><p><img src="https://img-blog.csdnimg.cn/20200818120259941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，特征向量可分为两部分：</p><ul><li>第一部分，是 tag 与 word 间的关系，并且维度是二者维度的乘积；</li></ul><p><img src="https://img-blog.csdnimg.cn/20200818120502645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"></p><ul><li>第二部分，是 tag 与 tag 之间的关系，要注意，要带上 Start 与 End 的向量。</li><li>此外，你还可以在 CRF 中自己定义Feature Vector。</li></ul><h3 id="Training-Criterion"><a href="#Training-Criterion" class="headerlink" title="Training Criterion"></a>Training Criterion</h3><p><img src="https://img-blog.csdnimg.cn/20200818120728255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>有些像交叉熵，最大化我们见过的序列对，最小化我们没见过的序列对。</p><p>使用梯度上升即可。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><img src="http://img-blog.csdnimg.cn/20200818121247699.png"><br>经过一些运算，得到一个具有物理含义的微分式子：</p><ul><li>出现次数越多，对应地 w 越大；</li><li>如果 s 与 t 出现在任意其他对中出现的次数也很大，那则应该把 w 减小。</li><li>二者有 trade-off ；</li><li>无需穷举$y’$ ，可以用 Viterbi Algorithm 来求解。</li></ul><p><img src="https://img-blog.csdnimg.cn/20200818121337423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>有特征向量定义，得到最终更新式如上。</p><h3 id="CRF-v-s-HMM"><a href="#CRF-v-s-HMM" class="headerlink" title="CRF v.s. HMM"></a>CRF v.s. HMM</h3><p><img src="https://img-blog.csdnimg.cn/20200818121609862.png"><br>如上，HMM 并不减少没出现过的项的几率。还是之前的例子，CRF 更倾向于通过历史数据<code>调整概率值</code>。</p><h4 id="Synthetic-Data"><a href="#Synthetic-Data" class="headerlink" title="Synthetic Data"></a>Synthetic Data</h4><p><img src="https://img-blog.csdnimg.cn/20200818121655743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，是一个对比 CRF 与 HMM 的实验。<br><img src="https://img-blog.csdnimg.cn/20200818121900512.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，横纵轴代表 HMM 与 CRF 犯错的百分比。当 α &lt; 1 2 \alpha &lt; \frac{1}{2}<em>α</em>&lt;21​ 时，CRF 的结果更好。</p><h2 id="Structured-Perceptron-SVM"><a href="#Structured-Perceptron-SVM" class="headerlink" title="Structured Perceptron / SVM"></a>Structured Perceptron / SVM</h2><h3 id="Structured-Perceptron"><a href="#Structured-Perceptron" class="headerlink" title="Structured Perceptron"></a>Structured Perceptron</h3><p><img src="https://img-blog.csdnimg.cn/20200818122054559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>其在本文题的建模如上。</p><h3 id="Structured-Perceptron-v-s-CRF"><a href="#Structured-Perceptron-v-s-CRF" class="headerlink" title="Structured Perceptron v.s. CRF"></a>Structured Perceptron v.s. CRF</h3><p><img src="https://img-blog.csdnimg.cn/20200818122151325.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，Structured Perceptron 其实与 CRF 的更新式很像。</p><h3 id="Structured-SVM"><a href="#Structured-SVM" class="headerlink" title="Structured SVM"></a>Structured SVM</h3><p><img src="https://img-blog.csdnimg.cn/202008181223375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，Structured SVM 中的特别之处就是，要考虑 error 这件事情。<br><img src="https://img-blog.csdnimg.cn/20200818122748511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，我们不再是找 a r g max ⁡ y [ w ⋅ ϕ ( x n , y ) ] arg \max_y [w\cdot \phi(x^n,y)]<em>a<strong>r</strong>g</em>max<em>y</em>​[<em>w</em>⋅<em>ϕ</em>(<em>x**n</em>,<em>y</em>)] ，而是带上了 Δ ( y ^ n , y ) \Delta (\hat{y}^n,y)Δ(<em>y</em>^​<em>n</em>,<em>y</em>) 。如果我们使用如上方式建模，定义 Δ ( y ^ n , y ) \Delta (\hat{y}^n,y)Δ(<em>y</em>^​<em>n</em>,<em>y</em>) ，可以求解，否则，不一定能求解。</p><h3 id="Performance-of-Different-Approaches"><a href="#Performance-of-Different-Approaches" class="headerlink" title="Performance of Different Approaches"></a>Performance of Different Approaches</h3><p><img src="https://img-blog.csdnimg.cn/20200818122921269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，Structured SVM 是表现最好的。</p><h2 id="How-about-RNN"><a href="#How-about-RNN" class="headerlink" title="How about RNN?"></a>How about RNN?</h2><p><img src="https://img-blog.csdnimg.cn/20200818123401304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，RNN的缺点是，没有考虑整个序列。但是双向的 RNN 可能会弥补这个缺点。</p><p>此外，我们可以把 label 的关系考虑进去，比如一定不考虑“声母接声母”。</p><p>但是，RNN 可以 Deep ，因此最终是 RNN 最强。</p><h3 id="Integrated-together⭐"><a href="#Integrated-together⭐" class="headerlink" title="Integrated together⭐"></a>Integrated together⭐</h3><p>我们可以将二者结合。<br><img src="https://img-blog.csdnimg.cn/20200818123657941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如上，我们用深度神经网络来计算 $P(x|y)$ ，再赋给结构化学习。我们做了一个小小的改变: $P(x|y) = \frac {P(y|x)P(x)} {P(y)}$，并且不管$P(x)$。</p><p><img src="https://img-blog.csdnimg.cn/20200818123746237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxNTYwOQ==,size_16,color_FFFFFF,t_70#pic_center"><br>此外，在 Semantic Tagging 中，二者结合也十分常见。</p><h2 id="Concluding-Remarks"><a href="#Concluding-Remarks" class="headerlink" title="Concluding Remarks"></a>Concluding Remarks</h2><p><img src="http://img-blog.csdnimg.cn/2020081812382435.png"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程笔记 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> CRF </tag>
            
            <tag> HMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nn.Sequential与nn.ModuleList的区别</title>
      <link href="/2021/01/21/nn-sequential-yu-nn-modulelist-de-qu-bie/"/>
      <url>/2021/01/21/nn-sequential-yu-nn-modulelist-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<p>简单的罗列一些重要的信息：</p><p><code>nn.ModuleList</code></p><ul><li>相当于Python中的<code>list</code>，当你想在PyTorch中使用<code>list</code>装载Module时，不能用<code>list</code>，而应该使用<code>nn.ModuleList</code>；</li><li>没有<code>forward</code>方法，只是一个容器</li></ul><p><code>nn.Sequential</code></p><ul><li>按照<strong>顺序</strong>将Module们组织在一起，前一个Module的输出是后一个Module的输入；</li><li>拥有<code>forward方法</code>，可以将其看做是一个Module直接使用</li></ul><p>参考资料</p><ol><li><a href="https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463">when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyCharm Tips</title>
      <link href="/2021/01/21/pycharm-tips/"/>
      <url>/2021/01/21/pycharm-tips/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="Python代码调试跳出for循环"><a href="#Python代码调试跳出for循环" class="headerlink" title="Python代码调试跳出for循环"></a>Python代码调试跳出for循环</h2><p>debug模式下，一步一步来，当遇到for循环就很苦恼，那么如何在遇到for循环后不再一步一步执行。</p><p><strong>可以在for循环结束后的下一句打上断点，按快捷键F9即可。</strong></p><p><strong>同样，如果我只想看断点处的程序状态，只需要在目标处打上断点，按快捷键F9即可快速运行到每个断点处。</strong></p><p>参考资料：</p><ol><li><a href="https://blog.csdn.net/qq_36441393/article/details/109848851">pycharm python 代码调试跳出for 循环</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> PyCharm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具使用 </tag>
            
            <tag> PyCharm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python学习之*arg、**kwargs</title>
      <link href="/2021/01/19/python-xue-xi-zhi-arg-kwargs/"/>
      <url>/2021/01/19/python-xue-xi-zhi-arg-kwargs/</url>
      
        <content type="html"><![CDATA[<p>*args和**kwargs是在Python函数中常见的参数，但是每次见到都是一知半解，所以打算这次用实例来记录一下：</p><h2 id="Introduction-to-args-and-kwargs-in-Python"><a href="#Introduction-to-args-and-kwargs-in-Python" class="headerlink" title="Introduction to *args and **kwargs in Python"></a>Introduction to *args and **kwargs in Python</h2><p>In Python, we can pass a variable number of arguments to a function using special symbols. There are two special symbols:</p><ol><li>*args (Non Keyword Arguments)</li><li>**kwargs (Keyword Arguments)</li></ol><p>We use *args and **kwargs as an argument when we are unsure about the number of arguments to pass in the functions.</p><hr><h2 id="Python-args"><a href="#Python-args" class="headerlink" title="Python *args"></a>Python *args</h2><p>As in the above example we are not sure about the number of arguments that can be passed to a function. Python has *args which allow us to pass the variable number of non keyword arguments to function.</p><p>In the function, we should use an asterisk <code>*</code> before the parameter name to pass variable length arguments.The arguments are passed as a tuple and these passed arguments make tuple inside the function with same name as the parameter excluding asterisk <code>*</code>.</p><h3 id="Example-2-Using-args-to-pass-the-variable-length-arguments-to-the-function"><a href="#Example-2-Using-args-to-pass-the-variable-length-arguments-to-the-function" class="headerlink" title="Example 2: Using *args to pass the variable length arguments to the function"></a>Example 2: Using *args to pass the variable length arguments to the function</h3><pre><code class="python">def adder(*num):    sum = 0        for n in num:        sum = sum + n    print("Sum:",sum)adder(3,5)adder(4,5,6,7)adder(1,2,3,5,6)</code></pre><p>When we run the above program, the output will be</p><pre><code class="tex">Sum: 8Sum: 22Sum: 17</code></pre><p>In the above program, we used *num as a parameter which allows us to pass variable length argument list to the <code>adder()</code> function. Inside the function, we have a loop which adds the passed argument and prints the result. We passed 3 different tuples with variable length as an argument to the function.</p><hr><h2 id="Python-kwargs"><a href="#Python-kwargs" class="headerlink" title="Python **kwargs"></a>Python **kwargs</h2><p>Python passes variable length non keyword argument to function using *args but we cannot use this to pass keyword argument. For this problem Python has got a solution called **kwargs, it allows us to pass the variable length of keyword arguments to the function.</p><p>In the function, we use the double asterisk <code>**</code> before the parameter name to denote this type of argument. The arguments are passed as a dictionary and these arguments make a dictionary inside function with name same as the parameter excluding double asterisk <code>**</code>.</p><h3 id="Example-3-Using-kwargs-to-pass-the-variable-keyword-arguments-to-the-function"><a href="#Example-3-Using-kwargs-to-pass-the-variable-keyword-arguments-to-the-function" class="headerlink" title="Example 3: Using **kwargs to pass the variable keyword arguments to the function"></a>Example 3: Using **kwargs to pass the variable keyword arguments to the function</h3><pre><code class="python">def intro(**data):    print("\nData type of argument:",type(data))    for key, value in data.items():        print("{} is {}".format(key,value))intro(Firstname="Sita", Lastname="Sharma", Age=22, Phone=1234567890)intro(Firstname="John", Lastname="Wood", Email="johnwood@nomail.com", Country="Wakanda", Age=25, Phone=9876543210)</code></pre><p>When we run the above program, the output will be</p><pre><code class="tex">Data type of argument: &lt;class 'dict'&gt;Firstname is SitaLastname is SharmaAge is 22Phone is 1234567890Data type of argument: &lt;class 'dict'&gt;Firstname is JohnLastname is WoodEmail is johnwood@nomail.comCountry is WakandaAge is 25Phone is 9876543210</code></pre><p>In the above program, we have a function <code>intro()</code> with **data as a parameter. We passed two dictionaries with variable argument length to the <code>intro()</code> function. We have for loop inside <code>intro()</code> function which works on the data of passed dictionary and prints the value of the dictionary.</p><hr><h2 id="Things-to-Remember"><a href="#Things-to-Remember" class="headerlink" title="Things to Remember:"></a>Things to Remember:</h2><ul><li>*args and *kwargs are special keyword which allows function to take variable length argument.</li><li>*args passes variable number of non-keyworded arguments list and on which operation of the list can be performed.</li><li>**kwargs passes variable number of keyword arguments dictionary to function on which operation of a dictionary can be performed.</li><li>*args and **kwargs make the function flexible.</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="https://www.programiz.com/python-programming/args-and-kwargs">Python *args and **kwargs</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python学习之argparse模块</title>
      <link href="/2021/01/18/python-xue-xi-zhi-argparse-mo-kuai/"/>
      <url>/2021/01/18/python-xue-xi-zhi-argparse-mo-kuai/</url>
      
        <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p><code>argparse</code>是python用于解析命令行参数和选项的标准模块，用于代替已经过时的<code>optparse</code>模块。<code>argparse</code>模块的作用是用于解析命令行参数。</p><p>我们很多时候，需要用到解析命令行参数的程序。</p><h2 id="二、使用步骤⭐"><a href="#二、使用步骤⭐" class="headerlink" title="二、使用步骤⭐"></a>二、使用步骤⭐</h2><p>我们常常可以把<code>argparse</code>的使用简化成下面四个步骤</p><ol><li><code>import argparse</code></li><li><code>parser = argparse.ArgumentParser()</code></li><li><code>parser.add_argument()</code></li><li><code>parser.parse_args()</code></li></ol><p><strong>上面四个步骤解释如下：首先导入该模块；然后创建一个解析对象；然后向该对象中添加你要关注的命令行参数和选项，每一个add_argument方法对应一个你要关注的参数或选项；最后调用<code>parse_args()</code>方法进行解析；解析成功之后即可使用。</strong></p><h2 id="三、例子讲解"><a href="#三、例子讲解" class="headerlink" title="三、例子讲解"></a>三、例子讲解</h2><p>下面我们通过一个例子来进行讲解说明</p><p>我们可以看到上面的第二个步骤，<code>parser = argparse.ArgumentParser()</code></p><p>它的作用就是：当调用<code>parser.print_help()</code>或者运行程序时由于参数不正确(此时python解释器其实也是调用了<code>print_help()</code>方法)时，会打印这些描述信息，一般只需要传递<code>description</code>参数。</p><p>下面会有例子输出，首先给出代码：</p><pre><code class="python">#-*- coding: UTF-8 -*-import argparse   #步骤一def parse_args():    """    :return:进行参数的解析    """    description = "you should add those parameter"                   # 步骤二    parser = argparse.ArgumentParser(description=description)        # 这些参数都有默认值，当调用parser.print_help()或者运行程序时由于参数不正确(此时python解释器其实也是调用了pring_help()方法)时，                                                                     # 会打印这些描述信息，一般只需要传递description参数，如上。    help = "The path of address"    parser.add_argument('--addresses',help = help)                   # 步骤三，后面的help是我的描述    args = parser.parse_args()                                       # 步骤四              return argsif __name__ == '__main__':    args = parse_args()    print(args.addresses)            #直接这么获取即可。</code></pre><p>上面四个步骤已经分别对应上了，当我们在命令行敲入：</p><pre><code class="python">python arg.py -h </code></pre><p>输出提示为：</p><p><img src="https://pic2.zhimg.com/80/v2-3eca02d154257faa18387db9374278dd_720w.png"></p><h2 id="四、如何获得命令参数值"><a href="#四、如何获得命令参数值" class="headerlink" title="四、如何获得命令参数值"></a>四、如何获得命令参数值</h2><pre><code class="python">我们可以直接通过args.addresses获得它的参数值。当我们敲入python arg.py --addresses this-is-parameter-of-addresses 命令时</code></pre><p>会输出this-is-parameter-of-addresses</p><p>到这里就总结了<code>argparse</code>模块常见的一些常见的用法。</p><h2 id="五、add-argument"><a href="#五、add-argument" class="headerlink" title="五、add_argument()"></a>五、add_argument()</h2><p>用<code>argparse</code>模块让python脚本接收参数时，对于<strong>True/False</strong>类型的参数，向<code>add_argument()</code>方法中加入参数<code>action=‘store_true’/‘store_false’</code>。</p><p>顾名思义，<code>store_true</code>就代表着一旦有这个参数，做出动作“将其值标为True”，也就是没有时，默认状态下其值为False。反之亦然，<code>store_false</code>也就是默认为True，一旦命令中有此参数，其值则变为False。</p><p>代码如下：</p><pre><code class="python">ce#-*- coding: UTF-8 -*-import argparse   #步骤一def parse_args():    """    :return:进行参数的解析    """    description = "you should add those parameter"                   # 步骤二    parser = argparse.ArgumentParser(description=description)        # 这些参数都有默认值，当调用parser.print_help()或者运行程序时由于参数不正确(此时python解释器其实也是调用了pring_help()方法)时，                                                                     # 会打印这些描述信息，一般只需要传递description参数，如上。    help = "The path of address"    parser.add_argument('--addresses',default=True,action='store_false',help = help)                   # 步骤三，后面的help是我的描述    args = parser.parse_args()                                       # 步骤四              return argsif __name__ == '__main__':    args = parse_args()    print(args.addresses)            #直接这么获取即可。</code></pre><p>测试结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmronfx6kej20bs02ejrc.jpg"></p><blockquote><p> <strong>Action:</strong> Arguments can trigger different actions, specified by the action argument to <code>add_argument()</code>. There are six built-in actions that can be triggered when an argument is encountered:</p><ol><li><code>store</code>: Save the value, after optionally converting it to a different type. This is the default action taken if none is specified explicitly.</li><li><code>store_true</code>/<code>store_false</code>: Save the appropriate boolean value.</li><li><code>store_const</code>: Save a value defined as part of the argument specification, rather than a value that comes from the arguments being parsed. This is typically used to implement command line flags that aren’t booleans.</li><li><code>append</code>: Save the value to a list. Multiple values are saved if the argument is repeated.</li><li><code>append_const</code>: Save a value defined in the argument specification to a list.</li><li><code>version</code>: Prints version details about the program and then exits.</li></ol></blockquote><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/28871131">python学习之argparse模块</a></li><li><a href="https://blog.csdn.net/liuweiyuxiang/article/details/82918911">argparse模块中的action参数</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Distant supervision for relation extraction without labeled data</title>
      <link href="/2021/01/15/distant-supervision-for-relation-extraction-without-labeled-data/"/>
      <url>/2021/01/15/distant-supervision-for-relation-extraction-without-labeled-data/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这篇论文已经很老了，看它的原因主要是想了解Distant Supervision的概念，所以论文有些不会并不会读得很清楚。</p></blockquote><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>在2009年的时候，关系抽取主要有下面三种，它们有各自的弊端：</p><ol><li><strong>Supervised approaches</strong>；首先数据层面需要大量的annotated corpora，而且训练出来的Model存在corpus-specific的问题（即跨域问题）。</li><li><strong>Unsupervised approaches</strong>；无监督产生的relation的名称是不规范的。</li><li><strong>Bootstrap learning</strong>；结果存在low precision和semantic drift的现象。</li></ol><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>如论文的名字所示——<code>relation extraction without labeled data</code>，不需要像Supervised approaches那样需要的annotated corpora；</li><li>首次在关系抽取任务上提出了<code>distant surpervision</code>的思想；</li></ol><h2 id="Terminologies"><a href="#Terminologies" class="headerlink" title="Terminologies"></a>Terminologies</h2><h3 id="Freebase"><a href="#Freebase" class="headerlink" title="Freebase"></a>Freebase</h3><p>一个数据库，数据库里面是relation及相应的relation instances。</p><p>p.s. 这个数据库特别特别大</p><h3 id="Distant-Supervision"><a href="#Distant-Supervision" class="headerlink" title="Distant Supervision"></a>Distant Supervision</h3><p>这里需要明确一个概念，<strong>远程监督是一种学习方式，而不是具体模型</strong>，类似于有监督学习、无监督学习一样。</p><p>字面理解，既然是远程监督，那么一定存在监督喽。我们之前提到，有监督就一定要有label，数据需要带标签，那么标签从哪儿来？答案是来自远方：）</p><blockquote><p> 论文中对于远程监督的定义提到了两次：</p><ul><li>The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way.</li><li>The intuition of our distant supervision approach is to use Freebase to give us a training set of relations and entity pairs that participate in those relations.</li></ul></blockquote><p>用通俗的话来说就是：<font color="red"><strong>两个实体如果在知识库中存在某种关系，则包含该两个实体的非结构化句子均能表示出这种关系。</strong></font></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>论文采用的model的输入是<strong>实体对+关系的特征向量</strong>，model的内部结构非常的简单，就是multi-class logistic classifier。</p><p>接下来，依据模型<code>train</code>和<code>test</code>的两个阶段，讲讲需要做哪些工作：</p><h3 id="Train-Step⭐"><a href="#Train-Step⭐" class="headerlink" title="Train Step⭐"></a>Train Step⭐</h3><blockquote><p> In the training step, all entities are identified in sentences using a named entity tagger that labels persons, organizations and locations. If a sentence contains two entities and those entities are an instance of one of our Freebase relations, features are extracted from that sentence and are added to the feature vector for the relation.</p><p>The distant supervision assumption is that if two entities participate in a relation, any sentence that contain those two entities might express that relation. Because any individual sentence may give an incorrect cue, our algorithm trains a multiclass logistic regression classifier, learning weights for each noisy feature. In training, the features for identical tuples (relation, entity1, entity2) from different sentences are combined, creating a richer feature vector.</p></blockquote><p>对于Freebase出现的每一个relation instance（即entity pair），在目前已有training corpus中找到包含这个relation instance的所有句子，对这些句子分别进行feature extraction，然后将这些句子的feature进行融合，得到这个<code>(relation, entity1, entity2) </code>的feature vector。</p><h3 id="Test-Step⭐"><a href="#Test-Step⭐" class="headerlink" title="Test Step⭐"></a>Test Step⭐</h3><blockquote><p>In the testing step, entities are again identified using the named entity tagger. This time, every pair of entities appearing together in a sentence is considered a potential relation instance, and whenever those entities appear together, features are extracted on the sentence and added to a feature vector for that entity pair. For example, if a pair of entities occurs in 10 sentences in the test set, and each sentence has 3 features extracted from it, the entity pair will have 30 associated features. Each entity pair in each sentence in the test corpus is run<br>through feature extraction, and the regression classifier predicts a relation name for each entity pair based on the features from all of the sentences in which it appeared.</p></blockquote><p>测试阶段，需要提取testset中所有出现过的entity pair（而不是Freebase中已有的），然后重复train step里面的操作，得到分类结果。</p><h3 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h3><p>上面提到了要提取每个sentence的feature，接下来就讲怎么提取这些feature。</p><p>对于传统的机器学习model，特征工程一直是比较难搞的一部分。这篇论文也不例外，有着大量的特征工程：</p><h4 id="Lexical-Feature"><a href="#Lexical-Feature" class="headerlink" title="Lexical Feature"></a>Lexical Feature</h4><p>词汇级别的feature。主要是捕捉下面的特征：</p><ul><li>The sequence of words between the two entities</li><li>The part-of-speech tags of these words</li><li>A flag indicating which entity came first in the sentence</li><li>A window of k words to the left of Entity 1 and their part-of-speech tags</li><li>A window of k words to the right of Entity 2 and their<br>part-of-speech tags</li></ul><p>示例如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmonrqd5lcj20oz08ljuo.jpg" alt="Lexical Feature"></p><h4 id="Syntactic-Feature"><a href="#Syntactic-Feature" class="headerlink" title="Syntactic Feature"></a>Syntactic Feature</h4><p>语法级别的feature。主要捕捉下面的特征：</p><ul><li>A dependency path between the two entities</li><li>For each entity, one ‘window’ node that is not part of the dependency path</li></ul><p>示例如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmont0v2akj20p10eyaeg.jpg" alt="Syntactic Feature"></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>除了Freebase，它的corpus来自Wikipedia上的文本。</p><h3 id="Held-out-evaluation"><a href="#Held-out-evaluation" class="headerlink" title="Held-out evaluation"></a>Held-out evaluation</h3><p>略</p><h3 id="Human-evaluation"><a href="#Human-evaluation" class="headerlink" title="Human evaluation"></a>Human evaluation</h3><p>略</p><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><ul><li><p>这篇远古论文复现也复现不了，主要还是用于理解Distant supervision的概念。</p></li><li><p><font color="red"><strong>于我而言，distant supervision就像是人为地合成了一个特殊的训练集。</strong></font></p><p><strong>它需要一个像Freebase这样超大超大的knowledge base，同时也需要一个corpus，这个corpus里面要尽量包含这个knowledge base里面的entities！<code>knowledge base+corpus</code>，两者缺一不可，合成一个新的训练集用于之后的关系抽取任务。</strong></p></li><li><p><code>distant supervision</code>这个假设非常的大，其实很多的共现 entities 都没有什么关系，仅仅是出现在同一个句子中；而有的 entities 之间的关系其实并不仅仅只有一种，可能有多种，比如奥巴马和美国的关系，可能是 born in，也可能是 is the president of 的关系。</p><p>基于这个假设条件下的关系抽取工作通常都存在两个明显的弱点：</p><ol><li><p>基于给出的假设，训练集会产生大量的 wrong labels，比如两个实体有多种关系或者根本在这句话中没有任何关系，这样的训练数据会对关系抽取器产生影响。</p></li><li><p>NLP 工具带来的误差，比如 NER，比如 Parsing 等，越多的 feature engineering 就会带来越多的误差，在整个任务的 pipeline 上会产生误差的传播和积累，从而影响后续关系抽取的精度。</p></li></ol><p>关于问题（1）中 wrong labels 的问题，有的工作将关系抽取定义为一个 Multi-instance Multi-label 学习问题，比如工作 <strong>Multi-instance Multi-label Learning for Relation Extraction</strong> ，训练集中的每个 instance 都可能是一种 label。</p><p>而有的工作则是将问题定义为 Multi-instance Single-label 问题，假设共现的 entity 对之间只存在一种关系或者没有关系，一组包括同一对 entities 的 instances 定义为一个 Bag，每一个 Bag 具有一个 label，最终训练的目标是优化 Bag Label 的准确率。第一种假设更加接近于实际情况，研究难度也相对更大一些。</p><p>关于问题（2）中的 pipeline 问题，用深度学习的思路来替代特征工程是一个非常自然的想法，用 word embedding 来表示句子中的 entity 和 word，用 RNN 或者 CNN 以及各种 RNN 和 CNN 的变种模型来对句子进行建模，将训练句子表示成一个 sentence vector，然后进行关系分类，近几年有几个工作都是类似的思路，比如：</p><p><strong>[3] Relation Classification via Convolutional Deep Neural Network</strong></p><p><strong>[4] Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</strong></p><p><strong>[5] Neural Relation Extraction with Selective Attention over Instances</strong></p><p><strong>[6] Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions</strong></p></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/72419965">远程监督（Distant Supervision）</a></li><li><a href="https://zhuanlan.zhihu.com/p/28596186">PaperWeekly 第46期 | 关于远程监督，我们来推荐几篇值得读的论文</a></li><li><a href="https://zhuanlan.zhihu.com/p/315450600">Distant supervision for relation extraction 远程监督</a>（写得很好的一篇博客）</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 事件抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Event Extraction from Text</title>
      <link href="/2021/01/14/a-survey-of-event-extraction-from-text/"/>
      <url>/2021/01/14/a-survey-of-event-extraction-from-text/</url>
      
        <content type="html"><![CDATA[<blockquote><p>🔗原文链接：<a href="https://ivenwang.com/2020/06/16/eesurvey/">https://ivenwang.com/2020/06/16/eesurvey/</a></p><p>p.s. 这个北大学长的博客真的做得好好啊，向他学习，他的研究方向也是信息抽取，他读了真的好多好多的论文，太厉害了。</p></blockquote><h2 id="1-事件抽取的任务"><a href="#1-事件抽取的任务" class="headerlink" title="1 事件抽取的任务"></a>1 事件抽取的任务</h2><p>事件抽取（EE）是从文本中提取事件的关键元素，比如“5W1H”这种。EE 分成 closed-domain 和 open-domain，closed-domain 是给定要抽取的 event type 和 argument role，抽取关键词做分类问题；open-domain 是没有设计好的结构，主要任务就是检验文本里有没有事件，以及对事件聚类，并给每一类一个 event type。</p><h2 id="2-事件抽取的数据集"><a href="#2-事件抽取的数据集" class="headerlink" title="2 事件抽取的数据集"></a>2 事件抽取的数据集</h2><p>ACE、TAC-KBP、TDT 略</p><h2 id="3-基于模式匹配的事件抽取"><a href="#3-基于模式匹配的事件抽取" class="headerlink" title="3 基于模式匹配的事件抽取"></a>3 基于模式匹配的事件抽取</h2><h3 id="3-1-手动模式匹配"><a href="#3-1-手动模式匹配" class="headerlink" title="3.1 手动模式匹配"></a>3.1 手动模式匹配</h3><p>AutoSlog（[1]）第一次引入了 Part of Speech (POS)的标注，按照词性的模板动手去匹配，每次新找到一个匹配到词性的模板的，比如“<victim> was murdered”，它就把 murdered 加入到 trigger dictionary 里面，victim 即为对应的 argument。当然，如果能直接匹配到 trigger dict 里面的词，那就直接抽出 trigger，再用词性模板匹配。</victim></p><p>PALKA [2] 提出一个事件可以有多个 argument，且如果一句话有多件事，就切成多句话。</p><p>手动模式匹配的方法只能针对固定的领域（比如生物、金融等），且需要操作者有专业知识，且花费大量时间精力，且无法标很大的规模。当然，这使得事件抽取的准确率非常高。</p><h3 id="3-2-半自动模式匹配"><a href="#3-2-半自动模式匹配" class="headerlink" title="3.2 半自动模式匹配"></a>3.2 半自动模式匹配</h3><p>AutoSlog-TS [3] 是 [1] 自己改的。以前不是新发现的加到 trigger dict 里面嘛，现在就直接把新发现的 event pattern 记录下来，以后匹配 event pattern。这样就可以用 event pattern 自动匹配了。</p><h2 id="4-基于机器学习的事件抽取"><a href="#4-基于机器学习的事件抽取" class="headerlink" title="4 基于机器学习的事件抽取"></a>4 基于机器学习的事件抽取</h2><p>都是有监督学习，都是先 trigger 再 argument。</p><h3 id="4-1-机器学习用到的文本特征"><a href="#4-1-机器学习用到的文本特征" class="headerlink" title="4.1 机器学习用到的文本特征"></a>4.1 机器学习用到的文本特征</h3><p>文本特征分成三类，即词汇特征、句法特征、语义特征。</p><p>词汇特征包括：原单词、全改成小写的单词、词性还原之后的单词、词性标签（POS tag）。</p><p>句法特征来自句法依存树，即找到各部分的句法关系。句法特征包括：在 dependency path 上的标签（主谓宾等）、dependency word 及其词汇特征、词在句法树的深度。</p><p>语义特征包括：同义词及其词汇特征、event type、entity type。</p><p>以上特征一开始是基于词袋模型的独热码。机器学习的目标，就是提取一句话最关键的特征，并且把这些特征整合成一个高维特征，来进行后续的分类。</p><h3 id="4-2-Pipeline-方法"><a href="#4-2-Pipeline-方法" class="headerlink" title="4.2 Pipeline 方法"></a>4.2 Pipeline 方法</h3><h4 id="4-2-1-句子级的事件提取"><a href="#4-2-1-句子级的事件提取" class="headerlink" title="4.2.1 句子级的事件提取"></a>4.2.1 句子级的事件提取</h4><p>[4] 是 trigger 和 argument 任务各自一个分类器。前一个叫做 TiMBL，用最近邻学习 trigger 识别模型，输入的包括了 lexical features, WordNet features, context features, dependency features and related entity features；后一个叫做 MegaM，用最大熵学习 argument 识别，输入包括 the features of trigger word, event type, entity mention, entity type, and dependency path between trigger word and entity。（这些 feature 都可以参考！）</p><p>还有方法结合了模式匹配和机器学习。[5] 先用模式匹配，找出几个潜在的 event type，再把这些潜在的 event type 和其他特征编码输入机器学习模型，帮助 trigger 识别。[6] 构建了一个 trigger-argument 的关系库，帮助 argument type 识别。</p><p>中文需要先分词得到 token，但是可能存在 trigger 被分成 多个 token，或者只是 token 的一部分。为了解决这个问题，[7] 先在句子里去找各种 event type 的同义词，作为 trigger；[8] 同时使用 word 和 character 特征输入，其中 character-level 特征包括这个字符和它的邻居；[9] 就暴力了：如果一个字符是 trigger，那么所有包含这个字符的词也都是 trigger，trigger 里面的每一个字符，都将和它的前后字符组合，成为可能的 trigger。</p><h4 id="4-2-2-文章级的事件提取"><a href="#4-2-2-文章级的事件提取" class="headerlink" title="4.2.2 文章级的事件提取"></a>4.2.2 文章级的事件提取</h4><p>如果我们是在一个大的语境（比如一篇文章，或多篇文章）的一句话做事件提取，那我们可以利用文章级的特征，来增加准确率。文章级的特征有两类用法。一类是用全局的（文章级）特征构造辅助模型，辅助之前的局部的（句子级）模型；另一类是把文章级特征加入到句子级模型的输入里。下面分别介绍。</p><p>辅助模型： [10] 认为，同一文章、不同句子之间的 argument 会有一致性，包括词义的一致性和 argument role 的一致性。为了用上这种一致性，他们把相似主题的 document 分类，每类都建立 trigger sense 和 argument role 的推断规则。[11] 把这种方法推广到了不同文章之间。[12] 提出了两种全局信息，即事件与事件的关系，和主题与事件的关系，用逻辑软概率模型 + 全局信息，处理 local 的事件提取结果。</p><p>特征输入： [13] 认为，相似的 entity，会在相似的事件里扮演相似的角色。因此，他们定义了包括 entity subtype、entity subtype 之间的共现概率、entity subtype 的 argument 等特征，来训练句子级别的 SVM 分类器。[14] 利用主题模型，将文章的主题作为特征输入分类器。</p><h3 id="4-3-Joint-方法"><a href="#4-3-Joint-方法" class="headerlink" title="4.3 Joint 方法"></a>4.3 Joint 方法</h3><p>上述的 pipeline 方法 ① 会有误差积累 ② 后一个分类器结果无法微调前一个分类器 ③ 两个子任务之间的关系没有用上。Joint 方法也分成两类：一类是引入子任务之间的关系，让各部分模型可以共同优化；另一类是真正的 joint 方法，一口气提取 trigger 和 argument。</p><p>子任务关系：[15] 通过整数线性规划，联系了基于 CRF 和基于最大熵的两个分类器，使之联合优化。</p><p>真正的 joint：[16] 把事件抽取当做 structured learning 问题，用 structured perceptron model 解决。具体来讲，the outcome of the entire sentence can be considered as a graph in which trigger or argument is represented as node, and the argument role is represented as a typed edge from a trigger to its argument. They applied the beam-search to perform inexact decoding on the graph to capture the dependencies between triggers and arguments. （没看懂，TODO）</p><h2 id="5-基于深度学习的事件抽取"><a href="#5-基于深度学习的事件抽取" class="headerlink" title="5 基于深度学习的事件抽取"></a>5 基于深度学习的事件抽取</h2><p>上面的机器学习方法，最重要的环节还是特征工程，这还是有领域限制、专业人员才能标特征等等。另外，特征大多是独热码，稀疏数据对模型训练有很大影响。深度神经网络就在隐层中，把底层的输入转换成了更抽象的特征表示，就能解决上述问题。一般过程是构建一个神经网络，将词嵌入作为输入并输出每个词的分类结果，即，对一个词是否是 trigger 进行分类，如果是，则对其 argument role 分类。</p><h3 id="5-1-基于卷积神经网络"><a href="#5-1-基于卷积神经网络" class="headerlink" title="5.1 基于卷积神经网络"></a>5.1 基于卷积神经网络</h3><p>[17] 是很朴素的想法，即输入词及其邻居的 embedding，卷积、池化，得到特征表示，直接分类。但是一句话不一定只有一件事呀！[18] 就采用了动态多池化，把句子用 trigger 分成多个部分，分别最大池化（这个我有写读后感，详见 <a href="https://ivenwang.com/2020/06/05/dmcnn/">DMCNN</a>）。上面俩都是做 localize 的卷积，[19] 是把句子里任意不连续的 k 个词，都卷积-池化，然后挑选最重要的。</p><p>CNN 的方法也有一些改进： [20] 设计了一种语义增强的深度学习模型，称为 Dual-CNN，该模型在典型的 CNN 中添加了语义层，以捕获上下文信息。[21] 提出了一个并行多池化卷积神经网络（PMCNN），它可以捕获句子的组合语义特征，以进行医学事件提取。PMCNN 还利用基于依存关系的 embedding 来进行单词的语义和句法表示，并采用整流线性单元作为非线性函数。[22] 用 bootstrapping 方法建立了全局上下文的表示，并将表示集成到 CNN 中进行事件提取。</p><h3 id="5-2-基于循环神经网络"><a href="#5-2-基于循环神经网络" class="headerlink" title="5.2 基于循环神经网络"></a>5.2 基于循环神经网络</h3><p>CNN ① 多是 pipeline 方法，会误差积累 ② 输入的是词向量的拼接，并不能找到距离比较远的词之间的关系，从而用上整句话的信息</p><p>[23] 使用 Bi-GRU，分成编码阶段和预测阶段，利用了 trigger subtype 之间、argument roles 之间、trigger subtype 和 argument roles 之间的依存关系（这个我也有写读后感，详见 <a href="https://ivenwang.com/2020/06/06/jrnn/">JRNN</a>）。</p><p>[24] 使用了句法结构，来增强 RNN，即把一些有关系词对应的 RNN 单元连上。[25] 就更直接了，直接用 dependency tree 作为 Bi-LSTM 的结构。[26] 更进一步，对于每一个词，都有一个以它为根的 dependency tree 的 LSTM 结构（叫做 Tree-LSTM），而且还是中文的事件抽取。[27] 把外部的实体知识引入 Tree-LSTM，做医学的实体抽取。</p><p>RNN 还能做文章级的事件抽取。[28] 是在 Bi-LSTM 的基础上，输入里加入了一个“概率向量”，表示当前文档。</p><p>上面的 RNN 都是 LSTM 或 GRU，有门控单元结构，但是这非常耗时。[29] 的 SRU 避免了当前单元和上一单元的乘法，让计算量大大减小。[30] 就构建了 Bi-SRU，一个用来学习词汇级的表示，另一个用来学习字符级的表示。</p><h3 id="5-3-基于图神经网络"><a href="#5-3-基于图神经网络" class="headerlink" title="5.3 基于图神经网络"></a>5.3 基于图神经网络</h3><p>GNN 是图结构的神经元组织到一起的，是非欧空间的结构。用 GNN 做事件抽取，核心问题是要先给文本中的词构建出图结构。</p><p>[31] 采用了一种语义分析技术，称为抽象意义表示（AMR），可以规范文本中的许多词汇和句法变体，并输出有向无环图，以捕获文本“谁对谁做了什么”的概念。另外，他们认为事件结构是 AMR 图的子图，并将事件提取任务转换为子图识别问题。他们训练了图 LSTM 模型，以识别用于生物医学事件提取的事件子图。</p><p>另外的方法是把 dependency tree 变做图。[32] 把 dependency tree 变成双向的，再加上每个词到自己的边，用这个图去进行 localized 卷积操作，就能不仅提取上下文特征，还能有长距离的、基于依存关系的词语特征。</p><h3 id="5-4-多种神经网络混合"><a href="#5-4-多种神经网络混合" class="headerlink" title="5.4 多种神经网络混合"></a>5.4 多种神经网络混合</h3><p>建立这种混合模型的常用方法，是使用不同的神经网络来学习不同类型的单词表示。 例如，[33] 和 [34] 都首先在图卷积运算之前，用 Bi-LSTM 来获取初始单词表示。</p><p>也有把 CNN 和 RNN 融合的。比如 [35] 就直接把 CNN 和 Bi-LSTM 的结果拼一起，再分类（这个我也有写读后感，详见 <a href="https://ivenwang.com/2020/06/12/cnnbilstmchinese/">这里</a>）。[36] 也是 CNN 和 Bi-LSTM，只不过这里的 Bi-LSTM 的序列输出结果被直接拼接，作为文章的特征，再把 CNN 得到的词的特征拼到一起，再分类。</p><p>另一种就是 GAN，即两个神经网络互相对抗，一个生成，一个判别真假。下面几个都是用 RNN 作为生成和对抗网络。在训练过程中，[37] 提出用双通道自我调节学习策略来调节学习过程。在自我调节过程中，对生成网络进行训练以产生最虚假的特征。而带有记忆抑制器的判别网络，经过训练可以消除虚假特征。[38] 提出了一种对抗模仿策略，将知识提炼模块纳入特征编码过程。在他们的混合模型中，使用了分别为 Bi-GRU 的教师编码器和学生编码器。教师编码器通过 gold annotations（我也不知道是啥）进行训练，而学生编码器则通过对抗式模仿学习，通过使其输出与教师编码器的输出之间的距离最小化来进行训练。[39] 使用强化学习（RL）策略在训练过程中更新了 Q 表，其中 Q 表记录了根据系统状态和动作计算出的奖励值。（这段我都不懂，翻译的）</p><h3 id="5-5-Attention-机制"><a href="#5-5-Attention-机制" class="headerlink" title="5.5 Attention 机制"></a>5.5 Attention 机制</h3><p>Attention 机制是一种区分机制，可根据其对给定任务的重要性，引导神经模型对输入的每个成分进行不平等的对待，且这些权重从模型训练过程中自学而来。</p><p>应该给谁更多注意力呢？[40] 认为，trigger 的 argument 应比其他词受到更多关注。为此，他们首先构造了 gold attention vectors，表示 argument 及其上下文的编码。此外，他们为每个单词设计了两个上下文关注向量：一个是前后单词；另一个是前后 entity，两个拼一起，最小化事件检测和注意差异的加权损失。</p><p>在词级注意力机制中，来自句法结构里面的 entity 关系也可以用来训练注意力 [41,42]。他的想法就是，句法依存关系可以让两个很远的词之产生关系。[43] 是中文的，字符级注意机制。</p><p>还有人把词级和句子级的注意力融合，以增强在有多个句子的文档里的事件提取。[44] 认为，在文章里，每句话即使是不同的 event type，也都和文章主题相关。他们提出一种基于 hierarchical and supervised attention mechanism 的 DEEB-RNN（以后看），用词汇级的注意力提取 trigger，句子级注意力 找 event（？）。这样就得到两个 gold attention vectors，一个是基于 trigger 的词汇级 attention，另一个是如果有 trigger，那就是句子级的 attention？[45] 提出了 HBT-NGMA 模型，是一个 gated 多层注意力机制，抽取句子之间的和句子里面的信息。</p><p>除了利用单词和句子之外，还有人整合了额外的知识，例如使用多语言知识 [46] 或先验 trigger 语料库 [47]。[46] 检查了 ACE 数据集里标注的数据，发现 57% 的 trigger 都是模糊的，就认为多语言可以解决这个模糊。他们提出了 gated multilingual attention，包括 mono-lingual context attention 和 gated cross-lingual attention（这都什么玩意？哭了）后者就是用先用机器翻译，然后再咋咋的。[47] 就把先验 trigger 和 attention 整合。（attention 这一大块，每个文章都得看啊！）</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 事件抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科大讯飞大赛 事件抽取挑战赛冠军分享</title>
      <link href="/2021/01/14/ke-da-xun-fei-da-sai-shi-jian-chou-qu-tiao-zhan-sai-guan-jun-fen-xiang/"/>
      <url>/2021/01/14/ke-da-xun-fei-da-sai-shi-jian-chou-qu-tiao-zhan-sai-guan-jun-fen-xiang/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>🔗原文链接：<a href="https://mp.weixin.qq.com/s/WXWD7fmitaFHfUAv1mNLPA">https://mp.weixin.qq.com/s/WXWD7fmitaFHfUAv1mNLPA</a></p><p>🔗赛题链接：<a href="http://challenge.xfyun.cn/topic/info?type=hotspot">http://challenge.xfyun.cn/topic/info?type=hotspot</a></p><p>🔗Github仓库链接：<a href="https://github.com/WuHuRestaurant/xf_event_extraction2020Top1">https://github.com/WuHuRestaurant/xf_event_extraction2020Top1</a></p><h2 id="赛题介绍📃"><a href="#赛题介绍📃" class="headerlink" title="赛题介绍📃"></a>赛题介绍📃</h2><h3 id="赛题背景"><a href="#赛题背景" class="headerlink" title="赛题背景"></a>赛题背景</h3><p>事件抽取将非结构化文本中的事件信息展现为结构化形式，在舆情监测、文本摘要、自动问答、事理图谱自动构建等领域有着重要应用。在真实新闻中，由于文本中可能存在句式复杂，主被动转换，多事件主客体共享等难点，因此“事件抽取”是一项极具挑战的抽取任务。</p><h3 id="赛事任务"><a href="#赛事任务" class="headerlink" title="赛事任务"></a>赛事任务</h3><p>本赛事任务旨在从通用新闻文本中抽取事件触发词、事件论元以及事件属性。在传统的事件定义中，事件由事件触发词（Trigger） 和描述事件结构的元素 （Argument）构成。事件触发词标识着事件的发生。事件论元为事件主体（Subject）、客体（Object）、时间（Time）、地点（Location）等，是表达事件重要信息的载体。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmpjvvobbtj20u00gwwl8.jpg"></p><p>事件属性包括事件极性（Polarity）、时态（Tense），是衡量事件是否真实发生的重要依据。通过极性，事件分为肯定、否定、可能事件。通过时态，事件分为过去发生的事件、现在正在发生的事件、将要发生的事件以及其他无法确定时态的事件。</p><p><strong>本赛事任务一为初赛任务，任务二为复赛任务，在任务一的基础上增加了事件属性识别。</strong>为了模拟真实场景，数据中包含了非实际发生的事件。</p><ul><li><code>任务一</code>：事件触发词及论元抽取</li></ul><p>该任务旨在从文本中抽取标识事件发生的触发词和论元，触发词往往为动词和名词。触发词对应的事件论元，主要为主体、客体、时间、地点，其中主体为必备论元。</p><ul><li><code>任务二</code>：事件属性抽取</li></ul><p>该任务旨在从文本中抽取表达事件发生状态的属性，包括极性、时态。极性分为：肯定、否定、可能；时态分为：过去、现在、将来、其他。</p><h3 id="评审规则"><a href="#评审规则" class="headerlink" title="评审规则"></a>评审规则</h3><ol><li><strong>初赛数据说明</strong>：本次比赛初赛为参赛选手提供了6958条中文句子，及其9644条提取结果（存在一对多的情况）。</li><li><strong>复赛数据说明</strong>：本次比赛复赛为参赛选手提供了3335条中文句子，及其3384条提取结果（存在一对多的情况）：</li><li><strong>评价指标</strong>：本模型依据提交的结果文件，采用F值进行评价。</li></ol><h2 id="冠军思路👑"><a href="#冠军思路👑" class="headerlink" title="冠军思路👑"></a>冠军思路👑</h2><p>将任务分割为<strong>触发词抽取</strong>，<strong>论元抽取</strong>，<strong>属性抽取</strong>。具体而言是论元和属性的抽取结果依赖于<strong>触发词</strong>，因此只有一步误差传播。</p><p><strong>因 <code>time</code>&amp;<code>loc</code> 并非每个句子中都存在，并且分布较为稀疏，因此将 <code>time</code> &amp; <code>loc</code> 与 <code>sub</code> &amp; <code>obj</code> 的抽取分开（role1 提取 sub &amp; obj；role2 提取 time &amp; loc）</strong></p><blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmpl803tefj20jr0ffjru.jpg"></p><p>具体的数值如下：<br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmv2umo1hdj20kc04iwes.jpg"></p></blockquote><p>模型先进行<strong>触发词提取</strong>，由于复赛数据集的特殊性，模型限制抽取的事件仅有一个，<font color="red"><strong>如果抽取出多个触发词，选择 logits 最大的 trigger 作为该句子的触发词</strong></font>，如果没有抽取触发词，筛选整个句子的 logits，取 argmax 来获取触发词；</p><p>然后根据触发词抽取模型抽取的触发词，分别输入到 <code>role1</code> &amp; <code>role2</code> &amp; <code>attribution</code> 模型中，进行后序的论元提取和属性分类；</p><p>四种模型都是基于 Roberta-wwm 进行实验，加入了不同的特征。最后将识别的结果进行整合，得到提交文件。</p><h3 id="pipeline-思路"><a href="#pipeline-思路" class="headerlink" title="pipeline 思路"></a>pipeline 思路</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmngejrzsvj20kg0cj74j.jpg"></p><blockquote><p>注意：</p><ul><li>输入的数据是<strong>复赛数据</strong>而不是<strong>初赛数据</strong>，这份开源代码中只考虑复赛任务，而不考虑初赛任务。但是由于复赛任务是在初赛任务上增加了属性抽取模块，所以复赛任务包含了初赛任务。唯一不同的是复赛和初赛的数据集不一样，而且在这份开源代码中，复赛任务也使用了初赛的数据进行数据增强！</li></ul></blockquote><h3 id="触发词提取模块"><a href="#触发词提取模块" class="headerlink" title="触发词提取模块"></a>触发词提取模块</h3><p>trigger 提取采用的特征是<font color="red"><strong>远程监督 trigger</strong></font>，把所有标注数据当做一个知识库，对当前文本进行匹配。</p><blockquote><p>注：</p><ul><li>在<code>train</code>时，需要排除自身的label，我们采用的是K-Fold的训练集 distant trigger 构造，即将训练集分成K份，用前K-1份的所有label当做后一份的知识库，构造训练数据的distant trigger；</li><li><code>test</code> 时候采用所有 trigger。</li></ul></blockquote><p>在测试时若出现预测为空，选取 distant trigger logits 最大的解码输出 trigger。</p><p>具体模型如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmngi3x3kxj20su0lyq3n.jpg"></p><h3 id="论元提取模块"><a href="#论元提取模块" class="headerlink" title="论元提取模块"></a>论元提取模块</h3><p>role 采用的特征是<strong>trigger的相对距离</strong>，然后采用了苏神的 <strong>Conditional Layer Norm</strong> 来让整个句子融入 trigger 信息，同样采用 Span 解码的方式。</p><p>由于数据中 subject/object 分布相似，同时与 time/loc 分布相差很大，我们进一步进行了优化，将前两者和后两者的抽取分开，防止 time/loc 的数据对 subject/object 的 logits 稀疏化。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnh1yku2cj20u00o8adh.jpg"></p><h3 id="attribution分类器"><a href="#attribution分类器" class="headerlink" title="attribution分类器"></a>attribution分类器</h3><p>attribution 分类器并没有进行特殊优化，采用了一个<strong>动态窗口</strong>的方法，我们认为某一 trigger 的 tense &amp; polarity 只与其附近的语境有关。</p><p>因此我们设定了一个窗口，对该窗口内进行 pooling 操作，然后利用 pooling 后的 logits 进行多任务学习，同时分类出 tense 和 polarity。因属性数据类别不均及其严重，最后我们用 ERNIE 模型做了一个<strong>10折交叉验证</strong>，有较大的提升。</p><blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmui05ex7zj20jo0f43yx.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmui5dwwz1j20jf0fbmxl.jpg"></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnh409kh9j20u00k8tai.jpg"></p><h3 id="方案优化✅"><a href="#方案优化✅" class="headerlink" title="方案优化✅"></a>方案优化✅</h3><h4 id="触发词提取优化"><a href="#触发词提取优化" class="headerlink" title="触发词提取优化"></a>触发词提取优化</h4><ul><li><p>初始方案：采用<strong>BERT-CRF</strong>的方式对触发词进行提取；</p></li><li><p>缺点：存在5%的句子会出现<strong>解码为空</strong>的现象，导致误差传播极大；</p></li><li><p>解决方案：</p><ol><li>舍弃CRF结构，采用<strong>指针式解码</strong>的方案，并利用前面提到的标注数据中的 trigger字典。如果未解码出trigger，则比较句子中匹配知识库的所有distant trigger 的 start + end logits，选取最大的一个作为解码出的 trigger；</li><li>观察复赛数据，发现绝大部分训练语料中仅有一个事件，故<strong>限制解码输出一个trigger</strong>，如果解码出多个trigger，选取 logits（应该是sum of start logits &amp; end logits） 最大的那个trigger作为候选trigger。</li></ol></li></ul><h4 id="论元提取优化"><a href="#论元提取优化" class="headerlink" title="论元提取优化"></a>论元提取优化</h4><ul><li><p>初始方案：采用BERT-指针式解码的方式<strong>同时对</strong> sub &amp; obj &amp; time &amp; loc 进行抽取。</p></li><li><p>缺点：sub &amp; obj 与 time &amp; loc 的分布差别很大，time &amp; loc 样本数量很少，造成 logits 很稀疏，降低了所有 role 的 recall。</p></li><li><p>改进方案：</p></li></ul><ol><li><p>将 time &amp; loc 和 subject &amp; object 的<strong>提取分开</strong>，采用两个独立的模型进行提取。</p></li><li><p>time &amp; loc 的抽取采用CRF，同时<strong>随机丢弃70%的负样本</strong>，使正负样本均衡。</p></li></ol><h4 id="属性分类优化"><a href="#属性分类优化" class="headerlink" title="属性分类优化"></a>属性分类优化</h4><blockquote><p>这个问题已经不像前面一样是Sequence Labeling的，这里显然是一个分类问题！整个句子输出一个标签。</p></blockquote><ul><li><p>初始方案：BERT + 两个分类器</p></li><li><p>难点：属性分类中的两个属性出现了严重的类别不均。</p></li><li><p>改进方案：</p><ol><li>观察数据样本发现，能决定事件属性的词大多<strong>存在触发词左右</strong>，故舍弃CLS中的全局特征，采用<strong>trigger左右两端动态池化特征</strong>作为全局特征；</li><li>由于样本类别不均极其严重，尝试很多增强方式都无效，故采用<strong>10折交叉</strong>验证的方法来提升模型的<strong>泛化性能</strong>。</li></ol></li></ul><h3 id="数据增强💎"><a href="#数据增强💎" class="headerlink" title="数据增强💎"></a>数据增强💎</h3><blockquote><p>数据增强（Data Augmentation）是NLP任务中常见的一种手段，用于缓解标注数据集少的问题。</p></blockquote><p>本次比赛主要的上分点在于数据增强的工作，初赛和复赛数据的分布差别极大，一起训练反而会导致结果下降。</p><p>因此我们做了一个初赛数据筛选的工作，筛选出与复赛数据分布相近的数据进行增量训练。主要流程详见PPT中<font color="red"><strong>基于标签验证的数据增强部分</strong>。</font></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnh52wty0j20u00gwgpm.jpg" alt="undefined"></p><blockquote><ol><li>使用复赛train set作为input，训练Trigger/Role Extract Model得到Base Model;</li><li>将初赛的train set作为input，输入到Base Model中，得到经过Base Model预测后的trigger/role；</li><li>对比Base Model的预测结果和初赛数据的原为label，筛选出<strong>与复赛数据分布相近的数据</strong>；</li><li>使用【复赛train set + 筛选后的初赛train set】作为input，训练Trigger/Role Extract Model得到Base Model得到Final Model。</li></ol></blockquote><h4 id="触发词增强"><a href="#触发词增强" class="headerlink" title="触发词增强"></a>触发词增强</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnh57l5c8j20u00gwwf6.jpg"></p><blockquote><p>上图表示的是<code>src_final/preprocess/convert_raw_data.py</code>中的<code>split_preliminary_trigger_data()</code>函数的执行流程。</p></blockquote><h4 id="论元增强"><a href="#论元增强" class="headerlink" title="论元增强"></a>论元增强</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnh5eatbbj20u00gw0z8.jpg"></p><blockquote><p>上图表示的是<code>src_final/preprocess/convert_raw_data.py</code>中的<code>split_preliminary_role_data()</code>函数的执行流程。</p></blockquote><h2 id="项目运行主要环境🔨"><a href="#项目运行主要环境🔨" class="headerlink" title="项目运行主要环境🔨"></a>项目运行主要环境🔨</h2><p>运行系统：</p><pre><code class="python">Ubuntu 18.04.4python3.7</code></pre><p>python 运行环境，可以通过以下代码完成依赖包安装：</p><pre><code class="python">pip install -r requirements.txt</code></pre><pre><code class="python">transformers==2.10.0pytorch_crf==0.7.2numpy==1.16.4torch==1.5.1+cu101tqdm==4.46.1scikit_learn==0.23.2torchcrf==1.1.0</code></pre><p>CUDA:</p><pre><code class="python">CUDA Version: 10.2  Driver 440.100 GPU：Tesla V100 (32G) * 2</code></pre><h2 id="项目目录说明"><a href="#项目目录说明" class="headerlink" title="项目目录说明"></a>项目目录说明</h2><pre><code class="tex">xf_ee├── data                                    # 数据文件夹│   ├── final                               # 复赛数据（处理过的）│   │   ├── mid_data                        # 中间数据 （词典等）│   │   ├── preliminary_clean               # 清洗后的初赛数据│   │   └── raw_data                        # 复赛经过初步清洗后的 raw_data│   └── preliminary                         # 初赛数据（略）│├── out                                     # 存放训练的模型│   ├── final                               # 复赛各个单模型（trigger/role/attribution）│   └── stack                               # 十折交叉验证的 attribution 模型│├── script/final                            # 放训练 / 评估 / 测试 的脚本│   ├── train.sh                            │   ├── dev.sh                     │   └── test.sh                │├── src_final│   ├── features_analysis                   # 数据分析│   │   └── images                          # 分析时画得一些图 │   ├── preprocess                       │   │   ├── convert_raw_data.py             # 处理转换原始数据│   │   ├── convert_raw_data_preliminary.py     # 转换初赛数据为复赛格式并处理│   │   └── processor.py                    # 转换数据为 Bert 模型的输入│   ├── utils                      │   │   ├── attack_train_utils.py           # 对抗训练 FGM / PGD│   │   ├── dataset_utils.py                # torch Dataset│   │   ├── evaluator.py                    # 模型评估│   │   ├── functions_utils.py              # 跨文件调用的一些 functions│   │   ├── model_utils.py                  # 四个任务的 models│   │   ├── options.py                      # 命令行参数│   |   └── trainer.py                      # 训练器|├── 答辩PPT                                 # 决赛PPT├── dev.py                                  # 用于模型评估├── ensemble_predict.py                     # 用百度 ERNIE 模型对 attribution 十折交叉验证├── predict_preliminary.py                  # 对初赛数据进行清洗├── readme.md                               # ...├── test.py                                 # pipeline 预测复赛数据 （包含 ensemble）└── train.py                                # 模型训练</code></pre><h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><h3 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h3><p>数据转换部分只提供<strong>代码</strong>和<strong>已经转换好的数据</strong>，具体操作在 <code>src_final/preprocess</code> 中的 <code>convert_raw_data()</code>中，包含对初赛/复赛数据的清洗和转换。</p><h3 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h3><pre><code class="shell">bash ./script/final/train.sh</code></pre><blockquote><p>注：脚本中指定的 BERT_DIR 指BERT所在文件夹，BERT采用的是哈工大的全词覆盖wwm模型，下载地址 <a href="https://github.com/ymcui/Chinese-BERT-wwm">https://github.com/ymcui/Chinese-BERT-wwm</a> ，自行下载并制定对应文件夹，并将 vocab.txt 中的两个 unused 改成 [INV] 和 [BLANK]（详见 processor 代码中的<code>fine_grade_tokenize</code>）</p></blockquote><p><strong>如果设备显存不够，自行调整 train_batch_size，脚本中的 batch_size（32）在上述环境中占用显存为16G</strong></p><p><strong>最终训练的结果是每一个 epoch 下存一次，线下评估结果在 <code>eval_metric.txt</code> 下，保留最优线下结果作为训练结果，其余删掉即可</strong></p><p>可更改的公共参数有</p><pre><code class="tex">lr: bert 模块的学习率other_lr: 除了bert模块外的其他学习率（差分学习率）weight_decay：...attack_train： 'pgd' / 'fgm' / '' 对抗训练 fgm 训练速度慢一倍, pgd 慢两倍，但是效果都有提升swa_start: 滑动权重平均开始的epoch</code></pre><h4 id="trigger提取模型训练-（TASK-TYPE-“trigger”）"><a href="#trigger提取模型训练-（TASK-TYPE-“trigger”）" class="headerlink" title="trigger提取模型训练 （TASK_TYPE=“trigger”）"></a>trigger提取模型训练 （TASK_TYPE=“trigger”）</h4><p>可更改的参数有</p><pre><code class="tex">use_distant_trigger: 是否使用复赛数据构造的远程监督库中的 trigger 信息</code></pre><blockquote><p>由于解决方案使用的是Ubuntu系统，但是我只有Windows系统，因此在执行上需要改动一下。</p><p>Windows下的命令操作：</p><pre><code class="shell">python train.py \--gpu_ids="0" \--mode="train" \--raw_data_dir="./data/final/raw_data" \--mid_data_dir="./data/final/mid_data" \--aux_data_dir="./data/final/preliminary_clean" \--bert_dir="../bert/torch_roberta_wwm" \--output_dir="./out" \--bert_type="roberta_wwm" \--task_type="trigger" \--max_seq_len=320  \--train_epochs=6 \--train_batch_size=1 \--lr=2e-5 \--other_lr=2e-4 \--attack_train="pgd" \--swa_start=4 \--eval_model \--enhance_data \--use_distant_trigger</code></pre><p>但是，为了方便起见，我就直接在<code>./train.py</code>的<code>__main__</code>函数中修改参数值了，代码如下：</p><pre><code class="python">args.gpu_ids = "0"args.mode = "train"args.raw_data_dir = "./data/final/raw_data"args.mid_data_dir = "./data/final/mid_data"args.aux_data_dir = "./data/final/preliminary_clean"args.bert_dir = "../bert/torch_roberta_wwm"args.output_dir = "./out"args.bert_type = "roberta_wwm"args.task_type = "trigger"args.max_seq_len = 320args.train_epochs = 6args.train_batch_size = 1args.lr = 2e-5args.other_lr = 2e-4args.attack_train = "pgd"args.swa_start = 4args.eval_model = Trueargs.enhance_data = Trueargs.use_distant_trigger = True</code></pre></blockquote><h4 id="role-提取模型训练-（TASK-TYPE-“role1-role2”）"><a href="#role-提取模型训练-（TASK-TYPE-“role1-role2”）" class="headerlink" title="role 提取模型训练 （TASK_TYPE=“role1/role2”）"></a>role 提取模型训练 （TASK_TYPE=“role1/role2”）</h4><p>可更改的参数有</p><pre><code class="tex">use_trigger_distance: 是否使用句子中的其他词到 trigger 的距离这一个特征</code></pre><h4 id="attribution-分类模型训练-（TASK-TYPE-“attribution”）"><a href="#attribution-分类模型训练-（TASK-TYPE-“attribution”）" class="headerlink" title="attribution 分类模型训练 （TASK_TYPE=“attribution”）"></a>attribution 分类模型训练 （TASK_TYPE=“attribution”）</h4><p>未使用其他特征</p><p><strong>MODE=“stack”</strong> 时候对 attribution 任务进行十折交叉验证，换用百度 ERNIE1.0 模型作为预训练模型</p><h3 id="验证阶段"><a href="#验证阶段" class="headerlink" title="验证阶段"></a>验证阶段</h3><pre><code class="tex">bash ./script/final/dev.sh</code></pre><p>主要的参数有三个：</p><ul><li>TASK_TYPE：需要验证任务的 type</li><li>start/end threshold ：trigger / role1 model 需要进行调整的阈值</li><li>dev_dir: 需要验证的模型的文件夹</li></ul><h3 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h3><pre><code class="tex">bash ./script/final/test.sh</code></pre><p>利用训练最优的四个单模型进行 pipeline 式的预测 sentences.json 文件，获取最终的 submit 文件，</p><p>其中 <strong>submit_{version},json</strong> 为四个单模型的结果， <strong>submit_{version}ensemble,json</strong> 为单模型 + attribution 交叉验证后的结果。</p><p>四个任务 model 的上级文件夹必须指定，同时文件夹名称应包含模型的参数特征。</p><ul><li><strong>trigger_ckpt_dir</strong>：       trigger 所在的文件夹</li><li><strong>role1_ckpt_dir</strong>：        role1 所在的文件夹</li><li><strong>role2_ckpt_dir：</strong>         role2 所在的文件夹</li><li><strong>attribution_ckpt_dir：</strong>   attribution所在的文件夹</li></ul><h2 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h2><table><thead><tr><th align="center">classification</th><th align="center">score</th></tr></thead><tbody><tr><td align="center">submit_v1.json</td><td align="center">0.73684</td></tr><tr><td align="center">submit_v1_ensemble.json</td><td align="center"><strong>0.73859</strong></td></tr></tbody></table><h3 id="各阶段提升"><a href="#各阶段提升" class="headerlink" title="各阶段提升"></a>各阶段提升</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmpjftweffj20u00gwglx.jpg"></p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在我们的训练过程中，实际使用了组委会提供的初赛（经过清洗和转换）+复赛数据进行训练，在项目内部提供了清洗完毕的初赛数据；</p><p>具体清洗流程如下所示：</p><ul><li><p>只使用复赛数据train得到trigger抽取模型和role1抽取模型（需指定model的上级文件夹）</p><p><strong>trigger_simple_ckpt_dir</strong>：单独复赛数据train trigger 所在的文件夹</p><p><strong>role1_simple_ckpt_dir</strong>：单独复赛数据train role1 所在的文件夹</p></li><li><p>使用<code>predict_prelimiary.py</code>调用train好的trigger model 和role1 model 预测初赛数据的trigger和sub/ob</p><pre><code class="powershell">python predict_preliminary.py --dev_dir_trigger trigger_simple_ckpt_dir  --dev_dir_role role1_simple_ckpt_dir</code></pre></li><li><p>运行<code>src_final/preprocess</code>下的<code>convert_raw_data_preliminary.py</code></p><pre><code class="shell">python convert_raw_data_preliminary.py</code></pre></li><li><p>运行<code>src_final/preprocess</code>下的<code>convert_raw_data.py</code>即完成了初赛数据的清洗</p><pre><code class="shell">python convert_raw_data.py</code></pre></li></ul><h2 id="需要学习的知识点"><a href="#需要学习的知识点" class="headerlink" title="需要学习的知识点"></a>需要学习的知识点</h2><ol><li><p>远程监督trigger</p><p>这个好像没啥重要的，就是一个knowledge base的概念。</p></li><li><p>NER中的span解码是什么意思？</p></li><li><p>数据增强</p></li></ol><h2 id="实操问题"><a href="#实操问题" class="headerlink" title="实操问题"></a>实操问题</h2><p>运行脚本：</p><pre><code class="python">你说的是ipython下吧，%run python_file.py</code></pre><h3 id="整体思路框架"><a href="#整体思路框架" class="headerlink" title="整体思路框架"></a>整体思路框架</h3><ol><li>解析命令行参数</li><li>数据预处理</li><li>train</li><li>eval</li><li>test</li></ol><h3 id="BERT问题"><a href="#BERT问题" class="headerlink" title="BERT问题"></a>BERT问题</h3><ol><li><p>在BERT文件夹下，有一个<code>vocab.txt</code>，这个东西是什么？</p><p><code>vocab.txt</code>是BERT model的词典。我们知道BERT model的输入是<code>a sequence of ID</code>。既然是ID，那么肯定要事先有一个词汇表，建立词汇表中单词到ID的映射，这样在碰到新的句子时才能给句子中每个词匹配上对应的ID。同时，规定死词汇表，也是为了Word Embedding考虑。时刻记得BERT就是<code>Contextualized Word Representation</code>。</p><blockquote><ul><li><a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/</a></li></ul></blockquote></li><li><p>承接上一个问题，<code>vocab.txt</code>里面有很多的<code>[unused***]</code>，它们是做什么的？</p><p>简单来说，就是如果你有自己<strong>想添加的新词汇</strong>，你就替换其中一个<code>[unused***]</code>就好了。</p><blockquote><p>Just replace the “[unusedX]” tokens with your vocabulary. Since these were not used they are effectively randomly initialized.</p></blockquote><blockquote><ul><li><a href="https://stackoverflow.com/questions/62452271/understanding-bert-vocab-unusedxxx-tokens?newreg=086b8f46787b4857805ed930352d5088">https://stackoverflow.com/questions/62452271/understanding-bert-vocab-unusedxxx-tokens?newreg=086b8f46787b4857805ed930352d5088</a></li><li><a href="https://blog.csdn.net/kyle1314608/article/details/106612044/">https://blog.csdn.net/kyle1314608/article/details/106612044/</a></li></ul></blockquote></li><li><p>什么是WordPiece？</p></li></ol><h3 id="trigger提取阶段"><a href="#trigger提取阶段" class="headerlink" title="trigger提取阶段"></a>trigger提取阶段</h3><p><code>TriggerFeature</code>里面封装的数据：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmrzfsziqpj20s30a2ta3.jpg"></p><p>控制台输出如下：</p><pre><code class="output">01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   *** train_example-0 ***01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   text: 原 标 题 ： 2 0 1 9 年 反 腐 败 “ 成 绩 单 ” 亮 眼 ● 从 数 据 上 看 ， 2 0 1 9 年 的 反 腐 败 成 绩 单 引 人 注 目 ， 反 映 了 当 前 反 腐 败 力 度 仍 然 没 有 放 松 、 反 腐 败 工 作 仍 然 按 照 既 定 目 标 推 进 、 高 压 反 腐 态 势 仍 然 保 持 ， 由 此 查 处 的 中 管 干 部 、 省 管 干 部 这 些 “ 关 键 少 数 ” 干 部 数 量 仍 然 保 持 高 位 并 呈 现 增 长 态 势 ● 2 0 1 9 年 反 腐 败 主 要 呈 现 三 个 方 面 的 特 点 ： 一 是 精 准 反 腐 ； 二 是 强 化 标 本 兼 治 ； 三 是 深 化 基 层 反 腐 败 ● 中 央 针 对 全 面 从 严 治 党 和 党 风 廉 政 建 设 领 域 ， 出 台 了 一 系 列 具 有 代 表 性 的 党 内 法 规 制 度 ， 为 整 个 监 察 体 制 改 革 提 供 了 制 度 保 障 ， 同 时 着 力 建 立 健 全 一 个 具 有 中 国 特 色 的 监 察 法 律 体 系 ， 推 动 制 度 反 腐 、 法 治 防 腐 制 图 / 李 晓 军 2 0 1 9 年 1 2 月 2 6 日 1 4 时 3 0 分 ， 中 央 纪 委 国 家 监 委 网 站 发 布 了 年 度 最 新 一 条 执 纪 审 查 消 息 ， “ 呼 和 浩 特 职 业 学 院 原 党 委 副 书 记 、 院 长 李 怀 柱 接 受 纪 律 审 查 和 监 察 调 查 ”01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   token_ids: [101, 1333, 3403, 7579, 8038, 123, 121, 122, 130, 2399, 1353, 5576, 6571, 100, 2768, 5327, 1296, 100, 778, 4706, 474, 794, 3144, 2945, 677, 4692, 8024, 123, 121, 122, 130, 2399, 4638, 1353, 5576, 6571, 2768, 5327, 1296, 2471, 782, 3800, 4680, 8024, 1353, 3216, 749, 2496, 1184, 1353, 5576, 6571, 1213, 2428, 793, 4197, 3766, 3300, 3123, 3351, 510, 1353, 5576, 6571, 2339, 868, 793, 4197, 2902, 4212, 3188, 2137, 4680, 3403, 2972, 6822, 510, 7770, 1327, 1353, 5576, 2578, 1232, 793, 4197, 924, 2898, 8024, 4507, 3634, 3389, 1905, 4638, 704, 5052, 2397, 6956, 510, 4689, 5052, 2397, 6956, 6821, 763, 100, 1068, 7241, 2208, 3144, 100, 2397, 6956, 3144, 7030, 793, 4197, 924, 2898, 7770, 855, 2400, 1439, 4385, 1872, 7270, 2578, 1232, 474, 123, 121, 122, 130, 2399, 1353, 5576, 6571, 712, 6206, 1439, 4385, 676, 702, 3175, 7481, 4638, 4294, 4157, 8038, 671, 3221, 5125, 1114, 1353, 5576, 8039, 753, 3221, 2487, 1265, 3403, 3315, 1076, 3780, 8039, 676, 3221, 3918, 1265, 1825, 2231, 1353, 5576, 6571, 474, 704, 1925, 7151, 2190, 1059, 7481, 794, 698, 3780, 1054, 1469, 1054, 7599, 2442, 3124, 2456, 6392, 7566, 1818, 8024, 1139, 1378, 749, 671, 5143, 1154, 1072, 3300, 807, 6134, 2595, 4638, 1054, 1079, 3791, 6226, 1169, 2428, 8024, 711, 3146, 702, 4664, 2175, 860, 1169, 3121, 7484, 2990, 897, 749, 1169, 2428, 924, 7397, 8024, 1398, 3198, 4708, 1213, 2456, 4989, 978, 1059, 671, 702, 1072, 3300, 704, 1744, 4294, 5682, 4638, 4664, 2175, 3791, 2526, 860, 5143, 8024, 2972, 1220, 1169, 2428, 1353, 5576, 510, 3791, 3780, 7344, 5576, 1169, 1745, 120, 3330, 3236, 1092, 123, 121, 122, 130, 2399, 122, 123, 3299, 123, 127, 3189, 122, 125, 3198, 124, 121, 1146, 8024, 704, 1925, 5279, 1999, 1744, 2157, 4664, 1999, 5381, 4991, 1355, 2357, 749, 2399, 2428, 3297, 3173, 671, 3340, 2809, 5279, 2144, 3389, 3867, 2622, 8024, 100, 1461, 1469, 3856, 102]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   distant trigger: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]convert examples:   0%|          | 1/7045 [00:00&lt;13:28,  8.71it/s]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   *** train_example-1 ***01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   text: 《 法 制 日 报 》 记 者 梳 理 中 央 纪 委 国 家 监 委 网 站 通 报 信 息 发 现 ， 2 0 1 9 年 中 央 纪 委 国 家 监 委 网 站 审 查 调 查 栏 目 总 计 通 报 执 纪 审 查 中 管 干 部 2 0 人 、 省 管 干 部 4 0 8 人01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   token_ids: [101, 517, 3791, 1169, 3189, 2845, 518, 6381, 5442, 3463, 4415, 704, 1925, 5279, 1999, 1744, 2157, 4664, 1999, 5381, 4991, 6858, 2845, 928, 2622, 1355, 4385, 8024, 123, 121, 122, 130, 2399, 704, 1925, 5279, 1999, 1744, 2157, 4664, 1999, 5381, 4991, 2144, 3389, 6444, 3389, 3408, 4680, 2600, 6369, 6858, 2845, 2809, 5279, 2144, 3389, 704, 5052, 2397, 6956, 123, 121, 782, 510, 4689, 5052, 2397, 6956, 125, 121, 129, 782, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   distant trigger: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   *** train_example-2 ***01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   text: 《 法 制 日 报 》 记 者 根 据 中 央 纪 委 国 家 监 委 网 站 审 查 调 查 栏 目 通 报 信 息 统 计 ， 2 0 1 9 年 总 计 通 报 执 纪 审 查 中 管 干 部 2 0 人 ， 与 此 前 两 年 持 平 （ 2 0 1 7 年 通 报 执 纪 审 查 中 管 干 部 1 8 人 ， 2 0 1 8 年 则 是 2 3 人 ）01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   token_ids: [101, 517, 3791, 1169, 3189, 2845, 518, 6381, 5442, 3418, 2945, 704, 1925, 5279, 1999, 1744, 2157, 4664, 1999, 5381, 4991, 2144, 3389, 6444, 3389, 3408, 4680, 6858, 2845, 928, 2622, 5320, 6369, 8024, 123, 121, 122, 130, 2399, 2600, 6369, 6858, 2845, 2809, 5279, 2144, 3389, 704, 5052, 2397, 6956, 123, 121, 782, 8024, 680, 3634, 1184, 697, 2399, 2898, 2398, 8020, 123, 121, 122, 128, 2399, 6858, 2845, 2809, 5279, 2144, 3389, 704, 5052, 2397, 6956, 122, 129, 782, 8024, 123, 121, 122, 129, 2399, 1156, 3221, 123, 124, 782, 8021, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/18/2021 14:01:28 - INFO - src_final.preprocess.processor -   distant trigger: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]convert examples: 100%|██████████| 7045/7045 [00:39&lt;00:00, 180.27it/s]</code></pre><h3 id="role提取阶段"><a href="#role提取阶段" class="headerlink" title="role提取阶段"></a>role提取阶段</h3><p>role1阶段，<code>RoleFeature</code>里面封装的数据：</p><pre><code class="tex">01/21/2021 10:07:09 - INFO - src_final.preprocess.processor -   *** train_example-0 ***01/21/2021 10:07:10 - INFO - src_final.preprocess.processor -   text: 原 标 题 ： 2 0 1 9 年 反 腐 败 “ 成 绩 单 ” 亮 眼 ● 从 数 据 上 看 ， 2 0 1 9 年 的 反 腐 败 成 绩 单 引 人 注 目 ， 反 映 了 当 前 反 腐 败 力 度 仍 然 没 有 放 松 、 反 腐 败 工 作 仍 然 按 照 既 定 目 标 推 进 、 高 压 反 腐 态 势 仍 然 保 持 ， 由 此 查 处 的 中 管 干 部 、 省 管 干 部 这 些 “ 关 键 少 数 ” 干 部 数 量 仍 然 保 持 高 位 并 呈 现 增 长 态 势 ● 2 0 1 9 年 反 腐 败 主 要 呈 现 三 个 方 面 的 特 点 ： 一 是 精 准 反 腐 ； 二 是 强 化 标 本 兼 治 ； 三 是 深 化 基 层 反 腐 败 ● 中 央 针 对 全 面 从 严 治 党 和 党 风 廉 政 建 设 领 域 ， 出 台 了 一 系 列 具 有 代 表 性 的 党 内 法 规 制 度 ， 为 整 个 监 察 体 制 改 革 提 供 了 制 度 保 障 ， 同 时 着 力 建 立 健 全 一 个 具 有 中 国 特 色 的 监 察 法 律 体 系 ， 推 动 制 度 反 腐 、 法 治 防 腐 制 图 / 李 晓 军 2 0 1 9 年 1 2 月 2 6 日 1 4 时 3 0 分 ， 中 央 纪 委 国 家 监 委 网 站 发 布 了 年 度 最 新 一 条 执 纪 审 查 消 息 ， “ 呼 和 浩 特 职 业 学 院 原 党 委 副 书 记 、 院 长 李 怀 柱 接 受 纪 律 审 查 和 监 察 调 查 ”01/21/2021 10:07:10 - INFO - src_final.preprocess.processor -   token_ids: [101, 1333, 3403, 7579, 8038, 123, 121, 122, 130, 2399, 1353, 5576, 6571, 100, 2768, 5327, 1296, 100, 778, 4706, 474, 794, 3144, 2945, 677, 4692, 8024, 123, 121, 122, 130, 2399, 4638, 1353, 5576, 6571, 2768, 5327, 1296, 2471, 782, 3800, 4680, 8024, 1353, 3216, 749, 2496, 1184, 1353, 5576, 6571, 1213, 2428, 793, 4197, 3766, 3300, 3123, 3351, 510, 1353, 5576, 6571, 2339, 868, 793, 4197, 2902, 4212, 3188, 2137, 4680, 3403, 2972, 6822, 510, 7770, 1327, 1353, 5576, 2578, 1232, 793, 4197, 924, 2898, 8024, 4507, 3634, 3389, 1905, 4638, 704, 5052, 2397, 6956, 510, 4689, 5052, 2397, 6956, 6821, 763, 100, 1068, 7241, 2208, 3144, 100, 2397, 6956, 3144, 7030, 793, 4197, 924, 2898, 7770, 855, 2400, 1439, 4385, 1872, 7270, 2578, 1232, 474, 123, 121, 122, 130, 2399, 1353, 5576, 6571, 712, 6206, 1439, 4385, 676, 702, 3175, 7481, 4638, 4294, 4157, 8038, 671, 3221, 5125, 1114, 1353, 5576, 8039, 753, 3221, 2487, 1265, 3403, 3315, 1076, 3780, 8039, 676, 3221, 3918, 1265, 1825, 2231, 1353, 5576, 6571, 474, 704, 1925, 7151, 2190, 1059, 7481, 794, 698, 3780, 1054, 1469, 1054, 7599, 2442, 3124, 2456, 6392, 7566, 1818, 8024, 1139, 1378, 749, 671, 5143, 1154, 1072, 3300, 807, 6134, 2595, 4638, 1054, 1079, 3791, 6226, 1169, 2428, 8024, 711, 3146, 702, 4664, 2175, 860, 1169, 3121, 7484, 2990, 897, 749, 1169, 2428, 924, 7397, 8024, 1398, 3198, 4708, 1213, 2456, 4989, 978, 1059, 671, 702, 1072, 3300, 704, 1744, 4294, 5682, 4638, 4664, 2175, 3791, 2526, 860, 5143, 8024, 2972, 1220, 1169, 2428, 1353, 5576, 510, 3791, 3780, 7344, 5576, 1169, 1745, 120, 3330, 3236, 1092, 123, 121, 122, 130, 2399, 122, 123, 3299, 123, 127, 3189, 122, 125, 3198, 124, 121, 1146, 8024, 704, 1925, 5279, 1999, 1744, 2157, 4664, 1999, 5381, 4991, 1355, 2357, 749, 2399, 2428, 3297, 3173, 671, 3340, 2809, 5279, 2144, 3389, 3867, 2622, 8024, 100, 1461, 1469, 3856, 102]01/21/2021 10:07:11 - INFO - src_final.preprocess.processor -   attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]01/21/2021 10:07:11 - INFO - src_final.preprocess.processor -   token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/21/2021 10:07:12 - INFO - src_final.preprocess.processor -   trigger location: [299, 300]</code></pre><p>以下是训练过程中的描述：</p><pre><code class="tex">01/21/2021 10:28:35 - INFO - src_final.utils.functions_utils -   Use single gpu in: ['0']01/21/2021 10:29:06 - INFO - src_final.utils.trainer -   ***** Running training *****01/21/2021 10:29:06 - INFO - src_final.utils.trainer -     Num Examples = 741601/21/2021 10:29:07 - INFO - src_final.utils.trainer -     Num Epochs = 101/21/2021 10:29:07 - INFO - src_final.utils.trainer -     Total training batch size = 201/21/2021 10:29:08 - INFO - src_final.utils.trainer -     Total optimization steps = 3708</code></pre><p>role的验证如何进行？</p><p>跟trigger的验证差不多。</p><hr><p>role2阶段，<code>RoleFeature</code>里面封装的数据：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmvdz7b9clj20no095gn0.jpg"></p><pre><code class="text">01/21/2021 15:53:09 - INFO - src_final.preprocess.processor -   *** train_example-0 ***01/21/2021 15:53:09 - INFO - src_final.preprocess.processor -   text: 原 标 题 ： 2 0 1 9 年 反 腐 败 “ 成 绩 单 ” 亮 眼 ● 从 数 据 上 看 ， 2 0 1 9 年 的 反 腐 败 成 绩 单 引 人 注 目 ， 反 映 了 当 前 反 腐 败 力 度 仍 然 没 有 放 松 、 反 腐 败 工 作 仍 然 按 照 既 定 目 标 推 进 、 高 压 反 腐 态 势 仍 然 保 持 ， 由 此 查 处 的 中 管 干 部 、 省 管 干 部 这 些 “ 关 键 少 数 ” 干 部 数 量 仍 然 保 持 高 位 并 呈 现 增 长 态 势 ● 2 0 1 9 年 反 腐 败 主 要 呈 现 三 个 方 面 的 特 点 ： 一 是 精 准 反 腐 ； 二 是 强 化 标 本 兼 治 ； 三 是 深 化 基 层 反 腐 败 ● 中 央 针 对 全 面 从 严 治 党 和 党 风 廉 政 建 设 领 域 ， 出 台 了 一 系 列 具 有 代 表 性 的 党 内 法 规 制 度 ， 为 整 个 监 察 体 制 改 革 提 供 了 制 度 保 障 ， 同 时 着 力 建 立 健 全 一 个 具 有 中 国 特 色 的 监 察 法 律 体 系 ， 推 动 制 度 反 腐 、 法 治 防 腐 制 图 / 李 晓 军 2 0 1 9 年 1 2 月 2 6 日 1 4 时 3 0 分 ， 中 央 纪 委 国 家 监 委 网 站 发 布 了 年 度 最 新 一 条 执 纪 审 查 消 息 ， “ 呼 和 浩 特 职 业 学 院 原 党 委 副 书 记 、 院 长 李 怀 柱 接 受 纪 律 审 查 和 监 察 调 查 ”01/21/2021 15:53:10 - INFO - src_final.preprocess.processor -   trigger location: [299, 300]01/21/2021 15:53:10 - INFO - src_final.preprocess.processor -   labels: [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]</code></pre><p>训练信息如下：</p><pre><code class="text">01/21/2021 19:44:49 - INFO - src_final.utils.functions_utils -   Use single gpu in: ['0']01/21/2021 19:45:06 - INFO - src_final.utils.trainer -   ***** Running training *****01/21/2021 19:45:06 - INFO - src_final.utils.trainer -     Num Examples = 580801/21/2021 19:45:06 - INFO - src_final.utils.trainer -     Num Epochs = 101/21/2021 19:45:07 - INFO - src_final.utils.trainer -     Total training batch size = 201/21/2021 19:45:08 - INFO - src_final.utils.trainer -     Total optimization steps = 2904</code></pre><hr><p><strong>为什么role1和role2使用的标签是不一样的？</strong></p><p>role1采用的是start-end类型的标签，分开训练sub和obj。</p><p>而role2采用的是s、i、e、s、x、o类型的标签，而且还是联合训练time和loc，没有分开进行训练。<img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmvkmvsfwkj21bf06hwfw.jpg" alt="bert_outputs的内容"></p><h3 id="attribution提取阶段"><a href="#attribution提取阶段" class="headerlink" title="attribution提取阶段"></a>attribution提取阶段</h3><p><code>AttributionFeature</code>里面封装的数据：</p><pre><code class="text">01/21/2021 21:01:17 - INFO - src_final.preprocess.processor -   *** train_example-0 ***01/21/2021 21:01:17 - INFO - src_final.preprocess.processor -   text: 原 标 题 ： 2 0 1 9 年 反 腐 败 “ 成 绩 单 ” 亮 眼 ● 从 数 据 上 看 ， 2 0 1 9 年 的 反 腐 败 成 绩 单 引 人 注 目 ， 反 映 了 当 前 反 腐 败 力 度 仍 然 没 有 放 松 、 反 腐 败 工 作 仍 然 按 照 既 定 目 标 推 进 、 高 压 反 腐 态 势 仍 然 保 持 ， 由 此 查 处 的 中 管 干 部 、 省 管 干 部 这 些 “ 关 键 少 数 ” 干 部 数 量 仍 然 保 持 高 位 并 呈 现 增 长 态 势 ● 2 0 1 9 年 反 腐 败 主 要 呈 现 三 个 方 面 的 特 点 ： 一 是 精 准 反 腐 ； 二 是 强 化 标 本 兼 治 ； 三 是 深 化 基 层 反 腐 败 ● 中 央 针 对 全 面 从 严 治 党 和 党 风 廉 政 建 设 领 域 ， 出 台 了 一 系 列 具 有 代 表 性 的 党 内 法 规 制 度 ， 为 整 个 监 察 体 制 改 革 提 供 了 制 度 保 障 ， 同 时 着 力 建 立 健 全 一 个 具 有 中 国 特 色 的 监 察 法 律 体 系 ， 推 动 制 度 反 腐 、 法 治 防 腐 制 图 / 李 晓 军 2 0 1 9 年 1 2 月 2 6 日 1 4 时 3 0 分 ， 中 央 纪 委 国 家 监 委 网 站 发 布 了 年 度 最 新 一 条 执 纪 审 查 消 息 ， “ 呼 和 浩 特 职 业 学 院 原 党 委 副 书 记 、 院 长 李 怀 柱 接 受 纪 律 审 查 和 监 察 调 查 ”01/21/2021 21:01:18 - INFO - src_final.preprocess.processor -   token_ids: [101, 1333, 3403, 7579, 8038, 123, 121, 122, 130, 2399, 1353, 5576, 6571, 100, 2768, 5327, 1296, 100, 778, 4706, 474, 794, 3144, 2945, 677, 4692, 8024, 123, 121, 122, 130, 2399, 4638, 1353, 5576, 6571, 2768, 5327, 1296, 2471, 782, 3800, 4680, 8024, 1353, 3216, 749, 2496, 1184, 1353, 5576, 6571, 1213, 2428, 793, 4197, 3766, 3300, 3123, 3351, 510, 1353, 5576, 6571, 2339, 868, 793, 4197, 2902, 4212, 3188, 2137, 4680, 3403, 2972, 6822, 510, 7770, 1327, 1353, 5576, 2578, 1232, 793, 4197, 924, 2898, 8024, 4507, 3634, 3389, 1905, 4638, 704, 5052, 2397, 6956, 510, 4689, 5052, 2397, 6956, 6821, 763, 100, 1068, 7241, 2208, 3144, 100, 2397, 6956, 3144, 7030, 793, 4197, 924, 2898, 7770, 855, 2400, 1439, 4385, 1872, 7270, 2578, 1232, 474, 123, 121, 122, 130, 2399, 1353, 5576, 6571, 712, 6206, 1439, 4385, 676, 702, 3175, 7481, 4638, 4294, 4157, 8038, 671, 3221, 5125, 1114, 1353, 5576, 8039, 753, 3221, 2487, 1265, 3403, 3315, 1076, 3780, 8039, 676, 3221, 3918, 1265, 1825, 2231, 1353, 5576, 6571, 474, 704, 1925, 7151, 2190, 1059, 7481, 794, 698, 3780, 1054, 1469, 1054, 7599, 2442, 3124, 2456, 6392, 7566, 1818, 8024, 1139, 1378, 749, 671, 5143, 1154, 1072, 3300, 807, 6134, 2595, 4638, 1054, 1079, 3791, 6226, 1169, 2428, 8024, 711, 3146, 702, 4664, 2175, 860, 1169, 3121, 7484, 2990, 897, 749, 1169, 2428, 924, 7397, 8024, 1398, 3198, 4708, 1213, 2456, 4989, 978, 1059, 671, 702, 1072, 3300, 704, 1744, 4294, 5682, 4638, 4664, 2175, 3791, 2526, 860, 5143, 8024, 2972, 1220, 1169, 2428, 1353, 5576, 510, 3791, 3780, 7344, 5576, 1169, 1745, 120, 3330, 3236, 1092, 123, 121, 122, 130, 2399, 122, 123, 3299, 123, 127, 3189, 122, 125, 3198, 124, 121, 1146, 8024, 704, 1925, 5279, 1999, 1744, 2157, 4664, 1999, 5381, 4991, 1355, 2357, 749, 2399, 2428, 3297, 3173, 671, 3340, 2809, 5279, 2144, 3389, 3867, 2622, 8024, 100, 1461, 1469, 3856, 102]01/21/2021 21:01:18 - INFO - src_final.preprocess.processor -   attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]01/21/2021 21:01:21 - INFO - src_final.preprocess.processor -   token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]01/21/2021 21:01:21 - INFO - src_final.preprocess.processor -   trigger loc: (299, 300)01/21/2021 21:01:22 - INFO - src_final.preprocess.processor -   labels: [0, 0]</code></pre><p>process data之后，打印的训练信息：</p><pre><code class="text">01/21/2021 21:11:27 - INFO - src_final.utils.functions_utils -   Use single gpu in: ['0']01/21/2021 21:13:04 - INFO - src_final.utils.trainer -   ***** Running training *****01/21/2021 21:13:05 - INFO - src_final.utils.trainer -     Num Examples = 249401/21/2021 21:13:06 - INFO - src_final.utils.trainer -     Num Epochs = 101/21/2021 21:13:06 - INFO - src_final.utils.trainer -     Total training batch size = 201/21/2021 21:13:07 - INFO - src_final.utils.trainer -     Total optimization steps = 1247</code></pre><h3 id="数据问题"><a href="#数据问题" class="headerlink" title="数据问题"></a>数据问题</h3><p>在看了<code>covert_raw_data.py</code>之后，我终于知道项目<code>data/final</code>下面的数据集都是什么意思了。</p><p>比如在<code>data/final/raw_data/</code>目录下：</p><ul><li><p><code>raw_stack.json</code></p><p>是官方提供的复赛<code>train set</code>，未经任何处理。</p></li><li><p><code>sentences.json</code></p><p>是官方提供的复赛<code>test set</code>，未经任何处理。</p></li></ul><hr><ul><li><p><code>stack.json</code></p><p><strong>处理后</strong>的官方提供的复赛<code>train set</code>；使用10折交叉验证的方法，加入了<code>distant trigger</code>，删去了<code>words</code>。</p></li><li><p><code>train.json</code></p><p>对官方提供的复赛<code>train set</code>采用8/2划分得到的<code>train set</code>（8）；</p></li><li><p><code>dev.json</code></p><p>对官方提供的复赛<code>train set</code>采用8/2划分得到的<code>dev set</code>（2）；</p></li></ul><hr><ul><li><p><code>test.json</code></p><p><strong>处理后</strong>（加入了<code>distant trigger</code>）的官方提供的复赛<code>test set</code>；</p></li></ul><hr><ul><li><p><code>preliminary_pred_triggers_pred_roles</code></p><p>【数据增强】阶段，初赛数据通过Base Model预测得到的结果；</p><p>至于<code>preliminary_pred_triggers_pred_roles</code>与<code>preliminary_data_pred_trigger_pred_role</code>的区别，我还没搞太清楚。</p></li><li><p><code>preliminary_stack</code><font color="red">（这里有疑问）</font></p><p><strong>处理后</strong>（加入了<code>distant trigger</code>）的初赛预测结果数据；</p></li></ul><hr><h3 id="src-final-preprocess-processor-py"><a href="#src-final-preprocess-processor-py" class="headerlink" title="src_final/preprocess/processor.py"></a>src_final/preprocess/processor.py</h3><p>转换数据为 Bert 模型的输入</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li><p><code>tqdm</code>是什么？</p><p>就是一个加强版的<code>enumerate()</code>，最大的作用是显示进度条：</p><p>76%|████████████████████████     | 7568/10000 [00:33&lt;00:10, 229.00it/s]</p><p>官方介绍：<a href="https://github.com/tqdm/tqdm">https://github.com/tqdm/tqdm</a></p></li></ol><h3 id="程序流程"><a href="#程序流程" class="headerlink" title="程序流程"></a>程序流程</h3><ol><li></li><li>转换数据为BERT需要的Token ID</li><li>构造Dataset：将数据集转换成PyTorch中的Dataset类型</li></ol><p>main()  <span class="github-emoji"><span>➡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/27a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> training() <span class="github-emoji"><span>➡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/27a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> train_base() <span class="github-emoji"><span>➡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/27a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> train() </p><h3 id="程序Bug"><a href="#程序Bug" class="headerlink" title="程序Bug"></a>程序Bug</h3><h4 id="list-index-out-of-range"><a href="#list-index-out-of-range" class="headerlink" title="list index out of range"></a>list index out of range</h4><script src="https://gist.github.com/yc1999/7e1867cf14e881186e78c2544c631926.js"></script><p>./out/final/trigger/roberta_wwm_distant_trigger_pgd_enhanced/checkpoint-3523</p><p>./out/final/trigger/roberta_wwm_distant_trigger_pgd_enhanced/checkpoint-7045</p><pre><code class="python">model_lists = ["./out\\final\\trigger\\roberta_wwm_distant_trigger_pgd_enhanced\\checkpoint-0001\\model.pt","./out\\final\\trigger\\roberta_wwm_distant_trigger_pgd_enhanced\\checkpoint-0003\\model.pt","./out\\final\\trigger\\roberta_wwm_distant_trigger_pgd_enhanced\\checkpoint-0002\\model.pt","./out\\final\\trigger\\roberta_wwm_distant_trigger_pgd_enhanced\\checkpoint-0004\\model.pt"]model_lists = sorted(model_lists,key=lambda x: (x.split('\\')[-3], int(x.split('\\')[-2].split('-')[-1])))</code></pre><pre><code class="python">model_lists = ["./out/final/trigger/roberta_wwm_distant_trigger_pgd_enhanced/checkpoint-0001/model.pt","./out/final/trigger/roberta_wwm_distant_trigger_pgd_enhanced/checkpoint-0002/model.pt","./out/final/trigger/roberta_wwm_distant_trigger_pgd_enhanced/checkpoint-0003/model.pt","./out/final/trigger/roberta_wwm_distant_trigger_pgd_enhanced/checkpoint-0004/model.pt"]model_lists = sorted(model_lists,key=lambda x: (x.split('/')[-3], int(x.split('/')[-2].split('-')[-1])))</code></pre><pre><code class="python">list = ["a/3","a/2","a/1"]list = sorted(list,key=lambda x:(x.split('/')[-2],int(x.split('/')[-1])),reverse=True)</code></pre><h3 id="理解pipeline和joint-learning"><a href="#理解pipeline和joint-learning" class="headerlink" title="理解pipeline和joint learning"></a>理解pipeline和joint learning</h3><ul><li>pipeline：每个模块单独训练，各模块之间完全独立，不存在输入和输出的依赖；</li><li>joint learning：一个模块的输出作为下一个模块的输入，最后一起计算误差，一起进行训练；</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 事件抽取 </tag>
            
            <tag> 信息抽取 </tag>
            
            <tag> 科大讯飞大赛-事件抽取挑战赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科大讯飞-事件抽取挑战赛 赛事概要</title>
      <link href="/2021/01/14/ke-da-xun-fei-shi-jian-chou-qu-tiao-zhan-sai-sai-shi-gai-yao/"/>
      <url>/2021/01/14/ke-da-xun-fei-shi-jian-chou-qu-tiao-zhan-sai-sai-shi-gai-yao/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：</p><p>毕设选题是事件抽取，但是这一周了解下来，我遇到了两个问题：</p><ol><li>网络上关于事件抽取的论文都好杂好乱，连一篇优质综述都没找到，一点都不系统化。</li><li>自己做的评测任务——《面向金融领域的小样本跨类迁移事件抽取》没有很好的开源解决方案，所以NLP小白一度觉得很迷，不知如何下手。</li></ol><p>因此，针对上面两个问题，我打算通过《科大讯飞-事件抽取挑战赛》这个比赛来解决：<font color="red">一边看开源解决方案，一边补充其中涉及到的事件抽取/NLP知识，以复现开源解决方案来促进自己的学习。同时也需要每周阅读几篇另外的顶会论文来扩展自己的视野。</font></p></blockquote><p>🔗原文链接：<a href="http://challenge.xfyun.cn/topic/info?type=hotspot">http://challenge.xfyun.cn/topic/info?type=hotspot</a></p><h2 id="赛事概要"><a href="#赛事概要" class="headerlink" title="赛事概要"></a>赛事概要</h2><h3 id="一、赛事背景"><a href="#一、赛事背景" class="headerlink" title="一、赛事背景"></a>一、赛事背景</h3><p>事件抽取将非结构化文本中的事件信息展现为结构化形式，在舆情监测、文本摘要、自动问答、事理图谱自动构建等领域有着重要应用。在真实新闻中，由于文本中可能存在句式复杂，主被动转换，多事件主客体共享等难点，因此“事件抽取”是一项极具挑战的抽取任务。</p><h3 id="二、赛事任务"><a href="#二、赛事任务" class="headerlink" title="二、赛事任务"></a>二、赛事任务</h3><p>本赛事任务旨在从通用新闻文本中抽取<strong>事件触发词</strong>、<strong>事件论元</strong>以及<strong>事件属性</strong>。</p><p>在传统的事件定义中，事件由<strong>事件触发词（Trigger）</strong> 和<strong>描述事件结构的元素（Argument）</strong>构成。事件触发词标识着事件的发生。事件论元为事件主体（Subject）、客体（Object）、时间（Time）、地点（Location）等，是表达事件重要信息的载体。</p><p><strong>事件属性</strong>包括事件极性（Polarity）、时态（Tense），是衡量事件是否真实发生的重要依据。 通过极性，事件分为肯定、否定、可能事件。通过时态，事件分为过去发生的事件、现在正在发生的事件、将要发生的事件以及其他无法确定时态的事件。</p><p>本赛事任务一为初赛任务，任务二为复赛任务，在任务一的基础上增加了事件属性识别。为了模拟真实场景，数据中包含了非实际发生的事件。</p><h4 id="📝任务一：事件触发词及论元抽取"><a href="#📝任务一：事件触发词及论元抽取" class="headerlink" title="📝任务一：事件触发词及论元抽取"></a>📝任务一：事件触发词及论元抽取</h4><p>该任务旨在从文本中抽取标识事件发生的触发词和论元，触发词往往为动词和名词。触发词对应的事件论元，主要为主体（Subject）、客体（Object）、时间（Time）、地点（Location），其中主体为必备论元。</p><p>示例 1：</p><p>文本：北京时间 3 月 27 日晚上 7 点 15 分，英国首相鲍里斯约翰逊确诊感染了新冠肺炎。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9hr17jaj20gz03at8m.jpg"></p><p>示例 2：</p><p>文本：4 月 1 日，因应英国央行英伦银行的要求，汇丰控股及渣打集团一举停止派息及回购。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9iw6aguj20h002yglh.jpg"></p><p>示例 3：</p><p>文本：过渡政府部队发言人说, 北约的战机 1 6 日在苏尔特附近击中了一座建筑，炸死大批卡扎菲部队士兵。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9kvzce2j20h103uq2w.jpg"></p><p>示例 4：</p><p>文本：中华人民共和国证监会正式表态，对中国星巴克瑞幸咖啡财务造假行为表示强烈的谴责。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9ldn4ltj20gz02xa9y.jpg"></p><h4 id="📝任务二：事件属性抽取"><a href="#📝任务二：事件属性抽取" class="headerlink" title="📝任务二：事件属性抽取"></a>📝任务二：事件属性抽取</h4><p>该任务旨在从文本中抽取表达事件发生状态的属性，包括极性、时态。极性分为：肯定、否定、可能；时态分为：过去、现在、将来、其他。</p><p>示例 1：</p><p>文本：中国驻俄罗斯大使张汉晖 4 月 7 日向媒体回应称，经向俄有关强力部门了解，目前在俄没有一起中国公民遭到拘留或受到俄强力部门限制的案例。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9o356fwj209l01ygle.jpg"></p><p>示例 2：</p><p>文本：过往世卫组织曾 5 度宣布“国际关注公共卫生紧急事件”。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9ohxz27j209k022dfm.jpg"></p><p>示例 3：</p><p>文本：英国很可能将恢复接受世界贸易组织条款的规范。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9owrhhwj209j023t8i.jpg"></p><p>示例 4：</p><p>文本：看守政府总理迈赫迪打算驱逐约 5300 名美国士兵。</p><p>抽取结果：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmn9pfa1nrj209l01ydfm.jpg"></p><h3 id="三、评审规则"><a href="#三、评审规则" class="headerlink" title="三、评审规则"></a>三、评审规则</h3><h4 id="1-初赛数据说明"><a href="#1-初赛数据说明" class="headerlink" title="1. 初赛数据说明"></a>1. 初赛数据说明</h4><blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnct1xl7nj20fn0j9tbu.jpg"></p></blockquote><p>本次比赛初赛为参赛选手提供了<strong>6958条中文句子</strong>，及其<strong>9644条提取结果（存在一对多的情况）</strong>：</p><p>1.1 训练集：共5758条句子，包含句子中对应的<strong>触发词、论元</strong>等，用于竞赛模型训练。（训练集在<code>data/preliminary/train.csv</code>当中）</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnasndqiyj20au08uq30.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnatt1aq4j207z01aq2q.jpg"></p><p>1.2 测试集：共1200条句子。（测试集在<code>data/preliminary/test.csv</code>中）</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnaw209blj20aj08t74d.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnawo8a4hj207301ijr6.jpg"></p><h4 id="2-复赛数据说明"><a href="#2-复赛数据说明" class="headerlink" title="2. 复赛数据说明"></a>2. 复赛数据说明</h4><blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnctrf3apj21160hk0ul.jpg"></p></blockquote><p>本次比赛复赛为参赛选手提供了<strong>3335条中文句子，及其3384条提取结果（存在一对多的情况）</strong>：</p><p>2.1 训练集：共2456条句子，包含句子中对应的<strong>触发词、论元及其角色、事件属性</strong>等，用于竞赛模型训练。</p><p>2.2 测试集：共879条句子。</p><h4 id="3-评价指标"><a href="#3-评价指标" class="headerlink" title="3. 评价指标"></a>3. 评价指标</h4><p>本模型依据提交的结果文件，采用F值进行评价。</p><h5 id="3-1-事件触发词及论元抽取评价指标"><a href="#3-1-事件触发词及论元抽取评价指标" class="headerlink" title="3.1 事件触发词及论元抽取评价指标"></a>3.1 事件触发词及论元抽取评价指标</h5><p>对于事件触发词及论元抽取，使用触发词进行事件对齐，对于触发词匹配的事件，计算论元F值。最终F值包括论元与触发词（<strong>将触发词当做一种论元计算得分</strong>）。论元F值为<strong>严格F值</strong>与<strong>松弛F值</strong>的平均得分。</p><p><strong>3.1.1 严格F值：预测论元与标注论元必须完全匹配（类型必须正确）</strong></p><p>论元准确率$P_{span}$ = 预测论元和标注论元匹配的个数 / 预测论元个数</p><p>论元召回率$R_{span}$ = 预测论元和标注论元匹配的个数 / 标注论元个数</p><p>论元F值 $F1_{span}$ =  $2P_{span}* R_{span} /( P_{span} + R_{span})$</p><p><strong>3.1.2 松弛F值：预测论元与标注论元存在字符级别匹配也能得到部分分数（类型必须正确）</strong></p><p>论元准确率$P_{char}$ = 预测论元和标注论元匹配的字符数 / 预测论元字符数</p><p>论元召回率$R_{char}$ = 预测论元和标注论元匹配的字符数 / 标注论元字符数</p><p>论元F值 $F1_{char}$ = $2 P_{char}*R_{char} /( P_{char} + R_{char})$</p><p>3.1.3 最终得分：$F1 = (F1_{span} + F1_{char}) / 2$</p><h5 id="3-2-事件属性抽取评价指标"><a href="#3-2-事件属性抽取评价指标" class="headerlink" title="3.2 事件属性抽取评价指标"></a>3.2 事件属性抽取评价指标</h5><p>对于事件属性抽取，使用F值进行评价。</p><p>属性准确率P = 预测属性和标注属性匹配的个数/ 预测属性个数</p><p>属性召回率R = 预测属性和标注属性匹配的个数 / 标注属性个数</p><p>属性F值 F1= 2 P R /( P+ R)</p><h4 id="4-评测及排行"><a href="#4-评测及排行" class="headerlink" title="4. 评测及排行"></a>4. 评测及排行</h4><ol><li>初赛和复赛均提供下载数据，选手在本地进行算法调试，在比赛页面提交结果。</li><li>每支团队每天最多提交3次。</li><li>排行按照得分从高到低排序，排行榜将选择团队的历史最优成绩进行排名。</li></ol><h3 id="四、作品提交要求"><a href="#四、作品提交要求" class="headerlink" title="四、作品提交要求"></a>四、作品提交要求</h3><ol><li><p>文件格式 ：按照 csv 格式提交</p></li><li><p>文件大小 ：无要求</p></li><li><p>提交次数限制 ：每支队伍每天最多 3 次</p></li><li><p>文件详细说明 ：</p><ol><li>对于该赛题<strong>复赛阶段</strong>的所有数据，统一采用json格式，编码格式为utf-8，结构及关键字解释如下：</li></ol><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmnbw7g7llj20ol0qlgpo.jpg"></p><p>2)提交格式见样例</p></li></ol><h3 id="五、赛程规则"><a href="#五、赛程规则" class="headerlink" title="五、赛程规则"></a>五、赛程规则</h3><h4 id="初赛-6月22日——8月21日"><a href="#初赛-6月22日——8月21日" class="headerlink" title="初赛 6月22日——8月21日"></a>初赛 6月22日——8月21日</h4><ol><li>初赛截止成绩以团队在初赛时间段内最优成绩为准（不含测试排名）。</li><li>初赛作品提交截止日期为８月20日17:00；初赛名次公布日期为8月21日10:00。</li></ol><h4 id="复赛-8月21日——9月21日"><a href="#复赛-8月21日——9月21日" class="headerlink" title="复赛 8月21日——9月21日"></a>复赛 8月21日——9月21日</h4><ol><li>排名前20%的团队晋级复赛，大赛官网将公示团队信息。选手通过大赛官网下载新增的训练集和开发集，本地调试算法，在线提交结果。</li><li>复赛成绩以参赛团队在复赛时间段内最优成绩为准。</li><li>复赛作品提交截止日期为９月20日17:00；复赛名次公布日期为９月21日10:00。</li></ol><h4 id="决赛-10月24日"><a href="#决赛-10月24日" class="headerlink" title="决赛 10月24日"></a>决赛 10月24日</h4><ol><li>前三名团队将受邀参加科大讯飞全球1024开发者节并于现场进行决赛。</li><li>决赛以答辩（10min陈述+5min问答）的形式进行。</li><li>根据复赛成绩和答辩成绩综合评分（复赛成绩占比70%，现场答辩分数占比30%）。</li></ol><h3 id="六、奖项设置"><a href="#六、奖项设置" class="headerlink" title="六、奖项设置"></a>六、奖项设置</h3><ul><li>入围复赛<ul><li>复赛入围证书</li><li>大赛专属Geek礼包</li><li>大赛限量文化衫</li></ul></li><li>入围决赛<ul><li>科大讯飞1024开发者节全场通票</li><li>决赛入围证书</li><li>科大讯飞创孵基地绿色入驻通道</li><li>A.I.服务市场入驻特权</li></ul></li><li>决赛胜出<ul><li>决赛奖金，各赛道TOP10选手将阶梯获得赛道奖金，第一名3万元、第二名2万元、第三名1万元、第四-第十名分别获得“算法菁英奖”2500元。</li><li>参与1024全球开发者节颁奖盛典，现场授予奖金、证书与定制奖杯</li><li>A.I.全链创业扶持</li><li>绿色就业通道&amp;讯飞Offer</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 事件抽取 </tag>
            
            <tag> 科大讯飞大赛-事件抽取挑战赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聊聊“事件抽取”</title>
      <link href="/2021/01/14/liao-liao-shi-jian-chou-qu/"/>
      <url>/2021/01/14/liao-liao-shi-jian-chou-qu/</url>
      
        <content type="html"><![CDATA[<p>作者：PaperWeekly<br>链接：<a href="https://zhuanlan.zhihu.com/p/27840591">https://zhuanlan.zhihu.com/p/27840591</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><blockquote><p><strong>「每周话题精选」</strong>是根据 PaperWeekly 最近一周的专题交流群讨论沉淀下来的精华内容。目前已成立的专题交流群有：知识图谱，量化，GAN，医疗 AI，CV，Chatbot 和 NVIDIA。</p></blockquote><p><strong>1. 本期问题清单</strong></p><ul><li>事件抽取的定义/概念是什么？哪些比赛/会议给出了定义？</li><li>有哪些常用的评测数据集和评测标准？</li><li>国内外有哪些研究团队和学者，它们主要研究的目标是什么？</li><li>事件抽取有哪些应用场景和实际的产品？</li><li>事件抽取的一般过程，有标注数据开展研究，如何扩展，没有数据怎么做？事件抽取有哪些经典的方法，效果如何？ </li><li>事件抽取与其他信息抽取任务（关系抽取、NER 等）有什么联系，难点在哪？ </li><li>事件之间的关系如何表示，如何做事件之间的关系抽取，目前有哪些研究？ </li><li>有哪些值得阅读的论文？有哪些开源了代码的工作？</li><li>最新的前沿进展有哪些？</li></ul><p><strong>2. 话题讨论精选</strong></p><p><strong>(1) 事件抽取的定义/概念是什么？哪些比赛/会议给出了定义？</strong></p><p><strong>A:</strong> 时间，地点，人物，故事情节。</p><p><strong>A:</strong> ACE 05 中对事件进行了明确的定义。</p><p><strong>A:</strong> 属性信息（Attribute），包括：类型（Type）、子类（Subtype）、模态（Modality）、倾向性（Polairty）、普遍性（Genericity）和时态（Tense）。</p><p><em><strong>Q: 不同任务对事件的定义不同吧，能具体解释下这些字段吗？</strong></em></p><p><strong>A:</strong> 属性是实体、数值和时间的集合。</p><p><strong>A:</strong> 我认为关系抽取一般来说是针对两个实体的，而事件抽取的话，不同事件类型会对应不同的元素元素（事件要素）。</p><p><strong>A:</strong> 一般来说是的，需要提前定义好事件的类型以及每种类型包含的属性。</p><p><strong>A:</strong> ACE05 中给出了类似的 schema，此处给出 ace05 对事件抽取的定义：</p><p><img src="https://pic3.zhimg.com/v2-a1494a14a3c47143dd10a5d435eb2bea_b.png"></p><p><em><strong>Q: 能简单介绍一些事件抽取的应用背景吗？</strong></em></p><p><strong>A:</strong> 比如一个事件里的被杀人数就是个数值，我记得最开始是用于反恐情报收集的。</p><p><strong>A:</strong> 之前看过有人写事件是一种特殊的关系，不知道是否正确。</p><p><strong>A:</strong> 新闻撰写机器人，比如百度知识图谱团队研发的写稿机器人，基于事件图谱自动生成一些大事件文章。</p><p><img src="https://pic1.zhimg.com/v2-82f1580619d4e185926027d1bdf0d8b8_b.png"></p><p><strong>Q:事件是要分类型的吧？</strong></p><p><strong>A:</strong> 看描述好像也有实体那种感觉。</p><p><strong>A:</strong> 事件类型要先定义出来。</p><p><strong>A:</strong> 有些研究是针对微博，将事件分为 4 元组：命名实体， 事件短句，日期，事件类型。</p><p><strong>A:</strong> 觉得定义事件跟抽取语义是一样的，此处放上一张分类ace05事件抽取分类图：</p><p><img src="https://pic4.zhimg.com/v2-6588ee8207b413ae0776cfef49897933_b.png"></p><p><strong>Q:事件抽取针对的是一段话还是一篇文章呢？</strong></p><p><strong>A:</strong> 针对一句话是 sentence-level 的，还有 document-level，cross-sentence level，cross-document level 的等等。</p><p><strong>(2) 有哪些常用的评测数据集和评测标准？</strong></p><p><strong>A:</strong> ACE2005</p><p><strong>(3) 国内外有哪些研究团队和学者，它们主要研究的目标是什么？</strong></p><p><strong>A:</strong> 国内好像苏州大学周国栋团队，哈工大刘挺，秦兵团队。</p><p><strong>A:</strong> 国外有韩家炜，纪恒团队。</p><p><strong>A:</strong> Heng 的相关文章推荐读。</p><p><strong>A:</strong> 国内企业有百度知识图谱团队。</p><p><strong>A:</strong> 国内外相关研究团队发表的论文：</p><p>A：自动化所的陈玉博，刘康~</p><p><img src="https://pic1.zhimg.com/v2-2a95b0010fe93e844be9774155798300_b.png"></p><p><img src="https://pic1.zhimg.com/v2-8fe466ebef8f6cc1af4e69c87b15bf1c_b.png"></p><p><img src="https://pic4.zhimg.com/v2-43fd2bce3c55851f568cf7519db2f783_b.png"></p><p><img src="https://pic3.zhimg.com/v2-80aed4daf070b089c4ef07055a68d2ce_b.png"></p><p><strong>(4) 事件抽取有哪些应用场景和实际的产品？</strong></p><p><strong>A:</strong> 股票，金融，QA，新闻趋势跟踪，舆情，事件型投资，并购。</p><p><strong>A:</strong> 反恐，反诈骗，政策性投资。</p><p><strong>A:</strong> 生物医学有类似药物不良反应的事件抽取。</p><p><strong>A:</strong> 通过对新闻热点事件的抽取，也许可以用来预测 IT 基础设施的故障，这个案例 NTT 做过，通过大量新闻事件的分析抽取预测了大规模网络故障。</p><p><em><strong>Q: 为什么通过新闻可以预测网络故障呢？</strong></em></p><p><strong>A:</strong> 如果突然有个突发事件，网络上也许会引发大规模的群体关注，相关网络的服务器也许突然大规模负载上升。</p><p><strong>A:</strong> 百度的知识图谱团队在事件图谱这块开展了不少前沿性的工作，并已经落地在了一些产品上；他们的目标是打造一个覆盖面最全时效性最快分析最全面精准的中文事件图谱。目前的产品形态比如事件脉络，明星事件追踪，明星历史热点等产品：</p><p><img src="https://pic4.zhimg.com/v2-e988e661dda53d29d3b9b8d309c55bd3_b.png"></p><p><img src="https://pic4.zhimg.com/v2-87b8c2bd9ae50671825c51212c8e3ae7_b.png"></p><p><img src="https://pic1.zhimg.com/v2-45d00adba1220d5662aba623e9c6b1a4_b.png"></p><p><strong>(5) 事件抽取的一般过程，有标注数据开展研究，如何扩展，没有数据怎么做？</strong></p><p><strong>A:</strong> 种子迭代，规则，模板。机器学习也可以用，比如论元的检测，就是构建一些特征，然后分类。</p><p><strong>A:</strong> 这个还是要做垂直领域，从规则和模板开始。</p><p><strong>A:</strong> 一些门户网站倒是可以通过访问量（检测波峰）的方法来看是不是发生了事件。</p><p><em><strong>Q: 事件抽取一般有什么方法呢？</strong></em></p><p><strong>A:</strong> 带监督的深度卷积网络肯定是一个。</p><p><strong>A:</strong> CNN 用的比较多。</p><p><strong>A:</strong> 估计从规则到机器学习都有，看具体的场景和数据。</p><p><strong>A:</strong> 经典方法就是：规则+模板，前沿方法：强化+模版（深度卷积）。</p><p><strong>A:</strong> 基于模板的抽取方法、半监督学习的模板抽取方法、经典机器学习方法、latent model 等等。</p><p><strong>(6) 深度学习在事件抽取上有哪些应用，与传统方法比有什么优势/劣势？</strong></p><p><strong>A:</strong> 性能好，不用人工构造特征。</p><p><strong>A:</strong> 触发词的识别和分类，CNN 模型要好。</p><p><strong>(7) 事件抽取与其他信息抽取任务（关系抽取、NER 等）有什么联系，难点在哪？</strong> </p><p><strong>A:</strong> 得先 NER。</p><p><strong>A:</strong> 时间是不是直接抽取就好了，其它属性该怎么办呢？</p><p><strong>A:</strong> 配模板的嘛，时间也是模板的一部分。</p><p><strong>Q:触发词一般是预定义好的，还是需要做检测任务？</strong></p><p><strong>A:</strong> 一般是定义好的，也有检测触发词的任务。</p><p><strong>(8) 事件之间的关系如何表示，如何做事件之间的关系抽取，目前有哪些研究？</strong> </p><p><strong>A:</strong> 我个人看法：事件也许应该是在时间轴上，有明确开始和结束的一段实体与实体产生关系的“运动”。</p><p><strong>A:</strong> 外国一般都是只做二元关系或者时序上的关系。</p><p><strong>A:</strong> 研究“事件”必须给他来个操作性定义。</p><p><strong>A:</strong> Semeval 2015 task4 是有定义的，但是产出产出太少。</p><p><strong>(9) 有哪些值得阅读的论文？有哪些开源了代码的工作？</strong></p><p><img src="https://pic2.zhimg.com/v2-ee395328ff92e5b75e06ccb68ca3b6a1_b.png"></p><p><strong>A:</strong> 基于符号特征的方法：</p><p><img src="https://pic1.zhimg.com/v2-f0a3f68319eeb9839335ea1aea2568c4_b.png"></p><p><strong>A:</strong> 基于表示学习的方法：</p><p><img src="https://pic1.zhimg.com/v2-f50f98f19ae90f6c7fedc4acfaf31580_b.png"></p><p><strong>(10) 最新的前沿进展有哪些？</strong> </p><p><strong>A:</strong> 我觉得事件之间的关系或网络会是将来的热点。</p><p><strong>A:</strong> 事件抽取必然会和监控视频结合。</p><p><strong>A:</strong> 和关系抽取在一起应用。检测事件的关系，舆情监测。其实对话系统也能用。</p><p><strong>A:</strong> 适合社交媒体，通过分析过往当事人发布的微信及 Facebook，可以做性格分析工作介绍、相亲配对。</p><p><strong>A:</strong> 延伸过去也可以做推荐系统，顾客销售行为预测。</p><p><strong>3. 相关资源</strong></p><p><strong>关于短句子事件短语抽取的论文：</strong></p><p><img src="https://pic3.zhimg.com/v2-e4d70351f6cade2819290b6cce93453a_b.png"></p><p><strong>ACE 数据：</strong><em><a href="https://link.zhihu.com/?target=https://github.com/oferbr/BIU-RPI-Event-Extraction-Project/tree/master/ACE_EVENT/corpus/orig">https://github.com/oferbr/BIU-RPI-Event-Extraction-Project/tree/master/ACE_EVENT/corpus/orig</a></em></p><p><strong>用 event embedding 做股票预测：</strong></p><p><img src="https://pic1.zhimg.com/v2-614a1abfeb5f36dfa8b9c1ec05902fbc_b.png"></p><p><strong>爬虫：</strong>该爬虫爬取了 36 kr（科技资讯网站） 的新闻快讯，以 json 的格式储存，适合用来做信息提取的测试样本或自动摘要的语料。</p><p><em><a href="https://link.zhihu.com/?target=https://github.com/HughWen/wen_spiders">https://github.com/HughWen/wen_spiders</a></em></p><p><strong>中文 NER 识别：</strong>作者希望大家可以贡献自己的力量一起维护一个开源的中文 NER 项目。</p><p><em><a href="https://link.zhihu.com/?target=https://github.com/zjy-ucas/ChineseNER">https://github.com/zjy-ucas/ChineseNER</a></em></p><p><strong>4. 参与讨论</strong></p><p>请添加群主微信：<strong>min279</strong>，备注<strong>「知识图谱」</strong>申请入群。</p><p><strong>关于PaperWeekly</strong></p><p>PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<strong>「交流群」</strong>，小助手将把你带入 PaperWeekly 的交流群里。</p><p><strong>微信公众号：PaperWeekly</strong></p><p><strong>新浪微博：@PaperWeekly</strong></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 事件抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transfer Learning for Small-scale Financial Event Extraction</title>
      <link href="/2021/01/13/transfer-learning-for-small-scale-financial-event-extraction/"/>
      <url>/2021/01/13/transfer-learning-for-small-scale-financial-event-extraction/</url>
      
        <content type="html"><![CDATA[<h2 id="Why？Background"><a href="#Why？Background" class="headerlink" title="Why？Background"></a>Why？Background</h2><p>事件抽取很重要，它的输出可以作为许多下游任务的输入</p><h2 id="What？"><a href="#What？" class="headerlink" title="What？"></a>What？</h2><p>a <strong>document-level event extraction paradigm</strong> which is composed of Event-classification and Event-Name-Entity-Recognition（Event-NER）</p><h2 id="How"><a href="#How" class="headerlink" title="How?"></a>How?</h2><p>model分为两个部分：</p><ol><li>Event-classification model</li><li>Event-NER model for each type of event</li></ol><blockquote><p>采用的是pipeline的方式，而不是joint learning。pipeline还是joint learning一直是NLP领域中热门讨论的两种训练方式，之前一直是joint learning比较火：</p><p>不是说pipeline方式存在误差积累吗，还会增加计算复杂度（实体冗余计算）吗？</p><p>不是说pipeline方式存在交互缺失，忽略实体和关系两个任务之间的内在联系吗？</p><p>陈丹琦用pipeline刷新了SOTA：<a href="https://zhuanlan.zhihu.com/p/274938894">反直觉！陈丹琦用pipeline方式刷新关系抽取SOTA</a></p></blockquote><h3 id="Event-classification-Model"><a href="#Event-classification-Model" class="headerlink" title="Event-classification Model"></a>Event-classification Model</h3><p>Event-classification Model使用的模型是pre-trained的BERT model，然后基于task进行fine-tune。</p><ul><li><p><strong>输入</strong>：[CLS] + text + [SEP]（就是常规的BERT的输入）</p></li><li><p><strong>输出</strong>：probability of each event type</p></li></ul><h3 id="Event-NER-Model"><a href="#Event-NER-Model" class="headerlink" title="Event-NER Model"></a>Event-NER Model</h3><p>每一种event都有自己的Event-NER Model，每一个Event-NER Model都是由一个pre-trained BERT和CRF构成。</p><ul><li><p><strong>pre-trained BERT的输入</strong>：The input is a sequence that has been classified as this type of event and pre-processed similarly as the Event-classification model.</p></li><li><p><strong>CRF的输入</strong>：承接BERT的输出，Instead of using the representation of the first token, Event-NER model feeds the hidden representations of all the word tokens into a following CRF layer that models the transition score of this type of Event-NER.For all the tokens, CRF layer outputs a score for each event-entity and uses Viterbi algorithm to compute the optimal labelling of the whole sequence.</p></li></ul><h3 id="Transfer-Learning-for-Event-NER"><a href="#Transfer-Learning-for-Event-NER" class="headerlink" title="Transfer Learning for Event-NER"></a>Transfer Learning for Event-NER</h3><p>正如前面所说的，每一种event都有自己的Event-NER Model，那么这样一来，输入每个Event-NER Model的数据集就会变小。为了解决这个问题，本解决方案决定采用<font color="red"><strong>Transfer Learning</strong></font>，采用的transfer learning的方式是fine-tune Model。首先利用所有的训练数据训练一个Base-NER Model，然后将这个Base-NER Model迁移到event-specific的event-NER Model。这样做的原因在于不同类型的金融事件文本数据具有可被利用的隐式结构。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmmf4opubfj20n10aljsh.jpg"></p><blockquote><p>上图需要注意，Event-BERT由Base-BERT初始化，但是Event-CRF并不是由Base-CRF初始化的，而是最原始的Init-CRF，这是因为different types of event have different paradigms, therefore, have different NER labels.</p></blockquote><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludqwh18j24462bcq4b.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludrbg7aj24462bc4p1.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludqw965j24462bcdhc.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludr63eij24462bc76j.jpg"><h3 id=""><a href="#" class="headerlink" title=""></a></h3><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludre5mbj24462bcwg5.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludre66dj21hc0u0ju2.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludrryk8j21mj0wxnc7.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludry0v2j24iz2jowrf.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmluds7t02j24iz2jowt1.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludsnupxj21z4140jw3.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmluds7gsqj24462bcjto.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtmjc5j24462bctag.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludsxui8j24462bc1it.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtkmtzj24462bch1o.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtbrsrj21hc0u0q5f.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtkgd5j21xn0ufq7f.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludvoxboj24iz2jox6p.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludus6k9j21hc0u0n1q.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmluduv4mej24462bcad1.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludua9uej24462bcjst.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludv5ziej22km1g3gur.jpg">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>【李宏毅-深度学习】24-Transfer Learning</title>
      <link href="/2021/01/13/li-hong-yi-shen-du-xue-xi-24-transfer-learning/"/>
      <url>/2021/01/13/li-hong-yi-shen-du-xue-xi-24-transfer-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><blockquote><p>迁移学习，主要介绍<font color="red"><strong>共享layer</strong></font>的方法以及<font color="red"><strong>属性降维对比</strong></font>的方法</p></blockquote><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>迁移学习（Transfer Learning）旨在利用<strong>一些不直接相关的数据</strong>对<strong>目标任务</strong>的完成做出贡献。</p><h3 id="not-directly-related"><a href="#not-directly-related" class="headerlink" title="not directly related"></a>not directly related</h3><p>以猫狗识别为例，解释“不直接相关”的含义：</p><ul><li><p>input domain是类似的，但task是无关的</p><p>比如输入都是动物的图像，但这些data是属于另一组有关大象和老虎识别的task</p></li><li><p>input domain是不同的，但task是一样的</p><p>比如task同样是做猫狗识别，但输入的是卡通类型的图像</p></li></ul><p><img src="https://camo.githubusercontent.com/7a7f75a03aa9597467ff1c3ca653a95ce33a270e599e1fb7f5f660357a30a15e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6e6f2d72656c617465642e706e67" alt="img"></p><h3 id="compare-with-real-life"><a href="#compare-with-real-life" class="headerlink" title="compare with real life"></a>compare with real life</h3><p>事实上，我们在日常生活中经常会使用迁移学习，比如我们会把漫画家的生活自动迁移类比到研究生的生活。</p><p><img src="https://camo.githubusercontent.com/4ec94257800195ec2b26bfdf2e315906f02900be6f105a42bffe58bd52bd38d9/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f74662d7265616c2e706e67" alt="img"></p><h3 id="overview——four-types-of-transfer-learning"><a href="#overview——four-types-of-transfer-learning" class="headerlink" title="overview——four types of transfer learning"></a>overview——four types of transfer learning</h3><p>迁移学习是很多方法的集合，这里介绍一些概念（注意：different terminology in different literature，术语很混乱）：</p><ul><li><strong>Target Data</strong>：和task直接相关的data；</li><li><strong>Source Data</strong>：和task没有直接关系的data；</li></ul><p>按照labelled data和unlabeled data又可以划分为四种：</p><p><img src="https://camo.githubusercontent.com/0a035ddc1420fcd95851a4068454520830b525237332f758679ba12dbc1f2810/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f746c2d6f766572766965772e706e67" alt="overview"></p><blockquote><p><em>not directly related to the task</em>的概念是模糊的。</p></blockquote><h2 id="Case-1"><a href="#Case-1" class="headerlink" title="Case 1"></a>Case 1</h2><p>这里target data和source data都是带有标签的：</p><ul><li><p>target data：$(x^t,y^t)$，作为有效数据，通常量是很少的</p><p>如果target data量非常少，则被称为<font color="red"><strong>one-shot learning</strong></font></p></li><li><p>source data：$(x^s, y^s)$，作为不直接相关数据，通常量是很多的</p></li></ul><blockquote><ul><li>one-shot learning aims to learn information about object categories from one, or only a few, training samples/images.</li><li>The key motivation for the one-shot learning technique is that systems, like humans, can use prior knowledge about object categories to classify new objects.</li></ul></blockquote><h3 id="Model-Fine-tuning"><a href="#Model-Fine-tuning" class="headerlink" title="Model Fine-tuning"></a>Model Fine-tuning</h3><h4 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1goya1a6c1ej20oj0fggvx.jpg"></p><blockquote><p>感觉one-shot learning和few-shot learning没啥大区别吧。</p></blockquote><p>模型微调的基本思想：<strong>用source data去训练一个model，再用target data对model进行微调（fine-tune）</strong></p><p>所谓“微调”，就是把用source data训练出的model参数当做是target data的model的参数初始值，再用target data继续训练下去即可，但当直接相关的数据量非常少时，<strong>这种方法很容易出现overfitting。</strong>所以训练的时候要小心，有许多技巧值得注意：</p><h4 id="Method-1-Conservative-Training"><a href="#Method-1-Conservative-Training" class="headerlink" title="Method 1: Conservative Training"></a>Method 1: Conservative Training</h4><blockquote><p>保守训练</p></blockquote><p>如果现在有大量的source data，比如在语音识别中有大量不同人的声音数据，可以拿它去训练一个语音识别的神经网络，而现在你拥有的target data，即特定某个人的语音数据，可能只有十几条左右，如果直接拿这些数据去再训练，肯定会出现overfitting。</p><p><img src="https://camo.githubusercontent.com/91f04393789459a23776a14421ca872e0cb43473f1ab94c885bdd1f6e75d8441/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f74662d63742e706e67" alt="img"></p><p>此时我们就需要在训练的时候加一些限制，让用target data训练前后的model不要相差太多：</p><ul><li><code>output close</code>: 我们可以让新旧两个model在看到同一笔data的时候，output越接近越好;</li><li><code>parameter close</code>: 让新旧两个model的L2 norm越小越好，参数尽可能接近;</li><li>总之让两个model不要相差太多，<strong>防止由于target data的训练导致过拟合</strong></li></ul><p>注：这里的限制就类似于regularization</p><h4 id="Method-2-Layer-Transfer"><a href="#Method-2-Layer-Transfer" class="headerlink" title="Method 2: Layer Transfer"></a>Method 2: Layer Transfer</h4><p>现在我们已经有一个用source data训练好的model，此时把该model的某几个layer拿出来复制到同样大小的新model里，接下来<font color="red"><strong>只</strong></font>用target data去训练得到余下的没有被复制到的layer。</p><p>这样做的好处是target data只需要考虑model中非常少的参数，这样就可以避免过拟合。</p><p><img src="https://camo.githubusercontent.com/9f8fddc418797f6ac686c82b9a2c0ede378e106468f3bddc504974ded585ce99/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f746c2d6c742e706e67"></p><p>这个对部分layer进行迁移的过程，就体现了迁移学习的思想，接下来要面对的问题是，哪些layer需要被复制迁移，哪些不需要呢？</p><p>值得注意的是，在不同的task上，需要被复制迁移的layer往往是不一样的：</p><ul><li><p>🎵在语音识别中，往往迁移的是最后几层layer，再重新训练与输入端相邻的那几层 </p><p>由于口腔结构不同，同样的发音方式得到的发音是不一样的，NN的前几层会从声音信号里提取出发音方式，再用后几层判断对应的词汇，从这个角度看，NN的后几层是跟特定的人没有关系的，因此可做迁移。</p></li><li><p>🗽在图像处理中，往往迁移的是前面几层layer，再重新训练后面的layer</p><p>CNN在前几层通常是做最简单的识别，比如识别是否有直线斜线、是否有简单的几何图形等，这些layer的功能是可以被迁移到其它task上通用的</p></li><li><p>主要还是具体问题具体分析</p></li></ul><p><img src="https://camo.githubusercontent.com/0931ebdb6a4049a12969e4146adf87c9dd97b0e13309fd14d528d20eaf309ca4/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f746c2d6c74322e706e67" alt="img"></p><p>下面的图，只看5和4。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1goyang3nz9j20on0hgqh9.jpg"></p><h3 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h3><h4 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h4><p><strong>fine-tune仅考虑在target data上的表现，而多任务学习，则是同时考虑model在source data和target data上的表现</strong></p><p>如果两个task的输入特征类似，则可以用同一个神经网络的前几层layer做相同的工作，到后几层再分方向到不同的task上（即Multitask的概念），这样做的好处是前几层吃的data比较多，可以被训练得更充分。</p><p>有时候task A和task B的输入输出都不相同，但中间可能会做一些类似的处理，则可以让两个神经网络共享中间的几层layer，也可以达到类似的效果。</p><p><img src="https://camo.githubusercontent.com/61f55b6a40d9d702f9c4eec3768d6bbc4dde2ea203f1978a636230b9ccb6dc3f/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6d756c74692d7461736b2e706e67" alt="img"></p><p><font color="red"><strong>注意，以上方法要求不同的task之间要有一定的共性，这样才有共用一部分layer的可能性</strong></font></p><h4 id="Multilingual-Speech-Recognition"><a href="#Multilingual-Speech-Recognition" class="headerlink" title="Multilingual Speech Recognition"></a>Multilingual Speech Recognition</h4><p>多任务学习在语音识别上比较有用，可以同时对法语、德语、西班牙语、意大利语训练一个model，它们在前几层layer上共享参数，而在后几层layer上拥有自己的参数</p><p>在机器翻译上也可以使用同样的思想，比如训练一个同时可以中翻英和中翻日的model</p><p><img src="https://camo.githubusercontent.com/fbd77f5001ae2e65db629d684d681b42b25e6365c2ea7939b54f689cbc8a916e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6d756c74692d7461736b2d7370656563682e706e67" alt="img"></p><p>注意到，属于同一个语系的语言翻译，比如欧洲国家的语言，几乎都是可以做迁移学习的；而语音方面则可迁移的范围更广。</p><p>下图展示了只用普通话的语音数据和加了欧洲话的语音数据之后得到的错误率对比，其中横轴为使用的普通话数据量，纵轴为错误率，可以看出使用了迁移学习后，只需要原先一半的普通话语音数据就可以得到几乎一样的准确率。</p><p><img src="https://camo.githubusercontent.com/faf1cce75b97fea24102ef3e5d63cca31d571083b666fb4b0df205bc62155475/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6d756c74692d7461736b2d737065656368322e706e67"></p><h4 id="Progressive-Neural-Network"><a href="#Progressive-Neural-Network" class="headerlink" title="Progressive Neural Network"></a>Progressive Neural Network</h4><p>如果两个task完全不相关，硬是把它们拿来一起训练反而会起到负面效果</p><p>而在Progressive Neural Network中，每个task对应model的hidden layer的输出都会被接到后续model的hidden layer的输入上，这样做的好处是：</p><ul><li>task 2的data并不会影响到task 1的model，因此task 1一定不会比原来更差</li><li>task 2虽然可以借用task 1的参数，但可以将之直接设为0，最糟的情况下就等于没有这些参数，也不会对本身的表现产生影响</li><li>task 3也做一样的事情，同时从task 1和task 2的hidden layer中得到信息</li></ul><p><img src="https://camo.githubusercontent.com/2bbca3bf9de41e1c28a271712bcb7ce0010d7d9da82a77241bbfa53592fa3e9a/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6d756c74692d7461736b2d70726f2e706e67"></p><h2 id="Case-2"><a href="#Case-2" class="headerlink" title="Case 2"></a>Case 2</h2><p>这里target data不带标签，而source data带标签：</p><ul><li>target data：$(x^t)$</li><li>source data：$(x^s, y^s)$</li></ul><h3 id="Domain-adversarial-Training"><a href="#Domain-adversarial-Training" class="headerlink" title="Domain-adversarial Training"></a>Domain-adversarial Training</h3><p>如果source data是有label的，而target data是没有label的，该怎么处理呢？</p><p>比如source data是labeled MNIST数字集，而target data则是加了颜色和背景的unlabeled数字集，虽然都是做数字识别，但两者的情况是非常不匹配的</p><p><a href="https://camo.githubusercontent.com/1b4c0ff356e94d71dbf4b68fe8052a37c24950b8171d12c38a8b9e6dc2d73c69/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c2e706e67"><img src="https://camo.githubusercontent.com/1b4c0ff356e94d71dbf4b68fe8052a37c24950b8171d12c38a8b9e6dc2d73c69/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c2e706e67" alt="img"></a></p><p>这个时候一般会把source data当做训练集，而target data当做测试集，如果不管训练集和测试集之间的差异，直接训练一个普通的model，得到的结果准确率会相当低</p><p>实际上，神经网络的前几层可以被看作是在抽取feature，后几层则是在做classification，如果把用MNIST训练好的model所提取出的feature做t-SNSE降维后的可视化，可以发现MNIST的数据特征明显分为紫色的十团，分别代表10个数字，而作为测试集的数据却是挤成一团的红色点，因此它们的特征提取方式根本不匹配</p><p><a href="https://camo.githubusercontent.com/85894b6bc91650a64d5a73998b7b6c7fb6b1f6ce71a37a2af2961e7579c6c6ba/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c322e706e67"><img src="https://camo.githubusercontent.com/85894b6bc91650a64d5a73998b7b6c7fb6b1f6ce71a37a2af2961e7579c6c6ba/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c322e706e67" alt="img"></a></p><p>所以我们希望前面的特征提取器(feature extractor)可以把domain的特性去除掉，不再使红点与蓝点分成两群，而是让它们都混在一起</p><p>这里采取的做法是，在特征提取器(feature extractor)之后接一个域分类器(domain classifier)，以便分类出这些提取到的feature是来自于MNIST的数据集还是来自于MNIST-M的数据集，这个生成+辨别的架构与GAN非常类似</p><p>只不过在这里，feature extractor可以通过把feature全部设为0，很轻易地骗过domain classifier，因此还需要给feature classifier增加任务的难度，它不只要骗过domain classifier，还要同时满足label predictor的需求</p><p><a href="https://camo.githubusercontent.com/0a844875fa66a4198a58e1a3efa2cffb294d99ee37a97716b62352f409054061/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c332e706e67"><img src="https://camo.githubusercontent.com/0a844875fa66a4198a58e1a3efa2cffb294d99ee37a97716b62352f409054061/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c332e706e67" alt="img"></a></p><p>此时通过特征提取器得到的feature不仅可以消除不同domain的特性，还要保留原先digit的特性，既可以区分不同类别的数字集，又可以正确地识别出不同的数字</p><p>通常神经网络的参数都是朝着最小化loss的目标共同前进的，但在这个神经网络里，三个组成部分的参数各怀鬼胎：</p><ul><li>对Label predictor，要把不同数字的分类准确率做的越高越好</li><li>对Domain classifier，要正确地区分某张image是属于哪个domain</li><li>对Feature extractor，要提高Label predictor的准确率，但要降低Domain classifier的准确率</li></ul><p>这里可以看出，Feature extractor和Domain classifier的目标是相反的，要做到这一点，只需要在两者之间加一层梯度反转的layer即可，当NN做backward的时候，两者的参数更新往相反的方向走</p><p><a href="https://camo.githubusercontent.com/a7920d4721561cf2e591c6481391aef1f05972647f334c31f4015463c7bef3f8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c342e706e67"><img src="https://camo.githubusercontent.com/a7920d4721561cf2e591c6481391aef1f05972647f334c31f4015463c7bef3f8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f646f6d61696e2d616476657273617269616c342e706e67" alt="img"></a></p><p>注意到，Domain classifier只能接受到Feature extractor给到的特征信息，而无法直接看到图像的样子，因此它最后一定会鉴别失败，所以如何提高Domain classifier的能力，让它经过一番奋力挣扎之后才牺牲是很重要的，如果它一直很弱，就无法把Feature extractor的潜能激发到极限</p><h3 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h3><p>同样是source data有label，target data没有label的情况，但在Zero-shot Learning中的定义更严格一些，它假设source和target是两个完全不同的task，数据完全不相关</p><p>在语音识别中，经常会遇到这个问题，毕竟词汇千千万万，总有一些词汇是训练时不曾遇到过的，它的处理方法是不要直接将识别的目标定成word，而是定成phoneme(音素)，再建立文字与phoneme之间的映射表即可</p><p><a href="https://camo.githubusercontent.com/2098d8739f84db875104f47e612cc69ff45e69dcb26b9496dda9e40f5a404969/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f7a65726f2d73686f742e706e67"><img src="https://camo.githubusercontent.com/2098d8739f84db875104f47e612cc69ff45e69dcb26b9496dda9e40f5a404969/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f7a65726f2d73686f742e706e67" alt="img"></a></p><p>在图像处理中，我们可以把每个类别都用其属性表示，并且要具备独一无二的属性，在数据集中把每种动物按照特性划分，比如是否毛茸茸、有几只脚等，在训练的时候我们不直接去判断类别，而是去判断该图像的属性，再根据这些属性去找到最契合的类别即可</p><p>有时候属性的维数也很大，以至于我们对属性要做embedding的降维映射，同样的，还要把训练集中的每张图像都通过某种转换投影到embedding space上的某个点，并且要保证属性投影的$g(y^i)$和对应图像投影的$f(x^i)$越接近越好，这里的$f(x^n)$和$g(y^n)$可以是两个神经网络</p><p>当遇到新的图像时，只需要将其投影到相同的空间，即可判断它与哪个属性对应的类别更接近</p><p><a href="https://camo.githubusercontent.com/8adb76cfe651477b87138c54c4a0d1e5a334ccbf352fa53b3ea826609d5bdc0f/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f7a65726f2d73686f74322e706e67"><img src="https://camo.githubusercontent.com/8adb76cfe651477b87138c54c4a0d1e5a334ccbf352fa53b3ea826609d5bdc0f/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f7a65726f2d73686f74322e706e67" alt="img"></a></p><p>但如果我们根本就无法找出每个动物的属性$y^i$是什么，那该怎么办？可以使用word vector，比如直接从维基百科上爬取图像对应的文字描述，再用word vector降维提取特征，映射到同样的空间即可</p><p>以下这个loss function存在些问题，它会让model把所有不同的x和y都投影到同一个点上： $$ f^*,g^*=\arg \min\limits_{f,g} \sum\limits_n ||f(x^n)-g(y^n)||<em>2 $$ 类似用t-SNE的思想，我们既要考虑同一对$x^n$和$y^n$距离要接近，又要考虑不属于同一对的$x^n$与$y^m$距离要拉大(这是前面的式子没有考虑到的)，于是有： $$ f^*,g^*=\arg \min\limits</em>{f,g} \sum\limits_n \max(0, k-f(x^n)\cdot g(y^n)+\max\limits_{m\ne n} f(x^n)\cdot g(y^m)) $$</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://sakura-gh.github.io/ML-notes/ML-notes-html/24_Transfer-Learning.html">https://sakura-gh.github.io/ML-notes/ML-notes-html/24_Transfer-Learning.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 李宏毅-深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> 迁移学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCKS 2020：面向金融领域的小样本跨类迁移事件抽取</title>
      <link href="/2021/01/12/ccks-2020-mian-xiang-jin-rong-ling-yu-de-xiao-yang-ben-kua-lei-qian-yi-shi-jian-chou-qu/"/>
      <url>/2021/01/12/ccks-2020-mian-xiang-jin-rong-ling-yu-de-xiao-yang-ben-kua-lei-qian-yi-shi-jian-chou-qu/</url>
      
        <content type="html"><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><blockquote><p>该数据集分为两大块：A榜数据和B榜数据</p></blockquote><p>本次数据来自真实金融新闻，由专业人员标注，训练集、验证集及测试集的说明如下（数据规模以实际下载数据为准）：</p><h3 id="ccks-3-nolabel-data"><a href="#ccks-3-nolabel-data" class="headerlink" title="ccks_3_nolabel_data"></a>ccks_3_nolabel_data</h3><blockquote><p>A榜数据集</p><p>存储地址：G:\SelfLearning\JupyterNotebook3.0\Conferences\CCKS\2020\task4\数据集\ccks_3_nolabel_data</p></blockquote><p>当前的数据总共包含以下几份文件：</p><h4 id="train-base-json"><a href="#train-base-json" class="headerlink" title="train_base.json"></a>train_base.json</h4><blockquote><ul><li>train代表训练集</li><li>base的中文是“基本”，代表的就是原始类别。</li></ul></blockquote><p><code>train_base.json</code>是原始训练集，包含<strong>质押</strong>、<strong>股份股权转让</strong>、<strong>投资</strong>、<strong>起诉</strong>和<strong>减持</strong>五个类别；</p><p>一共包含<strong>2732条annotated数据</strong>，一个示例为：</p><pre><code class="json">{    "id": "cbc5a8ed0bc6dcebd15cbe5d2b6b8311",     "content": "兴发集团发布公告,控股股东宜昌兴发集团有限责任公司于2019年11月20日将2000万股进行质押,质押方为上海浦东发展银行股份有限公司宜昌分行,质押股数占其所持股份比例的8.50%,占公司总股本的2.15%。",     "events":      [         {"type": "质押",           "mentions":                     [                        {"word": "股", "span": [43, 44], "role": "collateral"},                         {"word": "8.50%", "span": [85, 90], "role": "proportion"},                         {"word": "上海浦东发展银行股份有限公司宜昌分行", "span": [53, 71], "role": "obj-org"},                         {"word": "质押", "span": [46, 48], "role": "trigger"},                         {"word": "2000万", "span": [38, 43], "role": "number"},                         {"word": "2019年11月20日", "span": [26, 37], "role": "date"},                         {"word": "宜昌兴发集团有限责任公司", "span": [13, 25], "role": "sub-org"}                    ]         }     ]}</code></pre><p>关于JSON文件的语义理解，可以将一个<code>{}</code>看作是一个对象，<code>{}</code>里面的每一个<code>key:value</code>对看作是该对象的属性；而一个<code>[]</code>看作是一个数组，里面包含了很多的<code>{}</code>对象。这样就很好理解啦~</p><h4 id="dev-base-json"><a href="#dev-base-json" class="headerlink" title="dev_base.json"></a>dev_base.json</h4><p><code>dev_base.json</code>是原始验证集；</p><p>一共包含<strong>163763条unannotated数据</strong>，一个示例为：</p><pre><code class="json">{    "id": "d45bb6fb70598b9a472c14d28ad12708",      "content": "上述股权已于2012年9月25日办理了股权质押登记手续,股权质押期限自股权质押登记之日起至质权人办理解除质押登记为止。"}</code></pre><h4 id="trans-train-json"><a href="#trans-train-json" class="headerlink" title="trans_train.json"></a>trans_train.json</h4><p><code>trans_train.json</code>是A榜迁移训练集，包含<strong>收购</strong>和<strong>判决</strong>这两个类别；</p><p>一共包含<strong>361条annotated数据</strong>，一个示例为：</p><pre><code class="json">{    "id": "ecb7d40130299305dd53bc9096449919",     "content": "所以,鉴于三人特殊的亲戚关系,业内认为,金马股份收购众泰汽车的做法,更像是一场自导自演、自我抬高身价的家族游戏。",     "events":      [         {"type": "收购",           "mentions":                     [                        {"word": "金马股份", "span": [20, 24], "role": "sub-org"},                         {"word": "收购", "span": [24, 26], "role": "trigger"},                         {"word": "众泰汽车", "span": [26, 30], "role": "obj-org"}                    ]         }     ]}</code></pre><h4 id="trans-dev-json"><a href="#trans-dev-json" class="headerlink" title="trans_dev.json"></a>trans_dev.json</h4><p><code>trans_dev.json</code>是A榜迁移验证集。</p><p>一共包含<strong>60731条unannotate数据</strong>，一个示例为：</p><pre><code class="json">{    "id": "37ae36de04ec5f061ccbbc90fcdc1321",     "content": "智通财经APP讯,百信国际(00574)公布,于2019年3月8日,该公司直接全资附属ReadyGainLimited拟向余健伟及朱显明收购百胜百惠顾问有限公司的全部股权,代价为4532.5万港元。"}</code></pre><p>注意：<br>（1）选手需要根据<code>train_base.json</code>和<code>trans_train.json</code>做模型训练，然后在<code>dev_base.json</code>和<code>trans_dev.json</code>做模型推断。<br>其中：<code>dev_base.json</code>的类别为质押、股份股权转让、投资、起诉和减持，<code>trans_dev.json</code>的类别为收购和判决。选手将两份预测结果合并，提交到平台即可。<br>（2）担保、中标和签署合同这几个迁移事件的少量训练样本，会在最终评测的时候才放出。</p><h3 id="Final-Release-CCKS-2020-3"><a href="#Final-Release-CCKS-2020-3" class="headerlink" title="Final Release-CCKS 2020_3"></a>Final Release-CCKS 2020_3</h3><blockquote><p>B榜数据集</p><p>存储地址：G:\SelfLearning\JupyterNotebook3.0\Conferences\CCKS\2020\task4\数据集\Final Release-CCKS 2020_3</p></blockquote><h4 id="trans-train-json-1"><a href="#trans-train-json-1" class="headerlink" title="trans_train.json"></a>trans_train.json</h4><p><code>trans_train.json</code>，是在A榜的迁移训练集<code>trans_train.json</code>基础上，新增了担保、中标和签署合同三个类别的少量含标数据，加上A榜的判决和收购，总共含五个类别；其包含了A榜迁移训练集<code>trans_train.json</code>中的所有含标数据。</p><p>一共<strong>820条数据</strong>，一个示例为：</p><pre><code class="json">{    "id": "4ae4ae60de1bf10833f4dfc40cba96c8",     "content": "龙马环卫:中标8.85亿元环卫一体化项目龙马环卫(603686)7月25日晚间公告,公司中标三亚市崖州区环卫一体化项目,服务期限9年,合同年化金额9829.91万元/年,合同总金额8.85亿元。",     "events":     [        {"type": "中标",          "mentions":          [             {"word": "龙马环卫", "span": [20, 24], "role": "sub"},              {"word": "7月25日", "span": [32, 37], "role": "date"},              {"word": "中标", "span": [44, 46], "role": "trigger"},              {"word": "三亚市崖州区", "span": [46, 52], "role": "obj"},              {"word": "8.85亿元", "span": [90, 96], "role": "amount"}         ]        }    ]}</code></pre><h4 id="trans-test-json"><a href="#trans-test-json" class="headerlink" title="trans_test.json"></a>trans_test.json</h4><p><code>trans_test.json</code>是B榜测试集，与任何dev集都不相同。</p><p>一共<strong>32879条数据</strong>，一个示例如下：</p><pre><code class="json">{    "id": "7932deb94b830a80dd103c283872e8bc",     "content": "截至2019年7月底,中国证券投资基金业协会已登记私募基金管理人24322家,存续备案私募基金78734只,其中证券类私募基金管理人8776家,占比36.08%;"}</code></pre><p>注意：<br>（1）选手需要根据A榜的<code>train_base.json</code>和B榜的<code>trans_train.json</code>做模型训练，然后根据B榜的<code>trans_test.json</code>做模型推断。B榜的<code>trans_test.json</code>需要预测的类别为判决、收购、担保、中标和签署合同。<br>（2）最终排名以B榜的得分为主。仅在B榜得分小数点后三位一样时，再看A榜得分确定排名。</p><h3 id="数据解决方案"><a href="#数据解决方案" class="headerlink" title="数据解决方案"></a>数据解决方案</h3><p>原始类别包括：<strong>质押</strong>、<strong>股份股权转让</strong>、<strong>投资</strong>、<strong>起诉</strong>、<strong>减持</strong>，已有标注数量为2732，按照8-2原则，训练集拥有数据2185条，测试集拥有数据547条。</p><p>迁移类别包括：<strong>收购</strong>和<strong>判决</strong>（其他三个类别感觉数据量太小了，不好使用），已有标注数量为361，按照8-2原则，训练集拥有数据288条，测试集拥有数据73条。</p><h2 id="第一名方案"><a href="#第一名方案" class="headerlink" title="第一名方案"></a>第一名方案</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml8q739asj20te0gswml.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml8tcwdjfj20tx0ggqai.jpg" alt="image.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml8tzonjaj20tt0grth0.jpg" alt="image.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml8vre3k1j20sn0eqwl3.jpg" alt="image.png"></p><blockquote><p>什么是多标签多类型文本分类问题</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml8zaw5p9j20u80g8jyt.jpg" alt="image.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml90t97j7j20sw0evdnk.jpg" alt="image.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml92zkoicj20u90gjdoh.jpg" alt="image.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml93j7ffuj20u60ggn71.jpg" alt="image.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml94tf74gj20tz0gptci.jpg" alt="image.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gml965qywdj20tl0gkgry.jpg" alt="image.png"></p><p>消融实验</p><h2 id="第三名方案"><a href="#第三名方案" class="headerlink" title="第三名方案"></a>第三名方案</h2><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludqtkogj24462bct9i.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludqwh18j24462bcq4b.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludrbg7aj24462bc4p1.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludqw965j24462bcdhc.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludr63eij24462bc76j.jpg"><h3 id=""><a href="#" class="headerlink" title=""></a></h3><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludre5mbj24462bcwg5.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludre66dj21hc0u0ju2.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludrryk8j21mj0wxnc7.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludry0v2j24iz2jowrf.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmluds7t02j24iz2jowt1.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludsnupxj21z4140jw3.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmluds7gsqj24462bcjto.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtmjc5j24462bctag.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludsxui8j24462bc1it.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtkmtzj24462bch1o.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtbrsrj21hc0u0q5f.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludtkgd5j21xn0ufq7f.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludvoxboj24iz2jox6p.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludus6k9j21hc0u0n1q.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmluduv4mej24462bcad1.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludua9uej24462bcjst.jpg"><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gmludv5ziej22km1g3gur.jpg">]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 毕设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【李宏毅-人类语言处理】17-一文看尽各种NLP任务</title>
      <link href="/2021/01/04/li-hong-yi-ren-lei-yu-yan-chu-li-17-yi-wen-kan-jin-ge-chong-nlp-ren-wu/"/>
      <url>/2021/01/04/li-hong-yi-ren-lei-yu-yan-chu-li-17-yi-wen-kan-jin-ge-chong-nlp-ren-wu/</url>
      
        <content type="html"><![CDATA[<h2 id="NLP的两大任务"><a href="#NLP的两大任务" class="headerlink" title="NLP的两大任务"></a>NLP的两大任务</h2><p>前言：之前我们讲了很多与语音处理有关的任务，这次我们来讲和自然语言处理相关的任务。NLP任务大体可以分成两大类:</p><ol><li><strong>文本序列到文本序列</strong>，比如机器翻译，文本风格迁移等;</li><li><strong>序列到类别</strong>，比如情感分类，实体命名识别，主题分类，槽位填充（Slot Filling）等。</li></ol><p><img src="https://pic2.zhimg.com/80/v2-d297ef6baa97204d6fdd769ff16a76dd_720w.jpg"></p><p>NLP 这个词的用法有点模糊。Language 指的一般是人与人沟通时用的语言。因此 Natural Language 可以是文字也可以是语音。因此语音相关的技术，也应该算是 Natural Language 的范畴。但不知道为什么，一般我们在将自然语言处理的时候，指的都是文字处理相关的技术。而语音生成，语音分类和语音的风格转换，却被分成语音处理了。自然语言处理的应用非常广泛。尽管其变化多端，但无非是以下几种任务的变体。</p><p><img src="https://pic3.zhimg.com/80/v2-7f65a03d388de349fde1c1923d723b42_720w.jpg" alt="输入文字输出类别"></p><p>文本到类别可以分成两种：</p><ul><li>一种是输入序列输出代表这整个序列的类别，如情感分类。</li><li>另一种是输入序列输出序列上每个位置的类别，如实体命名识别。</li></ul><p>它们都可以接一个 LSTM Model来获取序列的上下文标注，后面再接一个 MLP 做线性映射，再用 sigmoid 或 softmax 来输出类别。</p><blockquote><p>目前流行的方式都是使用BERT来作为Model，而不是LSTM。</p></blockquote><p><img src="https://pic3.zhimg.com/80/v2-71ab4a531d7cda973ec6f5a24f9c250e_720w.jpg" alt="输入文本输出文本"></p><p>文本到文本则倾向于使用 <strong>Seq2Seq  Model</strong>，Encoder-Decoder 架构，中间需要用 attention 来对齐。</p><p>对于有些输入文字和输出文字内容有很多重复的任务，比如文本摘要，拼写检查，语法纠错，标题改写这类任务，我们还需要在中间加入复制的能力。比如 pointNet 和 CopyNet。</p><p><img src="https://pic2.zhimg.com/80/v2-b0b7ebec5c9da594ba2863be9e6df3f1_720w.jpg"></p><p><strong>如果输入的是多个序列要怎么办呢？</strong>我们有大概两种可能的解法：</p><ul><li>一种是把两个序列，分别用两个模型去做编码。再把它们编码后的嵌入，丢给另一个整合的模块，去得到最终的输出。有时，我们也会在两个模型之间加 Attention，确保二者的编码内容能互相意识。</li><li>近年来比较流行的做法是直接把两个句子连接起来，中间加一个特殊的字符，如 BERT 里面的 <sep>，来提示模型去意识到这是两个句子的分隔符。接起来的序列丢给模型后，就可以直接预测下游任务。</sep></li></ul><h2 id="一表览尽NLP的所有子任务⭐⭐⭐⭐⭐"><a href="#一表览尽NLP的所有子任务⭐⭐⭐⭐⭐" class="headerlink" title="一表览尽NLP的所有子任务⭐⭐⭐⭐⭐"></a>一表览尽NLP的所有子任务⭐⭐⭐⭐⭐</h2><p><img src="https://pic3.zhimg.com/80/v2-741192d080c451563bc4b7fb3133daba_720w.jpg"></p><p>虽然 NLP 的任务千变万化，但根据模型的输入输出可以分成几个大类。模型的输入可以分成一个序列和多个序列，模型的输出可以分成整个序列一个类别，每个位置都有类别，是否需要复制输入，还是要输出另一端文本。除了这些以外，还有一些例外，比如 Parsing 和 Coreference Resolution。</p><h2 id="Part-of-Speech（POS）-Tagging"><a href="#Part-of-Speech（POS）-Tagging" class="headerlink" title="Part-of-Speech（POS） Tagging"></a>Part-of-Speech（POS） Tagging</h2><blockquote><p>part  n. 部分；角色；零件；一些；片段<br>speech n.语言，说话<br>part-of-speech：说话中词的角色=&gt; 词的类别，简称词性。</p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-af870ca76cb57c7b244bb49e9010875f_720w.jpg"></p><p>Part-of-Speech (POS) Tagging 词性标注 需要我们标记出一个句子中的每个词的词性是什么。对应输入一个序列，输出序列每个位置的类别任务。</p><p><strong>output的标记类别</strong>可以用作下游任务（比如文本摘要，机器翻译）的input。</p><h2 id="Word-Segmentation"><a href="#Word-Segmentation" class="headerlink" title="Word Segmentation"></a>Word Segmentation</h2><p><img src="https://pic1.zhimg.com/v2-6eeff8ba4a42d37983097a970f987ed4_b.jpg"></p><p>中文分词。英文词汇有空格符分割词的边界，但中文中，却没有类似的方式来区分，所以我们需要中文分词。在一个句子中找出词的边界有时并不是一个简单的问题，所以也需要模型来做。一般中文分词是对句子中的每个字的位置做二分类。如果标注为Y，表示这个字是词汇的结尾边界，如果标注为 N，表示这个字不是词汇的结尾。到了下游任务，我们就可以利用分好的词来作为模型输入的基本单位，而不一定用字。但是否有必要用词表征来替代字表征还是一个值得探究的问题。因为 BERT 在处理中文的时候，它已经不是以字为基本单位了。它很有可能已经自动学到了分词这件事。因此输入要不要用词表征倒显得无关紧要。但 <strong>BERT-wwm（它相比于Bert的改进是用Mask标签替换一个完整的词而不是子词）</strong>的实验表明，预训练过程中，让 BERT 要预测的随机 MASK 掉的是一个分词的 span 而不是单独的字能表现更好。说明知道词汇的边界在哪里，对语义的理解是非常重要的。</p><h2 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h2><blockquote><ul><li>Parsing这个词，可以翻译为句法分析，也可以翻译为语法分析。</li><li>Parsing这个任务的输入和输出不符合我们之前表格的分类，以后会当做一个特殊的专题讨论。</li></ul></blockquote><p><img src="https://pic1.zhimg.com/v2-7113416d11b80448298f080faf6fb860_b.jpg"></p><p>还有一些也会用来作为自然语言理解的前处理的任务，比如说 Parsing。它的输入是一个句子，输出是一棵句法树。它的输出有时会被当作是额外的特征，在接下来的任务中被使用到。</p><p><img src="https://pic1.zhimg.com/v2-255f7f71b7d3ef27bda006efc3126608_b.jpg"></p><p>另一个也常来做前处理的任务叫指代消解 Coreference Resolution。模型需要把输入文章中指代同样东西的部分，找出来。比如文中，He 和 Paul Allen 指的就是同一个人。</p><h2 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h2><blockquote><p>Extractive Summarization &amp; Abstractive Summarization</p></blockquote><h3 id="Extractive-Summarization"><a href="#Extractive-Summarization" class="headerlink" title="Extractive Summarization"></a>Extractive Summarization</h3><p><img src="https://pic2.zhimg.com/v2-3e1cde876f537a4088d94a5091b177b9_b.jpg"></p><p>接下来是摘要，它可以分成两种。过去常用的是<strong>抽取式摘要</strong>。把一篇文档看作是许多句子的组成的序列，模型需要从中找出最能熔炼文章大意的句子提取出来作为输出。它相当于是对每个句子做一个二分类，来决定它要不要放入摘要中。但仅仅把每个句子分开来考虑是不够的。我们需要模型输入整篇文章后，再决定哪个句子更重要。这个序列的基本单位（Token）是一个句子的表征。</p><h3 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h3><p><img src="https://pic1.zhimg.com/v2-f3e2a113256be7e7f449913635e5b338_b.jpg"></p><p>随着深度学习的近年流行，人们开始关注<strong>生成式摘要</strong>了。模型的输入是一段长文本，输出是短文本。输出的短文本往往会与输出的长文本有很多共用的词汇。这就需要模型在生成的过程中有把文章中重要词汇拷贝出来，放到输出中的复制的能力，比如 Pointer Network。</p><p><img src="https://pic2.zhimg.com/v2-f682ab58ec0a372a8fa18ae15aaf918d_b.jpg"></p><p>输入文字或语音可以直接输出文字。为什么我们要做输入语音输出翻译文字的翻译模型呢？因为很多语言比如说当地的一些方言，连文字都没有。我们只能做语音到文字的对应。如果我们输入输出的语言都没有文字，我们有机会做语音到语音的翻译。对于机器翻译领域来说，一个很关键的问题就是无监督学习。因为世界上的语言有非常多，大约7000种语言。两两匹配大约有 7000² 对组合。我们现实中很难收集到所有的两两语言组合，这是不切实际的。因此无监督学习是很有必要的。模型通过看了一大段英文句子，也看了一大段中文句子，但没有给出中文和英文的对应关系，却能够自动学会把英文转换为中文，把中文转换为英文。（后面的课程会讲解如何实现这样的无监督机器翻译）</p><h2 id="Grammar-Error-Correction"><a href="#Grammar-Error-Correction" class="headerlink" title="Grammar Error Correction"></a>Grammar Error Correction</h2><p><img src="https://pic2.zhimg.com/v2-2ae632a611491121eec47eb4a708ad6d_b.jpg"></p><p>语法改错任务，也是文本序列到文本序列，理所当然会想到使用Seq2Seq Model。考虑到输入和输出有很多字是重复的，我们考虑用一些复制机制，让模型把没有错的词汇保留下来。</p><p>这种语法改错任务其实还可以更进一步简化，不使用Seq2Seq Model。输入是一个序列，输出是该序列上每个位置的类别标注。如上图中右半部分所示，标注类型包括是要<strong>保留复制</strong>，还是<strong>置换</strong>，还是<strong>插入</strong>。</p><blockquote><p>C  stands for 保留<br>R stands for  置换<br>A stands for 插入</p></blockquote><h2 id="Sentiment-Classification"><a href="#Sentiment-Classification" class="headerlink" title="Sentiment Classification"></a>Sentiment Classification</h2><p><img src="https://pic3.zhimg.com/v2-6ec116e364c9fff9964a596e3aa88eb2_b.jpg"></p><p>序列到类别则包括商品评论情感分类。句子中可以同时包括正面的词汇和负面的词汇，<strong>模型需要根据上下文学到语境中更侧重正面还是负面</strong>。比如虽然……但是这种转折关系，”但是”后面的词汇会得到更多的侧重。</p><h2 id="Stance-Detection"><a href="#Stance-Detection" class="headerlink" title="Stance Detection"></a>Stance Detection</h2><blockquote><p>stance detection，中文翻译为立场检测。</p></blockquote><p><img src="https://pic1.zhimg.com/v2-a0c7a21dd17e6763f86897439844b640_b.jpg"></p><p>立场检测任务也是分类。它的输入是两个序列，输出是一个类别，表示后面的序列是否与前面的序列站在同一立场。常用的立场检测包括 SDQC 四种标签，<font color="red">**支持 (Support)，否定 (Denying)，怀疑 (Querying)，Commenting (注释)**。</font></p><h2 id="Veracity-Prediction"><a href="#Veracity-Prediction" class="headerlink" title="Veracity Prediction"></a>Veracity Prediction</h2><blockquote><ul><li>veracity /vəˈræsəti/ n. 真实，准确</li></ul></blockquote><p><img src="https://pic1.zhimg.com/v2-6fe31d70c433025fcdd49a09dfbabd78_b.jpg"></p><p><strong>事实验证</strong>也是文本分类的一种。模型需要看一篇新闻文章，判断该文章内容是真的还是假的。假新闻检测是典型的。有时从文章本身，我们人自己都很难判断它的真假。因此有时我们还需要把文章的回复评论也加入模型的输入，去预测真假。<strong>如果一个文章它回应第一时间都是否认，往往这个新闻都是假新闻。</strong>我们还可以让模型看与文章有关的维基百科的内容，来增强它的事实审核能力。</p><h2 id="Natural-Language-Inference（NLI）"><a href="#Natural-Language-Inference（NLI）" class="headerlink" title="Natural Language Inference（NLI）"></a>Natural Language Inference（NLI）</h2><p><img src="https://pic1.zhimg.com/v2-ed8c8e48e07ce56d55e1bdb12ab653c4_b.jpg"></p><blockquote><ul><li>premise：前提</li><li>hypothesis：假设</li></ul></blockquote><p>还有一类任务叫自然语言推断 (NLI)。输入给模型的是一个陈述前提，和一个假设，输出是能否通过前提推出假设，它包含三个类别，分别是矛盾，蕴含和中性。比如前提是，一个人骑在马上跳过一架破旧的飞机，假设是这个人正在吃午餐。这显然是矛盾的。因为前提推不出假设。如果假设是，这个人在户外，在一匹马上。则可以推理出蕴含。再如果假设是这个人正在一个比赛中训练他的马。则推理不能确定，所以是中性的。</p><h2 id="Search-Engine"><a href="#Search-Engine" class="headerlink" title="Search Engine"></a>Search Engine</h2><p><img src="https://pic1.zhimg.com/v2-2c8dea70de003b30705ea94db92b44b4_b.jpg"></p><p>再往下的任务是搜索引擎。模型的输入是一个关键词或一个问句和一堆文章，输出是每篇文章与该问句的相关性。<font color="red"><strong>谷歌有把 BERT 用在搜素引擎上在语义理解上得到了提升。</strong></font>比如搜帮你做美容的人是否经常站着工作。没有 BERT 之前，模型会利用关键词 estheticians 和 stand-alone 做合并结果输出。但有了 BERT 之后，搜出的结果会更倾向于文章语义的理解而非单纯的关键字匹配。</p><hr><p>总结：李宏毅这节课汇总了大部分的典型 NLP 任务，下节课会讲<strong>问答系统</strong>，<strong>对话系统</strong>，<strong>信息抽取</strong>和<strong>文本生成</strong>等任务。当然还有一些其它任务，比如常识推理，文本主题模型聚类，时间处理等任务没有讲到。但不得不说，深入浅出，覆盖很全面。一遍看下来，对初学者而言可以构建知识体系，对进阶者而言可以查缺补漏。最后赠送一张 NLP 双生树思维导图作为福利。这两个月增粉很多，感谢大家一直以来的关注。</p><hr><h2 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h2><p>前言：接上一期继续讲剩下的<strong>问答系统</strong>，<strong>对话系统</strong>，<strong>信息抽取</strong>和<strong>文本生成</strong>等 NLP 任务。</p><p><img src="http://pic1.zhimg.com/80/v2-b79f0a5da0211ea86fa1f22a8bb4014c_720w.jpg"></p><p>问答系统一般都是基于检索式的，会有如下几个模块：</p><ul><li><strong>Question Processing</strong>：问题预处理（Query 规范化表示，答案类型）</li><li><strong>Document and Passage Retrieval</strong>：用 Query 去召回一波文档，根据一系列的特征去做排序，找出相关性较强的段落</li><li><strong>Answer Extraction</strong>：对候选答案评分再排序、用答案类型去约束提取出的候选答案，返回最终结果</li></ul><p><img src="https://pic2.zhimg.com/80/v2-87191ddb588cc3be87f2e080ea3799ed_720w.jpg"></p><p><code>Watson</code> 之所以强大是因为它把每个模块部分做的非常的精细。光问题处理的答案类型它就有几千个，使用了很多特征。在候选答案生成中，它也利用了海量的文档资源。输出的候选答案之后，它还会根据证据做进一步的再排序，选出置信度最高的那个答案。</p><p><img src="https://pic3.zhimg.com/80/v2-522391ec25933e822f562496eb67feae_720w.jpg"></p><h3 id="Extractive-QA"><a href="#Extractive-QA" class="headerlink" title="Extractive QA"></a>Extractive QA</h3><p>QA问题的输入是一连串句子，输出是答案序列。搜索引擎只是做相关性检索，但如果想要理解文档篇章，还是需要机器阅读理解。</p><p><img src="https://pic1.zhimg.com/80/v2-77cf85176ff56d57a2e7aaaa9ffc0ae0_720w.jpg"></p><p>当前主流的研究，其实并没有让模型吐出完整的答案。通常我们做的是抽取式的QA，即给定一段文章和问题，模型需要输出答案在文章段落中的位置。它是强制复制原文中的内容，而不是自主生成能基于文章回答问题，但不在文章中的答案。</p><h2 id="Chatting"><a href="#Chatting" class="headerlink" title="Chatting"></a>Chatting</h2><p><img src="https://pic3.zhimg.com/80/v2-941de32dbc0176cb9319a1e3437bbd4a_720w.jpg"></p><p>对话机器人可以分成两种，<strong>闲聊</strong>（Chatting）和<strong>任务导向型</strong>（Task-oriented）。闲聊机器人基本上都是在尬聊，有一堆问题待解决，比如角色一致性，多轮会话，对上下文保有记忆等。</p><h3 id="Chatbot"><a href="#Chatbot" class="headerlink" title="Chatbot"></a>Chatbot</h3><p><img src="https://pic4.zhimg.com/80/v2-e1bbc1ad7e3c9dad04d09e95057fd913_720w.jpg"></p><p>当前的闲聊机器人需要有一致的人格，懂常识和领域知识，有同理心。</p><h3 id="Task-Oriented"><a href="#Task-Oriented" class="headerlink" title="Task-Oriented"></a>Task-Oriented</h3><p><img src="https://pic1.zhimg.com/80/v2-3ea3e3a34c20182a5f701d20600e2090_720w.jpg"></p><p>任务导向的对话机器人能够协助人完成某件事，比如订机票，调闹钟，问天气等。<font color="red"><strong>我们需要一个模型把过去已经有的历史对话，统统都输入到一个模型中，这个模型可以输出一个序列当作现在机器的回复。（结合上下文来聊天~）</strong></font>一般而言，我们会把这个模型再细分成很多模块，而不会是端对端的。</p><h4 id="Natural-Language-Generation（NLG）"><a href="#Natural-Language-Generation（NLG）" class="headerlink" title="Natural Language Generation（NLG）"></a>Natural Language Generation（NLG）</h4><p><img src="http://pic1.zhimg.com/80/v2-84bc1debadd29daea087a2114e8364c0_720w.jpg"></p><p>这些模块通常包括<strong>自然语言理解NLU</strong>，<strong>行动策略管理</strong>，以及<strong>自然语言生成NLG</strong>。自然语言理解负责根据上下文去理解当前用户的意图，方便选出下一步候选的行为，如执行系统操作，澄清还是补全信息，确定好行动之后，自然语言生成模块会生成出对齐行动的回复。</p><h4 id="Policy-amp-State-Tracker"><a href="#Policy-amp-State-Tracker" class="headerlink" title="Policy &amp; State Tracker"></a>Policy &amp; State Tracker</h4><p><img src="https://pic3.zhimg.com/80/v2-bc154f45fe226a218ee6ee67fd73c172_720w.jpg"></p><p>具体地说，<strong>NLU 会用来根据上下文来理解哪些信息是对当前任务重要的</strong>，比如订房任务中的联系人，入住人数，入住日期等。理解出的信息，会变成一个类别作为状态，交给策略管理模块去判断当前还有哪些信息缺失，是否需要继续询问，还是说信息已经全部补全，可以执行命令。如果是要补充信息，则要根据缺失的信息去让自然语言生成模块生成问题。</p><h4 id="Natural-Language-Understanding（NLU）"><a href="#Natural-Language-Understanding（NLU）" class="headerlink" title="Natural Language Understanding（NLU）"></a>Natural Language Understanding（NLU）</h4><p><img src="https://pic4.zhimg.com/80/v2-a5b66416d8cb34982c8e7293f2df467f_720w.jpg"></p><p>NLU 模块在任务型对话机器人中，通常会有两个任务：</p><ul><li><strong>意图识别（Intent Classification）</strong></li><li><strong>槽位填充（Slot Filling）</strong></li></ul><h5 id="Intent-Classification-amp-Slot-Filling"><a href="#Intent-Classification-amp-Slot-Filling" class="headerlink" title="Intent Classification &amp; Slot Filling"></a>Intent Classification &amp; Slot Filling</h5><p>意图识别需要弄清楚用户在做什么，是提供信息还是询问问题，它是一个分类任务。假如确认了用户是在提供信息，则槽位填充任务需要从用户回复文本中提取出关键信息，如时间，地点，对应的是入住时间还是退房时间等。但如果意图识别出的是询问问题，比如9月9号还有没有空房，那9月9号就可能不是入住的日期。而是要根据当天有无空房间来进行判断。</p><p><strong>槽位填充实质在做的与实体命名识别一样。</strong></p><p><img src="https://pic2.zhimg.com/80/v2-17725bc40f9bb2d449cf68cdd179c899_720w.jpg"></p><p>除了之前三个模块，语音助理中，再加上语音识别 ASR 和 语音合成 TTS 就成了完整的对话系统。</p><h2 id="Knowledge-Extraction"><a href="#Knowledge-Extraction" class="headerlink" title="Knowledge Extraction"></a>Knowledge Extraction</h2><p><img src="https://pic3.zhimg.com/80/v2-7880bd37d0adb384b58489ecfe65d2d6_720w.jpg"></p><p>知识图谱的构建简化地去理解可以看作是实体提取和关系抽取。实体可以是人可以是物，也可以是组织机构，非常灵活。关系可以是人与人的关系，可以是谓语动作，也可以是企业之间的资本流动。<strong>信息抽取</strong>任务希望从海量文本中自动挖掘出实体关系三元组。这个问题其实非常地复杂。这里只是简单地讲。</p><h3 id="Name-Entity-Recognition（NER）"><a href="#Name-Entity-Recognition（NER）" class="headerlink" title="Name Entity Recognition（NER）"></a>Name Entity Recognition（NER）</h3><p><img src="https://pic2.zhimg.com/80/v2-cbb3d81115e13739e4a7ff525079d4c9_720w.jpg"></p><p>什么是实体命名识别呢？其实命名实体它的内容并没有非常清楚的定义。它取决于我们对哪些事情关心。随着领域的不同，有所差异。一般的实体包括人名、组织和地名等等。但这不是完整的实体的定义。它取决于我们的具体应用。比如我们想让机器读大量医学相关的文献，希望它自动知道有什么药物可以治疗新冠状肺炎。这些药物的名字，就是实体。它输入的是一个序列，输出的是序列上每个位置的类别。它就和词性标注、槽位填充一样。</p><p>NER常见的两个问题是：</p><ul><li>名字一样但指的是不同的东西，有多个标签需要实体消歧；</li><li>不一样的名字指的却是相同的东西，需要实体归一化。</li></ul><p>总之，怎么抽取实体是有非常多的相关研究。</p><h3 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h3><p><img src="https://pic2.zhimg.com/80/v2-0503600495faad2aa80cec78c4a22579_720w.jpg"></p><p>假如我们知道如何从文本中获得实体，接下来还需要知道它们之间的关系。比如哈利波特是霍格沃茨的学生。关系抽取的输入是序列和抽取出的实体，输出是两两实体之间的关系。它是一个分类任务。</p><h2 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h2><p><img src="http://pic3.zhimg.com/80/v2-b3f953592f9a030521e7a40d08ccf15a_720w.jpg"></p><p>过去往往是一个NLP任务设计一个模型。听起来不是非常地智能。<font color="red"><strong>我们希望知道模型在各种任务上的一般表现。</strong></font>于是就有了一些标准和竞赛，Benchmark。一个知名的竞赛叫作 GLUE，主办方集合了很多觉得重要的能理解人类语言有关的任务，希望我们的模型能去解这些任务，看看可以得到什么样的结果。GLUE中任务分成三大类：</p><ul><li>第一大类是分类任务，包括语法错误检测和情感分类。它们都是输入是一个序列，输出是一个类别；</li><li>第二大类是输入是两个句子，输出是二者的语义是否相似对应；</li><li>第三大类都是自然语言推理相关的任务。输入前提和假设，希望机器能判断二者是否矛盾蕴含还是无关。GLUE 并没有涵盖所有的 NLP 类型问题。</li></ul><p><img src="https://pic2.zhimg.com/80/v2-8ca03c0f5d67d3b3bbf38c8d36c766ed_720w.jpg"></p><p><img src="https://pic4.zhimg.com/80/v2-145570e85087e921084f0b71443c3cdb_720w.jpg"></p><p>有了 BERT 和它的好朋友以后，GLUE指标都被打破稍微有些超出人类的表现了，所以需要新的 Benchmark，于是就有了 Super GLUE。它的任务大都是和 QA 比较有关系的，比如：</p><ul><li>输入一个段落，询问一个一般疑问句，回答是yes or no。（BoolQ，COPA，MultiRC）</li><li>或者是常识、逻辑推理。（CB，RTE）</li><li>也有是把看一个段落，回答填空的。（ReCoRD）</li><li>或者是给机器两个句子，两个句子中都有同样的词汇。看看机器能不能知道这两个词汇意思是一样的还是不是一样的。（WiC）</li><li>或者是给机器一个句子，句子上标了一个名词和一个代名词，希望机器能够判断二者是不是指代同一个东西。（WSC）</li></ul><h2 id="DecaNLP"><a href="#DecaNLP" class="headerlink" title="DecaNLP"></a>DecaNLP</h2><blockquote><p>十项全能NLP</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-7e587152c5de075f0494f62431a5ce6c_720w.jpg"></p><p>除了Super GLUE 以外，还有一个 Benchmark 叫 DecaNLP。它希望你用同一个模型来解决十个不同的任务。Super GLUE 虽然比 GLUE 增加了难度，但它大多数也还是分类任务，近年来也一直被刷榜。而DecaNLP中它的任务会更加困难，很多任务像摘要、翻译需要输出一个完整的句子。所有这些不同的任务，都可以看作是 QA 类型问题。怎么把各式各样的 NLP 任务看成是 QA 问题呢？比如翻译就是给定一段要把A语言翻译成B语言的问句，和一段文章，希望机器能把这段文章翻译成指定语言。摘要情感分类也是类似的。谷歌的 T5 Text-to-Text 模型，就可以做类似的。<strong>把不同任务用统一框架集成起来的好处有两个：</strong></p><ul><li>一是可以评估一个模型的综合表现；</li><li>二是可以促进研究者寻找出更通用的自然语言处理模型。</li></ul><hr><p>后记：真心推荐访问一下 DecaNLP 的链接去看一下它的 Benchmark。QA 框架统一各种任务非常值得模型来刷榜。这其实也是<strong>多任务学习的一个研究方向</strong>。</p><blockquote><p> <a href="https://decanlp.com/">https://decanlp.com/</a></p></blockquote><p><img src="https://pic2.zhimg.com/80/v2-8428ec38d9e15ea76cc83709be3ed501_720w.jpg"></p><h2 id="学习Model的方法"><a href="#学习Model的方法" class="headerlink" title="学习Model的方法"></a>学习Model的方法</h2><ul><li><p><strong>When</strong></p><p>哪一年发布的Model？</p></li><li><p><strong>Why</strong></p><p>Model产生的背景？</p></li><li><p><strong>What</strong></p><p>Model的图示是怎样的？</p></li><li><p><strong>Who</strong></p><p>哪个团队提出来的？</p></li><li><p><strong>Where</strong></p><p>在哪篇paper上提出来的？</p></li><li><p><strong>How</strong></p><p>Model的实现细节</p></li><li><p><strong>Practice</strong></p><p>代码实战，主要使用PyTorch~</p></li></ul><h2 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/145190483">2020 年 3月 新番 李宏毅 人类语言处理 独家笔记 一文看尽各种 NLP 任务(上) - 17</a></li><li><a href="https://zhuanlan.zhihu.com/p/145435951">2020 年 3月 新番 李宏毅 人类语言处理 独家笔记 一文看尽各种 NLP 任务(下) - 18</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sequence to sequence model: Introduction and concepts</title>
      <link href="/2021/01/03/sequence-to-sequence-model-introduction-and-concepts/"/>
      <url>/2021/01/03/sequence-to-sequence-model-introduction-and-concepts/</url>
      
        <content type="html"><![CDATA[<p>If we take a high-level view, a seq2seq model has encoder, decoder and intermediate step as its main components:</p><p><img src="https://miro.medium.com/max/858/1*3lj8AGqfwEE5KCTJ-dXTvg.png"></p><p><img src="https://miro.medium.com/max/1661/1*Ismhi-muID5ooWf3ZIQFFg.png"></p><p>We use embedding, so we have to first compile <strong>a “vocabulary” list</strong> containing all the words we want our model to be able to use or read. The model inputs will have to be tensors containing the IDs of the words in the sequence.</p><p>There are four symbols, however, that we need our vocabulary to contain. Seq2seq vocabularies usually reserve the first four spots for these elements:</p><ul><li><strong><pad></pad></strong>: During training, we’ll need to feed our examples to the network in batches. The inputs in these batches all need to be the same width for the network to do its calculation. Our examples, however, are not of the same length. That’s why we’ll need to pad shorter inputs to bring them to the same width of the batch</li><li><strong><eos></eos></strong>: This is another necessity of batching as well, but more on the decoder side. It allows us to tell the decoder where a sentence ends, and it allows the decoder to indicate the same thing in its outputs as well.</li><li><strong><unk></unk></strong>: If you’re training your model on real data, you’ll find you can vastly improve the resource efficiency of your model by ignoring words that don’t show up often enough in your vocabulary to warrant consideration. We replace those with <unk>.</unk></li><li><strong><go></go></strong>: This is the input to the first time step of the decoder to let the decoder know when to start generating output.</li></ul><p>Note: Other tags can be used to represent these functions. For example I’ve seen &lt; s &gt;&nbsp; and&nbsp;&nbsp;&lt; /s &gt; used in place of <go> and <eos>. So make sure whatever you use is consistent through preprocessing, and model training/inference.</eos></go></p><p>In this article, I will try to give a short and concise explanation of the sequence to sequence model which have recently achieved significant results on pretty complex tasks like machine translation, video captioning, question answering etc.</p><p><em>Prerequisites: the reader should already be familiar with neural networks and, in particular, recurrent neural networks (RNNs). In addition, knowledge of LSTM or GRU models is preferable. If you are not yet familiar with RNNs, I recommend reading</em> <a href="https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7"><em>this article</em></a> <em>which will give you a quick start. For LSTM and GRU, I suggest looking into</em> <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"><em>“Understanding LSTM Networks”</em></a> <em>as well as</em> <a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be"><em>“Understanding GRU Networks”</em></a><em>.</em></p><h2 id="Why-——Use-Cases-of-the-Sequence-to-Sequence-Model"><a href="#Why-——Use-Cases-of-the-Sequence-to-Sequence-Model" class="headerlink" title="Why?——Use Cases of the Sequence to Sequence Model"></a>Why?——Use Cases of the Sequence to Sequence Model</h2><p>A sequence to sequence model lies behind numerous systems which you face on a daily basis. For instance, seq2seq model powers applications like Google Translate, voice-enabled devices and online chatbots. Generally speaking, these applications are composed of:</p><ul><li><p><em>Machine translation</em> — a 2016 <a href="https://arxiv.org/pdf/1409.3215.pdf">paper</a> from Google shows how the seq2seq model’s translation quality “approaches or surpasses all currently published results”.</p><p><img src="https://miro.medium.com/max/1598/1*D2eqzElv414FFFAN1hg0WQ.png"></p></li><li><p><em>Speech recognition</em> — another Google <a href="https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0233.PDF">paper</a> compares the existing seq2seq models on the speech recognition task.</p><p><img src="https://miro.medium.com/max/895/1*NM_l6Ww2kQeEms_opuXxAg.png"></p></li><li><p><em>Video captioning</em> — a 2015 <a href="https://arxiv.org/pdf/1505.00487.pdf">paper</a> shows how a seq2seq yields great results on generating movie descriptions.</p><p><img src="https://miro.medium.com/max/1573/1*lQGaPX7Vy1Q_s1sc5N_IXw.png"></p></li></ul><p>These are only some applications where seq2seq is seen as the best solution. This model can be used as a solution to any sequence-based problem, <font color="red"><strong>especially ones where the inputs and outputs have different sizes and categories</strong>. </font>We will talk more about the model structure below.</p><h2 id="What？——Definition-of-the-Sequence-to-Sequence-Model"><a href="#What？——Definition-of-the-Sequence-to-Sequence-Model" class="headerlink" title="What？——Definition of the Sequence to Sequence Model"></a>What？——Definition of the Sequence to Sequence Model</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gmbz1ne20sj20m7084jri.jpg"></p><p><a href="https://arxiv.org/pdf/1409.3215.pdf">Introduced for the first time in 2014 by Google</a>,<font color="red"> <strong>a sequence to sequence model aims to map a fixed-length input with a fixed-length output where the length of the input and output may differ.</strong></font></p><p>For example, translating “What are you doing today?” from English to Chinese has input of 5 words and output of 7 symbols (今天你在做什麼？). Clearly, we can’t use a regular LSTM network to map each word from the English sentence to the Chinese sentence.</p><blockquote><p>不能直接使用LSTM的原因在于：</p><p>“What are you doing today?”与“今天你在做什麼？”不是Aligned的，如果使用LSTM那样一个timestamp对应一个output是不符合逻辑的。</p></blockquote><p>This is why the sequence to sequence model is used to address problems like that one.</p><h2 id="How——How-the-Sequence-to-Sequence-Model-works"><a href="#How——How-the-Sequence-to-Sequence-Model-works" class="headerlink" title="How——How the Sequence to Sequence Model works?"></a>How——How the Sequence to Sequence Model works?</h2><p>In order to fully understand the model’s underlying logic, we will go over the below illustration:</p><p><img src="https://miro.medium.com/max/2483/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg" alt="Encoder-decoder sequence to sequence model"></p><p>The model consists of 3 parts: </p><ol><li>encoder;</li><li>intermediate (encoder) vector;</li><li>decoder;</li></ol><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>A stack of several recurrent units (<code>LSTM</code> or <code>GRU</code> cells for better performance) where each accepts a single element of the input sequence, collects information for that element and propagates it forward.</p></li><li><p>In question-answering problem, the input sequence is a collection of all words from the question. Each word is represented as <em>x_i</em> where <em>i</em> is the order of that word.</p></li><li><p>The hidden states <em>h_i</em> are computed using the formula:</p><p><img src="https://miro.medium.com/max/2138/1*sKqGIDJm3P8DeSwl0WHGkg.png"></p></li></ul><p>This simple formula represents the result of an ordinary recurrent neural network. As you can see, we just apply the appropriate weights to the previous hidden state <em>h_(t-1)</em> and the input vector <em>x_t.</em></p><h3 id="Encoder-Vector"><a href="#Encoder-Vector" class="headerlink" title="Encoder Vector"></a>Encoder Vector</h3><ul><li>This is the final hidden state produced from the encoder part of the model. It is calculated using the formula above.</li><li><font color="red"><strong>This vector aims to encapsulate the information for all input elements in order to help the decoder make accurate predictions.</strong></font></li><li>It acts as the initial hidden state of the decoder part of the model.</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li><p>A stack of several recurrent units where each predicts an output <em>y_t</em> at a time step <em>t</em>.</p></li><li><p>Each recurrent unit accepts a hidden state from the previous unit and produces output as well as its own hidden state.</p></li><li><p>In the question-answering problem, the output sequence is a collection of all words from the answer. Each word is represented as <em>y_i</em> where <em>i</em> is the order of that word.</p></li><li><p>Any hidden state <em>h_i</em> is computed using the formula:</p><p><img src="https://miro.medium.com/max/1840/1*sdxvcjeV7NOUsR_VQ_nrUQ.png"></p></li></ul><p>As you can see, we are just using the previous hidden state to compute the next one.</p><ul><li><p>The output <em>y_t</em> at time step <em>t</em> is computed using the formula:</p><p><img src="https://miro.medium.com/max/1975/1*y5T2-J2mrCRZp5M9Q4METw.png"></p></li></ul><p>We calculate the outputs using the hidden state at the current time step together with the respective weight W(S). <a href="https://www.youtube.com/watch?v=LLux1SW--oM">Softmax</a> is used to create a probability vector which will help us determine the final output (e.g. word in the question-answering problem).</p><p><strong>The power of this model lies in the fact that it can map sequences of different lengths to each other.</strong> As you can see the inputs and outputs are not correlated and their lengths can differ.  This opens a whole new range of problems which can now be solved using such architecture.</p><h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><p>The above explanation just covers the simplest sequence to sequence model and, thus, we cannot expect it to perform well on complex tasks. The reason is that using a single vector for encoding the whole input sequence is not capable of capturing the whole information.</p><p>This is why multiple enhancements are being introduced. Each one aims to strengthen the performance of this model on slightly complex tasks with long input and output sequences. Examples are:</p><ul><li><a href="https://arxiv.org/pdf/1409.3215.pdf">Reversing the order of the input sequence</a>.</li><li>Using LSTM or GRU cells.</li><li><a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">Introducing Attention mechanism</a>.</li><li>and many more.</li></ul><p>If you want to strengthen your knowledge of this wonderful deep learning model, I strongly recommend watching <a href="https://youtu.be/QuELiw8tbx8?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;t=1190">Richard Socher’s lecture on Machine Translation</a>.</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d">Sequence to sequence model: Introduction and concepts</a></li><li><a href="https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346">Understanding Encoder-Decoder Sequence to Sequence Model</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Seq2Seq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>依存句法分析</title>
      <link href="/2021/01/03/yi-cun-ju-fa-fen-xi/"/>
      <url>/2021/01/03/yi-cun-ju-fa-fen-xi/</url>
      
        <content type="html"><![CDATA[<p><strong>语法分析</strong>(syntactic parsing )是自然语言处理中一个重要的任务，其目标是分析句子的语法结构并将其表示为容易理解的结构(通常是树形结构)。同时，语法分析也是所有工具性NLP任务中较为高级、较为复杂的一种任务。 通过掌握语法分析的原理、实现和应用，我们将在NLP工程师之路上跨越一道分水岭。本章将会介绍<strong>短语结构树</strong>和<strong>依存句法树</strong>两种语法形式，并且着重介绍依存句法分析的原理和实现。</p><h2 id="短语结构树"><a href="#短语结构树" class="headerlink" title="短语结构树"></a>短语结构树</h2><p>语言其实具备自顶而下的层级关系，固定数量的语法结构能够生成无数句子。比如，仅仅利用下列两个语法规律，我们就能够生成所有名词短语。</p><ul><li>名词短语可以由名词和名词短语组成。</li><li>名词短语还可以由名词和名词组成。</li></ul><p>例如，“上海+浦东+机场+航站楼”，所以，汉语中大部分句子都可以通过这样的语法来生成。</p><p>在语言学中，这样的语法被称为<strong>上下文无关文法</strong>，它由如下组件构成:</p><ul><li>终结符结合 Σ，比如汉语的一个词表。</li><li>非终结符集合 V，比如“名词短语”“动词短语”等短语结构组成的集合。V 中至少包含一个特殊的非终结符，即句子符或初始符，计作 S。</li><li>推到规则 R，即推到非终结符的一系列规则: V -&gt; V U Σ。</li></ul><p>基于上下文无关文法理论，我们可以从 S 出发，逐步推导非终结符。一个非终结符至少产生一个下级符号，如此一层一层地递推下去，我们就得到了一棵语法树。但在NLP中，我们称其为短语结构树。也就是说，<font color="red"><strong>计算机科学中的术语“上下文无关文法”在语言学中被称作“短语结构语法”。</strong></font></p><h3 id="短语结构树-1"><a href="#短语结构树-1" class="headerlink" title="短语结构树"></a>短语结构树</h3><p>短语结构语法描述了如何自顶而下的生成一个句子，反过来，句子也可以用短语结构语法来递归的分解。层级结构其实是一种树形结构，例如这句话“上海 浦东 开发 与 法制 建设 同步”，分解成如下图的短语结构树:</p><p><img src="https://pic2.zhimg.com/v2-2f680ffae7a7c007fcd7f82c8a462961_b.jpg"></p><p>这样的树形结构称为<strong>短语结构树</strong>，相应的语法称为<strong>短语结构语法</strong>或<strong>上下文无关文法</strong>。至于树中的字母下面开始介绍。</p><h3 id="宾州树库和中文树库"><a href="#宾州树库和中文树库" class="headerlink" title="宾州树库和中文树库"></a>宾州树库和中文树库</h3><p>语言学家制定短语结构语法规范，将大量句子人工分解为树形结构，形成了一种语料库，称为<strong>树库</strong>( treebank )。常见的英文树库有宾州树库，相应地，中文领域有CTB。上图中叶子节点(词语)的上级节点为词性，词性是非终结符的一种，满足“词性生成词语”的推导规则。</p><p>常见的标记如下:</p><p><img src="https://pic2.zhimg.com/v2-18c176d1b65474295449334381e8da49_b.jpg"></p><p>但是由于短语结构语法比较复杂，相应句法分析器的准确率并不高，现在研究者绝大部分转向了另一种语法形式。</p><h2 id="依存句法树🌲🌲🌲🌲🌲"><a href="#依存句法树🌲🌲🌲🌲🌲" class="headerlink" title="依存句法树🌲🌲🌲🌲🌲"></a>依存句法树🌲🌲🌲🌲🌲</h2><p>不同于短语结构树，依存句法树并不关注如何生成句子这种宏大的命题。<font color="red"><strong>依存句法树关注的是句子中词语之间的语法联系，并且将其约束为树形结构。</strong></font></p><h3 id="依存句法理论"><a href="#依存句法理论" class="headerlink" title="依存句法理论"></a>依存句法理论</h3><p>依存句法理论认为词与词之间存在主从关系，这是一种二元不等价的关系。在句子中，如果一个词修饰另一个词，则称修饰词为<strong>从属词</strong>( dependent )，被修饰的词语称为<strong>支配词</strong>(head)，两者之间的语法关系称为**依存关系( dependency relation)**。比如句子“大梦想”中形容词“大”与名词“梦想”之间的依存关系如图所示:7</p><p><img src="https://pic3.zhimg.com/v2-86367b430600de3e682df1d4d3a6e12e_b.jpg"></p><p>图中的箭头方向由支配词指向从属词，这是可视化时的习惯。将一个句子中所有词语的依存关系以有向边的形式表示出来，就会得到一棵树，称为<strong>依存句法树</strong>( dependency parse tree)。比如句子“弱小的我也有大梦想”的依存句法树如图所示。</p><p><img src="https://pic4.zhimg.com/v2-13d51fddf84c8725da23436b9f9b5c73_b.jpg"></p><p>现代依存语法中，语言学家 Robinson 对依存句法树提了 4 个约束性的公理。</p><ul><li>有且只有一个词语(ROOT，虚拟根节点，简称虚根)不依存于其他词语。</li><li>除此之外所有单词必须依存于其他单词。</li><li>每个单词不能依存于多个单词。</li><li>如果单词 A 依存于 B，那么位置处于 A 和 B 之间的单词 C 只能依存于 A、B 或 AB 之间的单词。</li></ul><p>这 4 条公理分别约束了依存句法树(图的特例)的根节点唯一性、 连通、无环和投射性( projective )。这些约束对语料库的标注以及依存句法分析器的设计奠定了基础。</p><h3 id="中文依存句法树库"><a href="#中文依存句法树库" class="headerlink" title="中文依存句法树库"></a>中文依存句法树库</h3><p>目前最有名的开源自由的依存树库当属UD ( Universal Dependencies)，它以“署名-非商业性使用-相同方式共享4.0”等类似协议免费向公众授权。UD是个跨语种的语法标注项目，一共有 200 多名贡献者为 70 多种语言标注了 100 多个树库。具体到中文，存在4个不同领域的树库。本章选取其中规模最大的 UD_ Chinese GSD 作为示例。该树库的语种为繁体中文，将其转换为简体中文后，供大家下载使用。</p><p><strong><a href="https://link.zhihu.com/?target=http://file.hankcs.com/corpus/chs-gsd-ud.zip">http://file.hankcs.com/corpus/chs-gsd-ud.zip</a></strong></p><p>该树库的格式为 CoNLL-U，这是一种以制表符分隔的表格格式。CoNLL-U 文件有10列，每行都是一个单词， 空白行表示句子结束。单元中的下划线 _ 表示空白， 结合其中一句样例，解释如表所示。</p><p><img src="https://pic2.zhimg.com/v2-db015456831baabe39057fb170664b85_b.jpg"></p><p>词性标注集合依存关系标注集请参考 UD 的官方网站:</p><p><strong><a href="https://link.zhihu.com/?target=http://niversaldependencies.org/guidelines.html">http://niversaldependencies.org/guidelines.html</a></strong></p><p>另一份著名的语料库依然是 CTB，只不过需要额外利用一些工具将短语结构树转换为依存句法树。读者可以直接下载转换后的 CTB 依存句法树库，其格式是类似于 CoNLl-U 的 CoNLL。</p><p><img src="http://img-blog.csdnimg.cn/20200521165058505.png"></p><p>下面是对分析的结果中一些符号的解释：</p><p>ROOT：要处理文本的语句</p><p>IP：简单从句</p><p>NP：名词短语</p><p>VP：动词短语</p><p>PU：断句符，通常是句号、问号、感叹号等标点符号</p><p>LCP：方位词短语</p><p>PP：介词短语</p><p>CP：由‘的’构成的表示修饰性关系的短语</p><p>DNP：由‘的’构成的表示所属关系的短语</p><p>ADVP：副词短语</p><p>ADJP：形容词短语</p><p>DP：限定词短语</p><p>QP：量词短语</p><p>NN：常用名词</p><p>NR：固有名词</p><p>NT：时间名词</p><p>PN：代词</p><p>VV：动词</p><p>VC：是</p><p>CC：表示连词</p><p>VE：有</p><p>VA：表语形容词</p><p>AS：内容标记（如：了）</p><p>VRD：动补复合词</p><p>CD: 表示基数词</p><p>DT: determiner 表示限定词</p><p>EX: existential there 存在句</p><p>FW: foreign word 外来词</p><p>IN: preposition or conjunction, subordinating 介词或从属连词</p><p>JJ: adjective or numeral, ordinal 形容词或序数词</p><p>JJR: adjective, comparative 形容词比较级</p><p>JJS: adjective, superlative 形容词最高级</p><p>LS: list item marker 列表标识</p><p>MD: modal auxiliary 情态助动词</p><p>PDT: pre-determiner 前位限定词</p><p>POS: genitive marker 所有格标记</p><p>PRP: pronoun, personal 人称代词</p><p>RB: adverb 副词</p><p>RBR: adverb, comparative 副词比较级</p><p>RBS: adverb, superlative 副词最高级</p><p>RP: particle 小品词</p><p>SYM: symbol 符号</p><p>TO:”to” as preposition or infinitive marker 作为介词或不定式标记</p><p>WDT: WH-determiner WH限定词</p><p>WP: WH-pronoun WH代词</p><p>WP$: WH-pronoun, possessive WH所有格代词</p><p>WRB:Wh-adverb WH副词</p><p><strong>关系表示</strong></p><p>abbrev: abbreviation modifier，缩写</p><p>acomp: adjectival complement，形容词的补充；</p><p>advcl : adverbial clause modifier，状语从句修饰词</p><p>advmod: adverbial modifier状语</p><p>agent: agent，代理，一般有by的时候会出现这个</p><p>amod: adjectival modifier形容词</p><p>appos: appositional modifier,同位词</p><p>attr: attributive，属性</p><p>aux: auxiliary，非主要动词和助词，如BE,HAVE SHOULD/COULD等到</p><p>auxpass: passive auxiliary 被动词</p><p>cc: coordination，并列关系，一般取第一个词</p><p>ccomp: clausal complement从句补充</p><p>complm: complementizer，引导从句的词好重聚中的主要动词</p><p>conj : conjunct，连接两个并列的词。</p><p>cop: copula。系动词（如be,seem,appear等），（命题主词与谓词间的）连系</p><p>csubj : clausal subject，从主关系</p><p>csubjpass: clausal passive subject 主从被动关系</p><p>dep: dependent依赖关系</p><p>det: determiner决定词，如冠词等</p><p>dobj : direct object直接宾语</p><p>expl: expletive，主要是抓取there</p><p>infmod: infinitival modifier，动词不定式</p><p>iobj : indirect object，非直接宾语，也就是所以的间接宾语；</p><p>mark: marker，主要出现在有“that” or “whether”“because”, “when”,</p><p>mwe: multi-word expression，多个词的表示</p><p>neg: negation modifier否定词</p><p>nn: noun compound modifier名词组合形式</p><p>npadvmod: noun phrase as adverbial modifier名词作状语</p><p>nsubj : nominal subject，名词主语</p><p>nsubjpass: passive nominal subject，被动的名词主语</p><p>num: numeric modifier，数值修饰</p><p>number: element of compound number，组合数字</p><p>parataxis: parataxis: parataxis，并列关系</p><p>partmod: participial modifier动词形式的修饰</p><p>pcomp: prepositional complement，介词补充</p><p>pobj : object of a preposition，介词的宾语</p><p>poss: possession modifier，所有形式，所有格，所属</p><p>possessive: possessive modifier，这个表示所有者和那个’S的关系</p><p>preconj : preconjunct，常常是出现在 “either”, “both”, “neither”的情况下</p><p>predet: predeterminer，前缀决定，常常是表示所有</p><p>prep: prepositional modifier</p><p>prepc: prepositional clausal modifier</p><p>prt: phrasal verb particle，动词短语</p><p>punct: punctuation，这个很少见，但是保留下来了，结果当中不会出现这个</p><p>purpcl : purpose clause modifier，目的从句</p><p>quantmod: quantifier phrase modifier，数量短语</p><p>rcmod: relative clause modifier相关关系</p><p>ref : referent，指示物，指代</p><p>rel : relative</p><p>root: root，最重要的词，从它开始，根节点</p><p>tmod: temporal modifier</p><p>xcomp: open clausal complement</p><p>xsubj : controlling subject 掌控者</p><p><strong>中心语为谓词</strong></p><p>subj — 主语</p><p>nsubj — 名词性主语（nominal subject） （同步，建设）</p><p>top — 主题（topic） （是，建筑）</p><p>npsubj — 被动型主语（nominal passive subject），专指由“被”引导的被动句中的主语，一般是谓词语义上的受事 （称作，镍）</p><p>csubj — 从句主语（clausal subject），中文不存在</p><p>xsubj — x主语，一般是一个主语下面含多个从句 （完善，有些）</p><p><strong>中心语为谓词或介词</strong></p><p>obj — 宾语</p><p>dobj — 直接宾语 （颁布，文件）</p><p>iobj — 间接宾语（indirect object），基本不存在</p><p>range — 间接宾语为数量词，又称为与格 （成交，元）</p><p>pobj — 介词宾语 （根据，要求）</p><p>lobj — 时间介词 （来，近年）</p><p><strong>中心语为谓词</strong></p><p>comp — 补语</p><p>ccomp — 从句补语，一般由两个动词构成，中心语引导后一个动词所在的从句(IP) （出现，纳入）</p><p>xcomp — x从句补语（xclausal complement），不存在</p><p>acomp — 形容词补语（adjectival complement）</p><p>tcomp — 时间补语（temporal complement） （遇到，以前）</p><p>lccomp — 位置补语（localizer complement） （占，以上）</p><p>— 结果补语（resultative complement）</p><p><strong>中心语为名词</strong></p><p>mod — 修饰语（modifier）</p><p>pass — 被动修饰（passive）</p><p>tmod — 时间修饰（temporal modifier）</p><p>rcmod — 关系从句修饰（relative clause modifier） （问题，遇到）</p><p>numod — 数量修饰（numeric modifier） （规定，若干）</p><p>ornmod — 序数修饰（numeric modifier）</p><p>clf — 类别修饰（classifier modifier） （文件，件）</p><p>nmod — 复合名词修饰（noun compound modifier） （浦东，上海） amod — 形容词修饰（adjetive modifier） （情况，新）</p><p>advmod — 副词修饰（adverbial modifier） （做到，基本）</p><p>vmod — 动词修饰（verb modifier，participle modifier）</p><p>prnmod — 插入词修饰（parenthetical modifier）</p><p>neg — 不定修饰（negative modifier） (遇到，不)</p><p>det — 限定词修饰（determiner modifier） （活动，这些） possm — 所属标记（possessive marker），NP</p><p>poss — 所属修饰（possessive modifier），NP</p><p>dvpm — DVP标记（dvp marker），DVP （简单，的）</p><p>dvpmod — DVP修饰（dvp modifier），DVP （采取，简单）</p><p>assm — 关联标记（associative marker），DNP （开发，的）</p><p>assmod — 关联修饰（associative modifier），NP|QP （教训，特区） prep — 介词修饰（prepositional modifier） NP|VP|IP（采取，对） clmod — 从句修饰（clause modifier） （因为，开始）</p><p>plmod — 介词性地点修饰（prepositional localizer modifier） （在，上） asp — 时态标词（aspect marker） （做到，了）</p><p>partmod– 分词修饰（participial modifier） 不存在</p><p>etc — 等关系（etc） （办法，等）</p><p><strong>中心语为实词</strong></p><p>conj — 联合(conjunct)</p><p>cop — 系动(copula) 双指助动词？？？？</p><p>cc — 连接(coordination)，指中心词与连词 （开发，与）</p><p><strong>其它</strong></p><p>attr — 属性关系 （是，工程）</p><p>cordmod– 并列联合动词（coordinated verb compound） （颁布，实行） mmod — 情态动词（modal verb） （得到，能）</p><p>ba — 把字关系</p><p>tclaus — 时间从句 （以后，积累）</p><p>— semantic dependent</p><p>cpm — 补语化成分（complementizer），一般指“的”引导的CP （振兴，的）</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/107676106">《自然语言处理入门》12.依存句法分析–提取用户评论</a></li><li><a href="https://blog.csdn.net/lihaitao000/article/details/51812618">Stanford-parser依存句法关系解释</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【陈蕴侬-ADL】6-Recurrent Neural Network</title>
      <link href="/2020/12/30/chen-yun-nong-adl-6-recurrent-neural-network/"/>
      <url>/2020/12/30/chen-yun-nong-adl-6-recurrent-neural-network/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>裤子尺码</title>
      <link href="/2020/12/25/ku-zi-chi-ma/"/>
      <url>/2020/12/25/ku-zi-chi-ma/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>尺码</th><th>大小</th><th></th></tr></thead><tbody><tr><td>身高（CM）</td><td>164</td><td></td></tr><tr><td>胸围（CM）</td><td>84</td><td></td></tr><tr><td>腰围（CM）</td><td>71</td><td></td></tr><tr><td>臀围（CM）</td><td>86</td><td></td></tr><tr><td>腿内侧长（CM）</td><td>76</td><td></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>歌曲推荐</title>
      <link href="/2020/12/23/ge-qu-tui-jian/"/>
      <url>/2020/12/23/ge-qu-tui-jian/</url>
      
        <content type="html"><![CDATA[<h2 id="Jack-Antonoff"><a href="#Jack-Antonoff" class="headerlink" title="Jack Antonoff"></a>Jack Antonoff</h2><blockquote><p>Jack Antonoff是我最喜欢的一位音乐制作人，在我真正认识他之前，我听过很多他写的歌。当后来我发现这些歌都是他写的时候，我就一发不可收拾地爱上他了~</p></blockquote><p>全程操刀，三张神专：</p><ol><li><strong>Melodrama</strong></li><li><strong>Norman Fucking Rockwell!</strong></li><li><strong>Folklore</strong></li></ol><p>今年我还在探索的专辑：</p><ol><li>evermore</li></ol><p>其他好听的单曲：</p><ol><li>We Are Young</li><li>I Don’t Wanna Live Forever</li><li>Beautiful Trauma</li><li>Strawberries&amp;Cigarettes（好听哭惹）</li><li>Heaven</li><li>太多啦……</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>毕设方向探索</title>
      <link href="/2020/12/22/bi-she-fang-xiang-tan-suo/"/>
      <url>/2020/12/22/bi-she-fang-xiang-tan-suo/</url>
      
        <content type="html"><![CDATA[<h2 id="方向一：GNN的Representation-Learning改进"><a href="#方向一：GNN的Representation-Learning改进" class="headerlink" title="方向一：GNN的Representation Learning改进"></a>方向一：GNN的Representation Learning改进</h2><h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><p>首先，GNN作为一种神经网络（就像CNN，RNN这样的NN），它的<strong>应用领域</strong>有：</p><ol><li>Natural Language Processing<ol><li><strong>Neural Machine Translations</strong></li><li><strong>Reading Comprehension</strong></li><li><strong>Relation Extraction</strong></li><li><strong>Text Classification</strong></li><li><strong>Text Generation</strong></li><li><strong>Miscellaneous</strong></li></ol></li><li>Computer Vision</li><li><strong>Recommended System</strong></li><li>Intelligent Traffic</li><li>Programming Source Code</li><li>Physical System</li><li>Chemistry and Biology</li><li>Knowledge Graph</li><li>Combinatorial Optimization</li><li>Complex Network</li></ol><h3 id="GNN的未来研究方向"><a href="#GNN的未来研究方向" class="headerlink" title="GNN的未来研究方向"></a>GNN的未来研究方向</h3><ol><li><p>Model Depth</p><p>Deep Learning的一个特点在于网络的Deep，但是GNN却不能叠很深，一深性能就会下降。（6度原理~）</p></li><li><p>Heterogenity</p><p>用在异质信息网络上的GNN；</p></li><li><p>Dynamicity</p><p>现实中很多图并不是静态的，它会动态地增加和减少边和节点，如何将GNN用到这种动态图上是一个研究方向；</p></li><li><p>Highly Scalable GNNs</p><p>设计具有高度可扩展性的GNN，能够应用到具有大量edges，nodes的动态图上；</p></li><li><p>Robust GNN</p><p>目前的GNN中，一旦输入图的结构和/或初始特征被对手攻击，GNN模型的性能将急剧下降，因此需要健壮的GNN；</p></li><li><p>GNNs Going Beyond WL Test</p><p>空间GCNNs的性能受到1-WL检验的限制，而高阶WL检验计算量大（不太懂）</p></li><li><p>Interpretable GNNs</p><p>现有的GNN工作就像一个黑盒子，我们需要构建一个GNN的可解释框架。</p></li></ol><h3 id="感兴趣的领域——Recommender-System"><a href="#感兴趣的领域——Recommender-System" class="headerlink" title="感兴趣的领域——Recommender System"></a>感兴趣的领域——Recommender System</h3><p>根据输入数据的不同，分为两大类推荐：</p><ol><li>General Recommendation<ul><li>Without side information</li><li>Social network enhanced</li><li>Knowledge graph enhanced</li></ul></li><li>Sequential Recommendation<ul><li>Without side information</li><li>Social network enhanced</li></ul></li></ol><p>可以探索的方向：</p><ol><li>Efficient GNNs for Heterogeneous Graphs</li><li>Multi-graph information Integration</li><li>Scalability of GNNs in Recommendation</li><li>Sequence Graph Construction for Sequential Recommendation</li></ol><h2 id="方向二：NLP-with-GNN-approaches"><a href="#方向二：NLP-with-GNN-approaches" class="headerlink" title="方向二：NLP with GNN approaches"></a>方向二：NLP with GNN approaches</h2><blockquote><p>地址：G:\SelfLearning\JupyterNotebook3.0\Conferences\CCKS\2020</p></blockquote><h3 id="Task-4：面向金融领域的小样本跨类迁移事件抽取"><a href="#Task-4：面向金融领域的小样本跨类迁移事件抽取" class="headerlink" title="Task 4：面向金融领域的小样本跨类迁移事件抽取"></a><strong>Task 4：面向金融领域的小样本跨类迁移事件抽取</strong></h3><p>任务：在closed-domain下，给定<strong>句子</strong>，将大样本下训练的模型跨类迁移到小样本的其他事件类型上。</p><p>数据：</p><pre><code class="text">{"id": "ecb7d40130299305dd53bc9096449919","content": "所以,鉴于三人特殊的亲戚关系,业内认为,金马股份收购众泰汽车的做法,更像是一场自导自演、自我抬高身价的家族游戏。","events": [{"type": "收购", "mentions": [{"word": "金马股份", "span": [20, 24], "role": "sub-org"},{"word": "收购", "span": [24, 26], "role": "trigger"}, {"word": "众泰汽车", "span": [26, 30], "role": "obj-org"}]}]}</code></pre><p>视频：</p><ol><li>任务四“面向金融领域的小样本跨类迁移事件抽取”评测第一名报告  评测：A Joint Learning Framework for the CCKS-2020 Financial Event Extraction Task Speaker：盛傢伟 [<a href="https://hub.baai.ac.cn/view/4106">▶]</a> </li><li>任务四“面向金融领域的小样本跨类迁移事件抽取”报告  评测：Transfer Learning for Small-scale Financial Event Extraction Speaker：宁星星 [<a href="https://hub.baai.ac.cn/view/4122j">▶]</a> </li></ol><p>比赛链接：</p><ul><li><a href="https://biendata.xyz/competition/ccks_2020_3/">CCKS 2020：面向金融领域的小样本跨类迁移事件抽取</a></li></ul><h3 id="Task-5：面向金融领域的篇章级事件主体与要素抽取"><a href="#Task-5：面向金融领域的篇章级事件主体与要素抽取" class="headerlink" title="Task 5：面向金融领域的篇章级事件主体与要素抽取"></a><strong>Task 5：面向金融领域的篇章级事件主体与要素抽取</strong></h3><p>任务：在closed-domain下，给定<strong>篇章</strong>，完成两个独立子任务：（1）事件类型和事件主体抽取；（2）事件类型和篇章事件要素抽取</p><p>数据：</p><pre><code class="text">"2444634    世联君汇预计2017年净利下滑近8成至853万元中超电缆(002471)再遭中超集团减持5%股份    业绩下滑    世联君汇""2836026    LG空调亏损严重或效仿新科 两大缺陷遭退市尴尬华兰生物(002007)三季度净利下降45% 汇添富或为“失血门”跑路主力    业绩下滑    华兰生物""2809128    四方达(300179)股东减持60万股 套现414.6万元收到欧盟打款后希腊总理宣布辞职再选 欧盟紧急声明要求希腊恪守承诺巨力索具(002342)下调预测 因产品毛利率下降    业绩下滑    巨力索具"                                                    "2221860    单元式空调抽查不合格名单春兰、日立产品入列北大荒(600598)计提坏账净利骤降83.5%好利来(002729)股东高位减持    业绩下滑    北大荒"</code></pre><p>视频：</p><ol><li>任务五“面向金融领域的篇章级事件主题与要素抽取”子任务第一名报告  评测：基于BERT的事件主体抽取 Speaker：潘春光 [<a href="https://hub.baai.ac.cn/view/4142">▶]</a> [[PDF下载]](<a href="https://hub-cache.baai.ac.cn/hub-pdf/20201123/CCKS2020-PPT/%E8%AF%84%E6%B5%8B/%E4%BB%BB%E5%8A%A1%E5%9B%9B">https://hub-cache.baai.ac.cn/hub-pdf/20201123/CCKS2020-PPT/评测/任务四</a> 基于BERT的事件主体抽取.pdf)</li><li>任务五“面向金融领域的篇章级事件主题与要素抽取”子任务第一名报告 评测：A Prior Information Enhanced Extraction Framework for Document-level Financial Event Extraction Speaker：王海涛 [<a href="https://hub.baai.ac.cn/view/4105">▶]</a> [[PDF下载]](<a href="https://hub-cache.baai.ac.cn/hub-pdf/20201123/CCKS2020-PPT/%E8%AF%84%E6%B5%8B/%E4%BB%BB%E5%8A%A1%E4%BA%94">https://hub-cache.baai.ac.cn/hub-pdf/20201123/CCKS2020-PPT/评测/任务五</a> A Prior Information Enhanced Extraction Framework for Document-level Financial Event Extraction.pdf)</li></ol><p>比赛链接：</p><ul><li><a href="https://biendata.xyz/competition/ccks_2020_4_1/">CCKS 2020：面向金融领域的篇章级事件主体与要素抽取（一）事件主体抽取</a></li><li><a href="https://biendata.xyz/competition/ccks_2020_4_2/">CCKS 2020：面向金融领域的篇章级事件主体与要素抽取（二）篇章事件要素抽取</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 毕设 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode 1109. Corporate Flight Bookings【差分数组】</title>
      <link href="/2020/12/21/leetcode-1109-corporate-flight-bookings-chai-fen-shu-zu/"/>
      <url>/2020/12/21/leetcode-1109-corporate-flight-bookings-chai-fen-shu-zu/</url>
      
        <content type="html"><![CDATA[<p>读完本文，你不仅学会了算法套路，还可以顺便去 LeetCode 上拿下如下题目：</p><p><a href="http://ww1.sinaimg.cn/large/9b63ed6fgy1glvqyy7v1qj20xc0m8wgt.jpg">http://ww1.sinaimg.cn/large/9b63ed6fgy1glvqyy7v1qj20xc0m8wgt.jpg</a></p><p><a href="https://leetcode-cn.com/problems/corporate-flight-bookings">1109.航班预订统计（中等）</a></p><p>前文 <a href="https://labuladong.gitbook.io/algo/suan-fa-si-wei-xi-lie/3.3-qi-ta-suan-fa-pian/qian-zhui-he-ji-qiao">前缀和技巧详解</a> 写过的前缀和技巧是非常常用的算法技巧，<strong>前缀和主要适用的场景是原始数组不会被修改的情况下，频繁查询某个区间的累加和</strong>。</p><p>没看过前文没关系，这里简单介绍一下前缀和，核心代码就是下面这段：</p><pre><code class="java">class PrefixSum {    // 前缀和数组    private int[] prefix;    /* 输入一个数组，构造前缀和 */    public PrefixSum(int[] nums) {        prefix = new int[nums.length + 1];        // 计算 nums 的累加和        for (int i = 1; i &lt; prefix.length; i++) {            prefix[i] = prefix[i - 1] + nums[i - 1];        }    }    /* 查询闭区间 [i, j] 的累加和 */    public int query(int i, int j) {        return prefix[j + 1] - prefix[i];    }}</code></pre><p><img src="https://gblobscdn.gitbook.com/assets%2F-MOg91qJOV680ranYFeJ%2Fsync%2F84765a97833a4fd9fa0d43f8affecbe5a739bac5.jpeg?alt=media" alt="img"></p><p><code>prefix[i]</code> 就代表着 <code>nums[0..i-1]</code> 所有元素的累加和，如果我们想求区间 <code>nums[i..j]</code> 的累加和，只要计算 <code>prefix[j+1] - prefix[i]</code> 即可，而不需要遍历整个区间求和。</p><p>本文讲一个和前缀和思想非常类似的算法技巧「差分数组」，<strong>差分数组的主要适用场景是频繁对原始数组的某个区间的元素进行增减</strong>。</p><p>比如说，我给你输入一个数组 <code>nums</code>，然后又要求给区间 <code>nums[2..6]</code> 全部加 1，再给 <code>nums[3..9]</code> 全部减 3，再给 <code>nums[0..4]</code> 全部加 2，再给…</p><p>一通操作猛如虎，然后问你，最后 <code>nums</code> 数组的值是什么？</p><p>常规的思路很容易，你让我给区间 <code>nums[i..j]</code> 加上 <code>val</code>，那我就一个 for 循环给它们都加上呗，还能咋样？这种思路的时间复杂度是 O(N)，由于这个场景下对 <code>nums</code> 的修改非常频繁，所以效率会很低下。</p><p>这里就需要差分数组的技巧，类似前缀和技巧构造的 <code>prefix</code> 数组，我们先对 <code>nums</code> 数组构造一个 <code>diff</code> 差分数组，**<code>diff[i]</code>** <strong>就是</strong> <strong><code>nums[i]</code></strong> <strong>和</strong> <strong><code>nums[i-1]</code></strong> <strong>之差</strong>：</p><pre><code class="java">int[] diff = new int[nums.length];// 构造差分数组diff[0] = nums[0];for (int i = 1; i &lt; nums.length; i++) {    diff[i] = nums[i] - nums[i - 1];}</code></pre><p><img src="https://gblobscdn.gitbook.com/assets%2F-MOg91qJOV680ranYFeJ%2Fsync%2F211fccb2852e16f30e721fbe7e172ed85cd4bc89.jpeg?alt=media" alt="img"></p><p>通过这个 <code>diff</code> 差分数组是可以反推出原始数组 <code>nums</code> 的，代码逻辑如下：</p><pre><code class="java">int[] res = new int[diff.length];// 根据差分数组构造结果数组res[0] = diff[0];for (int i = 1; i &lt; diff.length; i++) {    res[i] = res[i - 1] + diff[i];}</code></pre><p><strong>这样构造差分数组<code>diff</code>，就可以快速进行区间增减的操作</strong>，如果你想对区间 <code>nums[i..j]</code> 的元素全部加 3，那么只需要让 <code>diff[i] += 3</code>，然后再让 <code>diff[j+1] -= 3</code> 即可：</p><p><img src="https://gblobscdn.gitbook.com/assets%2F-MOg91qJOV680ranYFeJ%2Fsync%2Fc8c78f98a56eab1444a656affc7993d1e0ca58cf.jpeg?alt=media" alt="img"></p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glvqiyz8xuj20hf0irq40.jpg"></p><h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><p>查分数组</p><h2 id="我的实现"><a href="#我的实现" class="headerlink" title="我的实现"></a>我的实现</h2><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glvqpbcl92j20hn08xmxn.jpg"></p><h3 id="码前思考"><a href="#码前思考" class="headerlink" title="码前思考"></a>码前思考</h3><ul><li>利用差分数组进行解题，<code>diff[i] = num[i] - num[i-1]</code>；</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code class="cpp">//使用差分数组解题//diff[i] = num[i]-num[i-1]class Solution {public:    vector&lt;int&gt; corpFlightBookings(vector&lt;vector&lt;int&gt;&gt;&amp; bookings, int n) {        vector&lt;int&gt; diff(n+2,0);        int len = bookings.size();        for(int idx=0;idx&lt;len;idx++){            int i = bookings[idx][0];            int j = bookings[idx][1];            int k = bookings[idx][2];            diff[i] = diff[i]+k;            diff[j+1] = diff[j+1]-k;              }        vector&lt;int&gt; res;        res.push_back(diff[1]);        for(int i=2;i&lt;=n;i++){            res.push_back(res[i-2]+diff[i]);        }        return res;    }};</code></pre><h3 id="码后反思"><a href="#码后反思" class="headerlink" title="码后反思"></a>码后反思</h3><ol><li>写代码的时候有点不清醒，把数组下标写错了；</li><li>对于这道题，还有一个类似的题目是<strong>上下车问题</strong>，此时<code>[i,j,k]</code>表示有<code>k</code>个人从<code>i</code>站坐到<code>j+1</code>站下车，求解每一站有多少人。</li><li>从今天开始每日一题了，不然研究生毕业可找不到工作的呢~</li></ol>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数组 </tag>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCKS（全国知识图谱与语义计算大会）</title>
      <link href="/2020/12/18/ccks-quan-guo-zhi-shi-tu-pu-yu-yu-yi-ji-suan-da-hui/"/>
      <url>/2020/12/18/ccks-quan-guo-zhi-shi-tu-pu-yu-yu-yi-ji-suan-da-hui/</url>
      
        <content type="html"><![CDATA[<p>今天跟老师聊了一下毕设的方向，<strong>老师提出了两方面的改进目标</strong>：</p><ol><li><p><strong>图神经网络的方法论上进行创新</strong></p><p>从GNN的构图，表征学习等方面进行创新，这样一来，数据集不再是局限于金融数据集，可以是其他开源的数据集。</p></li><li><p><strong>金融领域出发</strong></p><p>从金融领域出发就不再只有反欺诈了，老师将毕设的选题再次扩大到整个金融领域，可以是金融领域的事件抽取，知识图谱的建立等等。</p><p>CCKS的评测任务是金融中的反欺诈任务的事件抽取，整个任务可以围绕怎么去构造知识图谱。有一个叫做基于本体的金融知识图谱自动化构建技术，将GNN只作为解决这一个问题的工具，可以对这个问题的本身怎么去建模可能要优先大于在方法论上的创新。</p></li></ol><h2 id="什么是CCKS？（CCKS主要聚焦在哪些领域？）"><a href="#什么是CCKS？（CCKS主要聚焦在哪些领域？）" class="headerlink" title="什么是CCKS？（CCKS主要聚焦在哪些领域？）"></a>什么是CCKS？（CCKS主要聚焦在哪些领域？）</h2><p><strong>全国知识图谱与语义计算大会</strong>（CCKS: China Conference on Knowledge Graph and Semantic Computing）由中国中文信息学会语言与知识计算专业委员会主办。</p><p>全国知识图谱与语义计算大会源自中文知识图谱研讨会the <em>Chinese Knowledge Graph Symposium (CKGS)<em>和中国语义互联网与Web科学大会</em>Chinese Semantic Web and Web Science Conference (CSWS)。</em>2016年两会合并。</p><p>全国知识图谱与语义计算大会已经成为国内知识图谱、语义技术、链接数据等领域的核心学术会议，聚集了<strong>知识表示</strong>、<strong>自然语言理解</strong>、<strong>知识获取</strong>、<strong>智能问答</strong>、<strong>链接数据</strong>、<strong>图数据库</strong>、<strong>图计算</strong>、<strong>自动推理</strong>等相关技术领域的和研究人员的学者和研究人员。</p><p>2020年全国知识图谱和语义计算大会(<a href="http://sigkg.cn/ccks2020/">www.sigkg.cn/ccks2020</a>) 将于2020年11月12日至11月15日在南昌召开。</p><p>社区支持：智源社区。</p><h3 id="重要信息"><a href="#重要信息" class="headerlink" title="重要信息"></a>重要信息</h3><h4 id="主会"><a href="#主会" class="headerlink" title="主会"></a>主会</h4><ul><li><a href="https://hub.baai.ac.cn/view/4155">主会视频回放和Slides下载</a></li></ul><h4 id="讲习班"><a href="#讲习班" class="headerlink" title="讲习班"></a>讲习班</h4><ul><li><strong><a href="https://hub.baai.ac.cn/view/3931">讲习班视频回放和Slides下载</a></strong></li></ul><h3 id="重要通知"><a href="#重要通知" class="headerlink" title="重要通知"></a>重要通知</h3><ul><li><p><a href="http://sigkg.cn/ccks2020/?page_id=700"><strong>评测系统论文集</strong></a></p></li><li><p><a href="http://sigkg.cn/ccks2020/?page_id=53"><strong>录用论文</strong></a></p></li><li><p><del>大会征稿截止时间延期至：<strong>2020年8月7日</strong></del></p></li><li><p>会议延期至2020年11月12-15日在南昌召开</p></li><li><p><del><a href="http://sigkg.cn/ccks2020/?page_id=53">第</a><a href="http://sigkg.cn/ccks2020/?page_id=53">二轮论文征稿已经发布，详见Calls</a></del></p></li><li><p><del><a href="http://sigkg.cn/ccks2020/?page_id=69">评测任务已经发布</a></del></p></li><li><p><del>第一轮论文征稿已经发布，详见Calls</del></p></li><li><p><del>评测任务征集已经发布，详见Calls</del></p></li></ul><h2 id="CCKS的评测任务"><a href="#CCKS的评测任务" class="headerlink" title="CCKS的评测任务"></a>CCKS的评测任务</h2><p>CCKS技术评测旨在为研究人员提供测试知识图谱与语义计算技术、算法、及系统的平台和资源，促进国内知识图谱领域的技术发展，以及学术成果与产业需求的融合和对接。<a href="http://www.ccks2019.cn/?page_id=62">CCKS2019技术评测</a>吸引了1666支队伍报名参赛，形成了较高的影响力。经过前期的评测任务征集和评测组委会筛选，CCKS 2020共设立8个相关主题评测任务，分别是：</p><ol><li>新冠知识图谱构建与问答</li><li>面向中文短文本的实体链指</li><li>面向中文电子病历的医疗实体及事件抽取</li><li><strong>面向金融领域的小样本跨类迁移事件抽取</strong></li><li><strong>面向金融领域的篇章级事件主题与要素抽取</strong></li><li><strong>基于本体的金融知识图谱自动化构建技术评测</strong></li><li>基于标题的大规模商品实体检索</li><li>面向试验鉴定的命名实体识别</li></ol><h3 id="任务四：面向金融领域的小样本跨类迁移事件抽取"><a href="#任务四：面向金融领域的小样本跨类迁移事件抽取" class="headerlink" title="任务四：面向金融领域的小样本跨类迁移事件抽取"></a><strong>任务四：面向金融领域的小样本跨类迁移事件抽取</strong></h3><p>在金融领域，事件抽取是一项十分重要的任务，也是自然语言处理领域一项比较复杂的任务，而小样本下的事件抽取模型在落地应用中也极为需要。</p><p>本任务需要从金融领域新闻资讯句子中，抽取事件知识（包括事件类型、触发词和事件元素），并将大样本下训练的模型跨类迁移到小样本的其他事件类型上。</p><p>其中，事件类型分为两类，初始事件类型限定为：质押、股份股权转让、投资、起诉和高管减持，需要迁移的事件类型为：收购、担保、中标、签署合同和判决，每个事件类型都有其对应的事件框架，需要抽取出每个事件对应的事件元素。即给出一段句子级新闻资讯文本，针对该文本需要判断其所属的事件类型，抽取该事件的各个事件元素。</p><h3 id="任务五：面向金融领域的篇章级事件主题与要素抽取"><a href="#任务五：面向金融领域的篇章级事件主题与要素抽取" class="headerlink" title="任务五：面向金融领域的篇章级事件主题与要素抽取"></a><strong>任务五：面向金融领域的篇章级事件主题与要素抽取</strong></h3><p>“事件抽取”是舆情监控领域和金融领域的重要任务之一，“事件”在金融领域是投资分析，资产管理的重要决策参考；事件也是知识图谱的重要组成部分，事件抽取是进行图谱推理、事件分析的必要过程。本次评测任务的文本范围包括互联网上的新闻文本，上市公司发布的公告文本（PDF文档已转成无结构化的文本内容）。 本次评测任务的事件类型包括：财务造假、偿付能力不足、高层失联/去世、企业破产、重大资损、重大赔付、重大事故、股权冻结、股权质押、增持、减持等。</p><p>本次评测包括两个子任务：</p><p>1）事件主体抽取：旨在从文本中抽取事件类型和对应的事件主体。即给定文本T，抽取T中所有的事件类型集合S，对于S中的每个事件类型s，从文本T中抽取s的事件主体。其中各事件类型的主体实体类型为公司名称或人名或机构名称。</p><p>2）篇章事件要素抽取：旨在从文本中抽取事件类型和对应的事件要素。即给定文本T，抽取T中所有的事件类型集合S，对于S中的每个事件类型s，从文本T中抽取s的事件要素。</p><h3 id="任务六：基于本体的金融知识图谱自动化构建技术评测"><a href="#任务六：基于本体的金融知识图谱自动化构建技术评测" class="headerlink" title="任务六：基于本体的金融知识图谱自动化构建技术评测"></a><strong>任务六：基于本体的金融知识图谱自动化构建技术评测</strong></h3><p>金融研报是各类金融研究结构对宏观经济、金融、行业、产业链以及公司的研究报告。报告通常是由专业人员撰写，对宏观、行业和公司的数据信息搜集全面、研究深入，质量高，内容可靠。报告内容往往包含产业、经济、金融、政策、社会等多领域的数据与知识，是构建行业知识图谱非常关键的数据来源。另一方面，由于研报本身所容纳的数据与知识涉及面广泛，专业知识众多，不同的研究结构和专业认识对相同的内容的表达方式也会略有差异。这些特点导致了从研报自动化构建知识图谱困难重重，解决这些问题则能够极大促进自动化构建知识图谱方面的技术进步。</p><p>本评测任务参考TAC KBP中的Cold Start评测任务的方案，围绕金融研报知识图谱的自动化图谱构建所展开。评测从预定义图谱模式（Schema）和少量的种子知识图谱开始，从非结构化的文本数据中构建知识图谱。其中图谱模式包括10种实体类型，如机构、产品、业务、风险等；19个实体间的关系，如(机构，生产销售，产品)、(机构，投资，机构)等；以及若干实体类型带有属性，如（机构，英文名）、（研报，评级）等。在给定图谱模式和种子知识图谱的条件下，评测内容为自动地从研报文本中抽取出符合图谱模式的实体、关系和属性值，实现金融知识图谱的自动化构建。所构建的图谱在大金融行业、监管部门、政府、行业研究机构和行业公司等应用非常广泛，如风险监测、智能投研、智能监管、智能风控等，具有巨大的学术价值和产业价值。</p><p>评测本身不限制各参赛队伍使用的模型、算法和技术。希望各参赛队伍发挥聪明才智，构建各类无监督、弱监督、远程监督、半监督等系统，迭代的实现知识图谱的自动化构建，共同促进知识图谱技术的进步。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://sigkg.cn/ccks2020/">2020全国知识图谱与语义计算大会</a></li></ol><h1 id="基于本体的金融知识图谱自动化构建技术评测"><a href="#基于本体的金融知识图谱自动化构建技术评测" class="headerlink" title="基于本体的金融知识图谱自动化构建技术评测"></a>基于本体的金融知识图谱自动化构建技术评测</h1><p>输入：文本<br>输出：关系和属性抽取</p><p>本评测任务围绕从金融研报自动化构建知识图谱所展开：</p><ul><li>给定：预定义图谱模式（Schema，本体）</li><li>给定：种子知识图谱开始</li><li>给定：金融研报的文本，经过人工处理过的txt格式</li><li>要求：选手实现自动化构建图谱的算法、模型和软件</li><li>要求包括：实体抽取</li><li>要求包括：关系和属性抽取</li><li>要求包括：实体合并和对齐</li><li>期望：迁移学习、无监督或弱监督、远程监督等</li><li>期望：多用算法少用规则</li></ul><h2 id="数据集：schema"><a href="#数据集：schema" class="headerlink" title="数据集：schema"></a>数据集：schema</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscz1ujspj23341qi7wh.jpg"><br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscyz4ia8j23341qinpd.jpg"><br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glsczh7u6kj23341qiu0y.jpg"><br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscz66jugj23341qinpd.jpg"><br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscz77cx7j23341qinpd.jpg"><br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscy2ojt4j213e0jjkex.jpg"></p><p>上图是知识图谱涉及的3个方面：实体，关系和属性。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscz3f6kpj23341qi7wh.jpg" alt="121822001478_0任务6基于本体的金融知识图谱自动化构建技术评测评测总台报告_7.jpeg"></p><p>关系三元组是指：（实体，关系，实体）</p><p>属性三元组是指：（属性，属性名，属性值）</p><h2 id="评测标准"><a href="#评测标准" class="headerlink" title="评测标准"></a>评测标准</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscz7idx4j23341qiqv5.jpg" alt="121822001478_0任务6基于本体的金融知识图谱自动化构建技术评测评测总台报告_8.jpeg"></p><h2 id="任务价值"><a href="#任务价值" class="headerlink" title="任务价值"></a>任务价值</h2><p>技术价值与业务价值两个方面。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscz8ov9fj23341qinpd.jpg"></p><h2 id="报名参赛情况"><a href="#报名参赛情况" class="headerlink" title="报名参赛情况"></a>报名参赛情况</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscz812cpj23341qib29.jpg"></p><h2 id="获奖情况"><a href="#获奖情况" class="headerlink" title="获奖情况"></a>获奖情况</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glsczagamdj23341qie81.jpg"></p><h2 id="获奖技术方案总结：实体抽取"><a href="#获奖技术方案总结：实体抽取" class="headerlink" title="获奖技术方案总结：实体抽取"></a>获奖技术方案总结：实体抽取</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glsczc3dlqj23341qihdt.jpg"></p><p>实体抽取中最基本和主流的方案：BERT+规则</p><h2 id="获奖技术方案总结：关系和属性抽取"><a href="#获奖技术方案总结：关系和属性抽取" class="headerlink" title="获奖技术方案总结：关系和属性抽取"></a>获奖技术方案总结：关系和属性抽取</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glsczebj86j23341qikjl.jpg"></p><p>实体间的共现存在着一定的问题，因为两个实体之间可以存在多种关系的。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glscze8gsaj23341qib29.jpg" alt="121822001478_0任务6基于本体的金融知识图谱自动化构建技术评测评测总台报告_14.jpeg"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glsczezfvlj23341qix6p.jpg"><br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glsczexhxaj23341qib29.jpg" alt="121822001478_0任务6基于本体的金融知识图谱自动化构建技术评测评测总台报告_16.jpeg"><br><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glsczhp9x9j23341qiu0y.jpg" alt="121822001478_0任务6基于本体的金融知识图谱自动化构建技术评测评测总台报告_17.jpeg"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>【论文复现】Graph Attention Networks</title>
      <link href="/2020/12/17/lun-wen-fu-xian-graph-attention-networks/"/>
      <url>/2020/12/17/lun-wen-fu-xian-graph-attention-networks/</url>
      
        <content type="html"><![CDATA[<p>对照PyG官方文档，源码和examples对GAT进行了代码学习。</p><h2 id="API文档"><a href="#API文档" class="headerlink" title="API文档"></a>API文档</h2><p>首先来看一下，PyG官方文档定义的<code>GATConv</code>API：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glr4t5qlcjj20pi11ljyx.jpg" alt="image.png"></p><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><pre><code class="python">from typing import Union, Tuple, Optionalfrom torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,                                    OptTensor)import torchfrom torch import Tensorimport torch.nn.functional as Ffrom torch.nn import Parameter, Linearfrom torch_sparse import SparseTensor, set_diagfrom torch_geometric.nn.conv import MessagePassingfrom torch_geometric.utils import remove_self_loops, add_self_loops, softmaxfrom ..inits import glorot, zeros# 继承自PyG独有的MessagePassing父类，用于实现消息传递。class GATConv(MessagePassing):    r"""The graph attentional operator from the `"Graph Attention Networks"    &lt;https://arxiv.org/abs/1710.10903&gt;`_ paper    .. math::        \mathbf{x}^{\prime}_i = \alpha_{i,i}\mathbf{\Theta}\mathbf{x}_{i} +        \sum_{j \in \mathcal{N}(i)} \alpha_{i,j}\mathbf{\Theta}\mathbf{x}_{j},    where the attention coefficients :math:`\alpha_{i,j}` are computed as    .. math::        \alpha_{i,j} =        \frac{        \exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}        [\mathbf{\Theta}\mathbf{x}_i \, \Vert \, \mathbf{\Theta}\mathbf{x}_j]        \right)\right)}        {\sum_{k \in \mathcal{N}(i) \cup \{ i \}}        \exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}        [\mathbf{\Theta}\mathbf{x}_i \, \Vert \, \mathbf{\Theta}\mathbf{x}_k]        \right)\right)}.    Args:        in_channels (int or tuple): Size of each input sample. A tuple            corresponds to the sizes of source and target dimensionalities.        out_channels (int): Size of each output sample.        heads (int, optional): Number of multi-head-attentions.            (default: :obj:`1`)        concat (bool, optional): If set to :obj:`False`, the multi-head            attentions are averaged instead of concatenated.            (default: :obj:`True`)        negative_slope (float, optional): LeakyReLU angle of the negative            slope. (default: :obj:`0.2`)        dropout (float, optional): Dropout probability of the normalized            attention coefficients which exposes each node to a stochastically            sampled neighborhood during training. (default: :obj:`0`)        add_self_loops (bool, optional): If set to :obj:`False`, will not add            self-loops to the input graph. (default: :obj:`True`)        bias (bool, optional): If set to :obj:`False`, the layer will not learn            an additive bias. (default: :obj:`True`)        **kwargs (optional): Additional arguments of            :class:`torch_geometric.nn.conv.MessagePassing`.    """    _alpha: OptTensor    def __init__(self, in_channels: Union[int, Tuple[int, int]],                 out_channels: int, heads: int = 1, concat: bool = True,                 negative_slope: float = 0.2, dropout: float = 0.,                 add_self_loops: bool = True, bias: bool = True, **kwargs):        kwargs.setdefault('aggr', 'add')        super(GATConv, self).__init__(node_dim=0, **kwargs)        self.in_channels = in_channels        self.out_channels = out_channels        self.heads = heads        self.concat = concat        self.negative_slope = negative_slope        self.dropout = dropout        self.add_self_loops = add_self_loops        #判断输入的in_channels是int类型还是tuple类型        if isinstance(in_channels, int):            #如果是int类型，直接同时对l和r执行输入为in_channels，输出为heads * out_channels的单层MLP            self.lin_l = Linear(in_channels, heads * out_channels, bias=False)            self.lin_r = self.lin_l        else:            self.lin_l = Linear(in_channels[0], heads * out_channels, False)            self.lin_r = Linear(in_channels[1], heads * out_channels, False)        # attention向量        self.att_l = Parameter(torch.Tensor(1, heads, out_channels))        self.att_r = Parameter(torch.Tensor(1, heads, out_channels))        if bias and concat:            #为输出增加转置            self.bias = Parameter(torch.Tensor(heads * out_channels))        elif bias and not concat:            self.bias = Parameter(torch.Tensor(out_channels))        else:            self.register_parameter('bias', None)        self._alpha = None        self.reset_parameters()        def reset_parameters(self):        glorot(self.lin_l.weight)        glorot(self.lin_r.weight)        glorot(self.att_l)        glorot(self.att_r)        zeros(self.bias)    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,                size: Size = None, return_attention_weights=None):        # type: (Union[Tensor, OptPairTensor], Tensor, Size, NoneType) -&gt; Tensor  # noqa        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, NoneType) -&gt; Tensor  # noqa        # type: (Union[Tensor, OptPairTensor], Tensor, Size, bool) -&gt; Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa        # type: (Union[Tensor, OptPairTensor], SparseTensor, Size, bool) -&gt; Tuple[Tensor, SparseTensor]  # noqa        r"""        Args:            return_attention_weights (bool, optional): If set to :obj:`True`,                will additionally return the tuple                :obj:`(edge_index, attention_weights)`, holding the computed                attention weights for each edge. (default: :obj:`None`)        """        H, C = self.heads, self.out_channels        x_l: OptTensor = None        x_r: OptTensor = None        alpha_l: OptTensor = None        alpha_r: OptTensor = None        if isinstance(x, Tensor):            assert x.dim() == 2, 'Static graphs not supported in `GATConv`.'            x_l = x_r = self.lin_l(x).view(-1, H, C)            alpha_l = (x_l * self.att_l).sum(dim=-1)            alpha_r = (x_r * self.att_r).sum(dim=-1)        else:            x_l, x_r = x[0], x[1]            assert x[0].dim() == 2, 'Static graphs not supported in `GATConv`.'            x_l = self.lin_l(x_l).view(-1, H, C)            alpha_l = (x_l * self.att_l).sum(dim=-1)            if x_r is not None:                x_r = self.lin_r(x_r).view(-1, H, C)                alpha_r = (x_r * self.att_r).sum(dim=-1)        assert x_l is not None        assert alpha_l is not None        if self.add_self_loops:            if isinstance(edge_index, Tensor):                num_nodes = x_l.size(0)                if x_r is not None:                    num_nodes = min(num_nodes, x_r.size(0))                if size is not None:                    num_nodes = min(size[0], size[1])                edge_index, _ = remove_self_loops(edge_index)                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)            elif isinstance(edge_index, SparseTensor):                edge_index = set_diag(edge_index)        # propagate_type: (x: OptPairTensor, alpha: OptPairTensor)        out = self.propagate(edge_index, x=(x_l, x_r),                             alpha=(alpha_l, alpha_r), size=size)        alpha = self._alpha        self._alpha = None        if self.concat:            out = out.view(-1, self.heads * self.out_channels)        else:            out = out.mean(dim=1)        if self.bias is not None:            out += self.bias        if isinstance(return_attention_weights, bool):            assert alpha is not None            if isinstance(edge_index, Tensor):                return out, (edge_index, alpha)            elif isinstance(edge_index, SparseTensor):                return out, edge_index.set_value(alpha, layout='coo')        else:            return out    def message(self, x_j: Tensor, alpha_j: Tensor, alpha_i: OptTensor,                index: Tensor, ptr: OptTensor,                size_i: Optional[int]) -&gt; Tensor:        alpha = alpha_j if alpha_i is None else alpha_j + alpha_i        alpha = F.leaky_relu(alpha, self.negative_slope)        alpha = softmax(alpha, index, ptr, size_i)        self._alpha = alpha        alpha = F.dropout(alpha, p=self.dropout, training=self.training)        return x_j * alpha.unsqueeze(-1)    def __repr__(self):        return '{}({}, {}, heads={})'.format(self.__class__.__name__,                                             self.in_channels,                                             self.out_channels, self.heads)</code></pre><ul><li><code>propagate()</code>函数中<code>x=(x_l,x_r)</code>和<code>alpha=(alpha_l,alpha_r)</code>，<code>l</code>表示起点，<code>r</code>表示终点，表明当一个顶点是起点或者终点时，它的<code>x</code>和<code>alpha</code>是不一样的。</li><li><code>message()</code>函数中对注意力系数进行了<code>dropout</code>操作，这是在论文里面有提到的；</li></ul><h2 id="实验：Cora数据集"><a href="#实验：Cora数据集" class="headerlink" title="实验：Cora数据集"></a>实验：Cora数据集</h2><pre><code class="python"># -*- coding:utf-8 -*-import os.path as ospimport torchimport torch.nn.functional as Ffrom torch_geometric.datasets import Planetoidimport torch_geometric.transforms as Tfrom torch_geometric.nn import GATConvdataset = 'Cora'path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', dataset)#使用了transform，作用是以行为单位，对特征进行归一化dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())data = dataset[0]class Net(torch.nn.Module):    def __init__(self):        super(Net, self).__init__()        #下面的配置都是按照原论文的要求配置的        self.conv1 = GATConv(dataset.num_features, 8, heads=8, dropout=0.6)        # On the Pubmed dataset, use heads=8 in conv2.        self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=False,                             dropout=0.6)    def forward(self):        #在使用F.dropout的时候一定要记得设置training=self.training，否则会在model.eval()时出现问题        x = F.dropout(data.x, p=0.6, training=self.training)    #论文中要求的每次进入GATconv之前需要dropout        x = F.elu(self.conv1(x, data.edge_index))   #论文中要求的使用exponential linear unit(ELU)        x = F.dropout(x, p=0.6, training=self.training) #论文中要求的每次进入GATconv之前需要dropout        x = self.conv2(x, data.edge_index)        return F.log_softmax(x, dim=1)device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')model,data = Net().to(device),data.to(device)optimizer = torch.optim.Adam(model.parameters(),lr=0.005,weight_decay=5e-4)def train():    model.train()    optimizer.zero_grad()    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()    optimizer.step()def test():    model.eval()    logits, accs = model(), []    for _, mask in data('train_mask', 'val_mask', 'test_mask'): #访问类的属性很奇特的方式        pred = logits[mask].max(1)[1]        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()        accs.append(acc)    return accsfor epoch in range(1, 201):    train()    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'    print(log.format(epoch, *test()))</code></pre><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><h3 id="反思1：关于log-softmax-，nll-loss-，cross-entropy-和binary-cross-entropy-with-logits"><a href="#反思1：关于log-softmax-，nll-loss-，cross-entropy-和binary-cross-entropy-with-logits" class="headerlink" title="反思1：关于log_softmax()，nll_loss()，cross_entropy()和binary_cross_entropy_with_logits()"></a>反思1：关于<code>log_softmax()</code>，<code>nll_loss()</code>，<code>cross_entropy()</code>和<code>binary_cross_entropy_with_logits()</code></h3><p><code>torch.nn.functional.log_softmax</code>(<em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em>)</p><ul><li>Applies a softmax followed by a logarithm.</li><li><strong>While mathematically equivalent to log(softmax(x))</strong>, doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.</li></ul><p><code>torch.nn.functional.nll_loss</code>(<em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction=’mean’</em>)</p><ul><li><strong>The negative log likelihood loss.</strong></li><li>The input given through a forward call is expected to contain log-probabilities of each class. inputhas to be a Tensor of size either $(minibatch, C)$ or $(minibatch, C, d_1, d_2, …, d_K)$with $K \geq 1$ for the K-dimensional case (described later).</li><li>Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.</li><li>The target that this loss expects should be a class index in the range [0, C-1][0,<em>C</em>−1] where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range).</li></ul><p>综上，其实<code>log_softmax()</code>和<code>nll_loss()</code>可以直接由<code>cross_entropy()</code>代替：</p><ul><li>This criterion combines <code>log_softmax</code> and <code>nll_loss</code> in a single function.</li></ul><p>注意区分<code>cross_entropy()</code>和<code>binary_cross_entropy_with_logits()</code>，两者的区别在于<code>log</code>里面是<code>sigmoid</code>还是<code>softmax</code>函数。</p><h3 id="反思2：访问类的属性"><a href="#反思2：访问类的属性" class="headerlink" title="反思2：访问类的属性"></a>反思2：访问类的属性</h3><p>类的属性还能这样访问</p><pre><code class="python">for _, mask in data('train_mask', 'val_mask', 'test_mask'): #访问类的属性很奇特的方式</code></pre><h3 id="反思3：Questions-amp-Help"><a href="#反思3：Questions-amp-Help" class="headerlink" title="反思3：Questions &amp; Help"></a>反思3：Questions &amp; Help</h3><blockquote><p>这个是在PyG的Git repo上面的<a href="https://github.com/rusty1s/pytorch_geometric/issues/1851">issue</a>，刚好解决了我的疑惑。看来看issue还是非常有用的。</p></blockquote><p>I have a few questions from a newbie in PyTorch Geometric regarding the GAT model:</p><p>1/ In the <code>forward</code> method, we call <code>propagate</code> as follows:</p><pre><code class="python">out = self.propagate(edge_index, x=(x_l, x_r), alpha=(alpha_l, alpha_r), size=size)</code></pre><p>Should I understand that <code>x_l</code> will map to the source features (<code>x_j</code> in <code>message</code>) whereas <code>x_r</code> will map to the target features (<code>x_i</code> in <code>message</code>) ? same thing for <code>alpha</code> ?</p><p>2/ In <code>message</code>, we treat the messages on all edges of the graph (well, the graphs in the mini-batch); I have hard times understanding how we can perform a softmax here; it is as if the <code>message</code> method was dealing only with the messages to a single target (can I stick to this view though it might not be right?). How is it done under the hood?</p><p>3/ What is the difference between a <code>Tensor</code> and a <code>OptTensor</code> ?</p><p>Thanks !</p><hr><p>Hi,</p><ol><li>That’s correct. We refer to <code>x_l</code> as the source nodes, and <code>x_r</code> as the target nodes (in a bipartite graph). So for example, <code>GATConv((128, 256), 256)</code> will aggregate 128-dimensional feature vectors and combine them to 256-dimensional feature vectors.</li><li>Usually, <code>message</code> can be seen as a method that operates independently for each edge. However, in <code>GATConv</code> messages are inter-dependent (via the softmax). PyTorch Geometric provides a <code>softmax</code> function (<code>torch_geometric.utils.softmax</code>) that normalizes inputs across the same target nodes. This function is used here.</li><li><code>OptTensor</code> describes an optional tensor, i.e. a <code>Tensor</code> or <code>None</code>. This might be useful in same cases, e.g., when you have a bipartite graph for which features only exist for source nodes (and you want to obtain target node features based on aggregating incoming edges). You can then do something like: <code>conv((x_l, None), edge_index)</code>.</li></ol><h3 id="反思4：关于源码中-vec-a-T-W-vec-h-i-W-vec-h-j-的实现"><a href="#反思4：关于源码中-vec-a-T-W-vec-h-i-W-vec-h-j-的实现" class="headerlink" title="反思4：关于源码中$\vec a^{T}[W \vec h_i || W \vec h_j ]$的实现"></a>反思4：关于源码中$\vec a^{T}[W \vec h_i || W \vec h_j ]$的实现</h3><p>源码中实现的实在是太完美了！将$\vec a^{T}$拆分成<code>att_l</code>和<code>att_r</code>，这样就能完美解决二部图的问题了！实在是妙啊。我之前都没有意识到。还是没有好好地理解算法呀~</p><p>TO DO</p><ul><li>PyG的二部图思想</li></ul>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 论文复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Graph Attention Networks</title>
      <link href="/2020/12/17/graph-attention-networks/"/>
      <url>/2020/12/17/graph-attention-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>图数据是生活中常见的一种数据，它具有非欧几里得的性质。随着深度学习的发展，基于图数据的深度学习方法日趋流行，主要包括RecGNN、ConvGNN、GAEs、STGNN。</p><p>ConvGNN可以分为Spectral ConvGNN和Spatial ConvGNN两种：</p><p>Spectral ConvGNN基于图信号处理理论，需要使用图的Laplacian Matrix，，以及对其进行特征值分解。这就使得：</p><ol><li>一个图上的Spectral ConvGNN不能直接迁移到另一个图上（<em>not inductive</em>）；</li><li>特征值分解也涉及了大量的矩阵操作，增大了时间复杂度；</li></ol><p>而Spatial ConvGNN则不依赖于Laplacian Matrix，它的思想是邻居聚合，主要的研究方向包括聚合函数、采样等等。</p><p>同时，在许多sequence-based任务中，注意力机制被广泛使用。因此，Spatial ConvGNN+Attention Mechanism就成了GAT的inspiration！</p><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li><strong>计算高效性</strong>：<ul><li>self-attentional layer操作可以按edge实现并行；</li><li>message aggregation操作可以按node实现并行；</li><li>多头之间可以实现并行；</li><li>不需要复杂的矩阵运算；</li><li>a single GAT attention head的时间复杂度为：$O(|V|FF^‘+|E|F’)$</li></ul></li><li><strong>通过注意力机制，可以学习到每个邻居的importance weight</strong></li><li><strong>注意力机制不依赖于图的结构，是一种inductive的模型，具有很好的泛化性</strong></li></ol><h2 id="Model：Graph-Attentional-Layer"><a href="#Model：Graph-Attentional-Layer" class="headerlink" title="Model：Graph Attentional Layer"></a>Model：Graph Attentional Layer</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glqy9ki2ydj20l90dvacn.jpg" alt="Graph Attentional Layer"></p><p>Graph Attentionnode feature的输入为：<br>$$<br>{\bf {h}} = \lbrace \vec h_1,\vec h_2,…,\vec h_N \rbrace$，$\vec h_i \in \mathbb R^F<br>$$<br>首先，用一个共享的线性变换模块，对每个点进行处理。线性变换模块的参数为： ${\bf W} \in \mathbb R^{F’ \times F}$，则所有点的feature变成：</p><p><img src="http://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cmathbf%7BW%7D%5Cvec%7Bh%7D_%7B1%7D,+%5Cmathbf%7BW%7D%5Cvec%7Bh%7D_%7B2%7D,+%5Cldots,+%5Cmathbf%7BW%7D%5Cvec%7Bh%7D_%7BN%7D%5Cright%5C%7D,+%5Cmathbf%7BW%7D%5Cvec%7Bh%7D_%7Bi%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7BF%5E%5Cprime%7D+%5C%5C"></p><p>接下来就是常见的注意力机制操作，对节点$i$的邻居节点$j$计算注意力系数（节点$i$自己也算作自己的邻居节点）：</p><p><img src="https://www.zhihu.com/equation?tex=e_%7Bi+j%7D=a%5Cleft(%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bi%7D,+%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bj%7D%5Cright),+e_%7Bij%7D+++%5Cin+%5Cmathbb%7BR%7D%5C%5C"></p><p>用$softmax$函数对$i$的所有邻居节点的注意力系数进行归一化操作有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bi+j%7D=%5Coperatorname%7Bsoftmax%7D_%7Bj%7D%5Cleft(e_%7Bi+j%7D%5Cright)=%5Cfrac%7B%5Cexp+%5Cleft(e_%7Bi+j%7D%5Cright)%7D%7B%5Csum_%7Bk+%5Cin+%5Cmathcal%7BN%7D_%7Bi%7D%7D+%5Cexp+%5Cleft(e_%7Bi+k%7D%5Cright)%7D+%5C%5C"></p><p>具体实现时，函数$a$为：</p><p><img src="https://www.zhihu.com/equation?tex=a%5Cleft(%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bi%7D,+%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bj%7D%5Cright)+=+%5Cmathbf%7BLeakyReLU%7D%5Cleft(%5Coverrightarrow%7B%5Cmathbf%7Ba%7D%7D%5E%7BT%7D%5Cleft%5B%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bi%7D+%5C%7C+%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bj%7D%5Cright%5D%5Cright),+%5Coverrightarrow%7B%5Cmathbf%7Ba%7D%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7B2+F%5E%7B%5Cprime%7D%7D+%5C%5C"></p><p>相当于将两个点的feature拼接起来后，用一个全连接层+leakyReLU进行处理。所以：</p><p><img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bi+j%7D=%5Cfrac%7B%5Cexp+%5Cleft(%5Ctext+%7B+LeakyReLU+%7D%5Cleft(%5Coverrightarrow%7B%5Cmathbf%7Ba%7D%7D%5E%7BT%7D%5Cleft%5B%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bi%7D+%7C+%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bj%7D%5Cright%5D%5Cright)%5Cright)%7D%7B%5Csum_%7Bk+%5Cin+%5Cmathcal%7BN%7D_%7Bi%7D%7D+%5Cexp+%5Cleft(%5Ctext+%7B+LeakyReLU+%7D%5Cleft(%5Coverrightarrow%7B%5Cmathbf%7Ba%7D%7D%5E%7BT%7D%5Cleft%5B%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bi%7D+%5C%7C+%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bk%7D%5Cright%5D%5Cright)%5Cright)%7D+%5C%5C"></p><p>最后，对点 $i$的所有邻居点的feature（包含点  $i$本身），用  $\alpha$ 加权求和，并有激活函数$\sigma$处理，得到点$i$经过Graph Attention处理后的feature：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cvec%7Bh%7D_%7Bi%7D%5E%7B%5Cprime%7D=%5Csigma%5Cleft(%5Csum_%7Bj+%5Cin+%5Cmathcal%7BN%7D_%7Bi%7D%7D+%5Calpha_%7Bi+j%7D+%5Cmathbf%7BW%7D+%5Cvec%7Bh%7D_%7Bj%7D%5Cright)+%5C%5C"></p><p><strong>为增加训练过程的稳定性</strong>，可以使用 $\bf K$个Attention分支并行计算，最后将所有分支的结果拼接起来，或者求$\bf K$个分支的均值。</p><ul><li><p>拼接 $\bf K$ 个分支： </p><p><img src="https://www.zhihu.com/equation?tex=%5Cvec%7Bh%7D_%7Bi%7D%5E%7B%5Cprime%7D=%5C%7C_%7Bk=1%7D%5E%7BK%7D+%5Csigma%5Cleft(%5Csum_%7Bj+%5Cin+%5Cmathcal%7BN%7D_%7Bi%7D%7D+%5Calpha_%7Bi+j%7D%5E%7Bk%7D+%5Cmathbf%7BW%7D%5E%7Bk%7D+%5Cvec%7Bh%7D_%7Bj%7D%5Cright)"></p></li><li><p>求 $\bf K$ 个分支的均值： </p><p><img src="https://www.zhihu.com/equation?tex=%5Cvec%7Bh%7D_%7Bi%7D%5E%7B%5Cprime%7D=%5Csigma%5Cleft(%5Cfrac%7B1%7D%7BK%7D+%5Csum_%7Bk=1%7D%5E%7BK%7D+%5Csum_%7Bj+%5Cin+%5Cmathcal%7BN%7D_%7Bi%7D%7D+%5Calpha_%7Bi+j%7D%5E%7Bk%7D+%5Cmathbf%7BW%7D%5E%7Bk%7D+%5Cvec%7Bh%7D_%7Bj%7D%5Cright)"></p></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><blockquote><p>本篇论文的实验部分是值得深思的，它比较了许多知名的GNN模型。它将这些模型分为两大类：transductive和insductive。相应的数据集也分为transductive和insductive两种</p></blockquote><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><strong>Transductive learning：</strong></p><ul><li>Cora</li><li>Citeseer</li><li>Pubmed</li></ul><p><strong>Inductive learning：</strong></p><ul><li>PPI</li></ul><h3 id="State-of-the-art-methods"><a href="#State-of-the-art-methods" class="headerlink" title="State-of-the-art methods"></a>State-of-the-art methods</h3><p><strong>Transductive learning：</strong></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glqz10fey2j20lf0b0gor.jpg" alt="transductive"></p><p><strong>Inductive learning：</strong></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glr1nzpop5j20lc0agtb5.jpg" alt="inductive"></p><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><ol><li><p>下面三篇论文递进关系：</p><ul><li>Semi-Supervised Classification with Graph Convolutional Networks，ICLR 2017，图卷积网络 <a href="https://arxiv.org/abs/1609.02907">https://arxiv.org/abs/1609.02907</a></li><li>Graph Attention Networks，ICLR 2018.  图注意力网络，就是此篇文章所解析的论文 <a href="https://arxiv.org/abs/1710.10903">https://arxiv.org/abs/1710.10903</a></li><li>Relational Graph Attention Networks ，ICLR2019  关联性图注意力网络，整合了GCN+Attention+Relational</li></ul></li><li><p>原论文在介绍的时候讲得非常清楚，连Dropout的probability都介绍了，所以原论文的实验部分还是值得仔细推敲，对自己以后做实验也有用！</p></li><li><p>我之前学习的GeniePath、SemiGNN、GraphConsis在聚合邻居的时候无不例外地都使用到了GAT（GEM并没有用GAT），说明GAT基本上是聚合邻居的基础操作了，那么基于GNN的金融反欺诈应该朝着哪里研究呢？</p><p>我觉得主要<strong>从数据出发</strong>，观察数据的特性（比如数据的异质性，数据的种类）。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【论文复现】A Semi-supervised Graph Attentive Network for Financial Fraud Detection</title>
      <link href="/2020/12/16/lun-wen-fu-xian-a-semi-supervised-graph-attentive-network-for-financial-fraud-detection/"/>
      <url>/2020/12/16/lun-wen-fu-xian-a-semi-supervised-graph-attentive-network-for-financial-fraud-detection/</url>
      
        <content type="html"><![CDATA[<p>对DGFraud仓库中SemiGNN的tensorflow复现版本进行了注释理解，整理了下面的代码框架。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glpzcbzcspj22t33wh4qp.jpg" alt="SemiGNN代码框架.png"></p><blockquote><ul><li>代码在G:\SelfLearning\JupyterNotebook3.0\DGFraud_copy文件夹下</li><li>思维导图在G:\SelfLearning\JupyterNotebook3.0\GNN\GNN Papers\Financial Fraud Detection\A Semi-supervised Graph Attentive Network for Financial Fraud Detection文件夹下</li></ul></blockquote><p>后记：</p><ul><li>复现代码在view-attention level处好像有问题，需要反复思考。其他方面都挺好的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 论文复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Alleviating the Inconsistency Problem of Applying Graph Neural Netwrok to Fraud Detection</title>
      <link href="/2020/12/15/alleviating-the-inconsistency-problem-of-applying-graph-neural-netwrok-to-fraud-detection/"/>
      <url>/2020/12/15/alleviating-the-inconsistency-problem-of-applying-graph-neural-netwrok-to-fraud-detection/</url>
      
        <content type="html"><![CDATA[<blockquote><p>GraphConsis模型建立在异质信息网络之上，考虑反欺诈工作中的inconsistency问题，对GNN的Aggregation函数进行改进，综合了GraphSAGE的采样思想和GAT的注意力权重思想，将它们运用到异质信息网络中。</p><p>是一种Supervised的方法。<font color="red"><strong>需要区别Semi-supervised和transductive的区别！</strong></font>：Semi-supervised的损失函数包括supervised loss和unsupervised loss两个的结合，而transductive的损失函数只包含supervised loss！</p></blockquote><p>文章信息：</p><pre><code class="BibTex">@inproceedings{liu2020alleviating,  title={Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection},  author={Liu, Zhiwei and Dou, Yingtong and Yu, Philip S. and Deng, Yutong and Peng, Hao},  booktitle={Proceedings of the 43nd International ACM SIGIR Conference on Research and Development in Information Retrieval},  year={2020}}</code></pre><h2 id="1-Background-背景知识"><a href="#1-Background-背景知识" class="headerlink" title="1. Background 背景知识"></a>1. Background 背景知识</h2><p>随着GNN的流行，其被广泛地运用到欺诈检测领域。基于GNN的欺诈检测优点是在<strong>结合原有的Feature的基础上，引入节点与节点之间的关系</strong>，从而更好地学习Node Embedding，提供一种End-to-End的Semi-Supervised的学习方式。</p><p>GNN的核心在于Neighborhood Aggregation，Neighborhood Aggregation基于的一个假设是<strong>邻居之间的feature和label应该是similar（或者说是consistent）</strong>。例如，正是因为这个假设，我们能够用GNN进行Community Detection。</p><p>然而，在欺诈检测中，这个假设被削弱了，欺诈者往往潜伏在良性用户之中，因此，如果不进一步对邻居进行筛选，而是直接聚合的话，在认知上是不正确的，欺诈用户会因此成功伪装自己。这就是本论文要讨论的<strong>inconsistency问题</strong>：</p><p><img src="https://img-blog.csdnimg.cn/20201215145749262.png"></p><h3 id="Context-Inconsistency"><a href="#Context-Inconsistency" class="headerlink" title="Context Inconsistency"></a>Context Inconsistency</h3><p>如上图，聪明的欺诈者可以将自己与良性用户联系起来，$v_1$作为fraudster，它的邻居中包含了良性用户$v_4,v_6,v_7$，如果在邻居聚合过程中，聚合了它们的信息，无疑会使得欺诈检测的难度变大。</p><p>因此，要想办法在聚合的时候，筛去这些context inconsistency的邻居。</p><h3 id="Feature-Inconsistency"><a href="#Feature-Inconsistency" class="headerlink" title="Feature Inconsistency"></a>Feature Inconsistency</h3><p>举一个例子，假设现在进行review fraud detection，一个user发表了两条review，因此我们将两条review之间连一条边（因为它们来自同一用户），两条review因此成了邻居，然而它们的内容可能天差地别，如果在邻居聚合过程中，聚合了它们的信息，无疑会使得欺诈检测的难度变大。</p><p>例如，上图中$v_1$与$v_4,v_6,v_7$。（这个例子不太好，即使都是良性也有可能Feature Inconsistency的）</p><h3 id="Relation-Inconsistency"><a href="#Relation-Inconsistency" class="headerlink" title="Relation Inconsistency"></a>Relation Inconsistency</h3><p>如上图，在Relation Ⅱ下，fraudster更易与fraudster相连，这说明，Relation Ⅱ对欺诈检测来说贡献更高，我们更希望聚合来自Relation Ⅱ的邻居信息。</p><h2 id="2-Contributions-贡献"><a href="#2-Contributions-贡献" class="headerlink" title="2. Contributions 贡献"></a>2. Contributions 贡献</h2><p><img src="https://img-blog.csdnimg.cn/20201215151829686.png"></p><p>为了解决3个inconsistency的问题，论文提出GraphConsis模型，对应的解决办法如下：</p><ol><li><strong>context inconsistency</strong>：to handle the context inconsistency of neighbors, GraphConsis assigns each node a trainable context embedding, which is illustrated as the gray block aside nodes in Figure（Right）</li><li><strong>feature inconsistency</strong>：Secondly, to aggregate consistent neighbor embeddings, we design a new metric to measure the embedding consistency between nodes. By incorporating the embedding consistency score into the aggregation process, we ignore the neighbors with a low consistency score （e.g. the node v4 is dropped in Figure （Right）） and generate the sampling probability.</li><li><strong>relation inconsistency</strong>：Last but not least, we learn relation attention weights associated with<br>neighbors in order to alleviate the relation inconsistency problem.</li></ol><p>本论文的贡献：</p><ol><li>第一个提出GNN based fraud detection中存在inconsistency问题，并且提出解决办法的；</li><li>通过实验验证了3种inconsistency问题对GNN based fraud detection的结果影响；（见@4. 实验）</li><li>提出GraphConsis模型，模型通过context embedding，neighborhood sampling，relational attention三种方法，解决了3种inconsistency问题。</li></ol><h2 id="3-Model：GraphConsis⭐"><a href="#3-Model：GraphConsis⭐" class="headerlink" title="3. Model：GraphConsis⭐"></a>3. Model：GraphConsis⭐</h2><h3 id="Model-Illustration"><a href="#Model-Illustration" class="headerlink" title="Model Illustration"></a>Model Illustration</h3><p><img src="https://img-blog.csdnimg.cn/20201215153623731.png"></p><h3 id="Context-Embedding"><a href="#Context-Embedding" class="headerlink" title="Context Embedding"></a>Context Embedding</h3><p>为了解决context inconsistency的问题，在模型的第一层，引入一个<strong>trainable context embedding</strong> ${\bf {c}}_v$ for every node：</p><p>$$<br>{\bf {h_v}}^{(1)} = \lbrace {\bf {x_v}} || {\bf {c_v}}  \rbrace \oplus AGG^{(1)}(\lbrace {\bf {x_{v’}}} || {\bf {c_{v’}}} : v \in {\mathcal {N_v}} \rbrace)<br>$$</p><p>论文中说，这个context embedding能够代表节点的local structure，至于为什么代表我就不清楚了。而且怎么对于未知的样本生成 ${\bf {c}}_v$也是一个问题。</p><h3 id="Neighbor-Sampling"><a href="#Neighbor-Sampling" class="headerlink" title="Neighbor Sampling"></a>Neighbor Sampling</h3><p>为了能够聚合更加相关的邻居，首先定义了邻居之间的<strong>consistency score</strong>：</p><p>$$<br>s^{(l)} (u,v)= \exp (-|| {\bf {h_{u}}}^{(l)} -{\bf {h_{v}}}^{(l)}    ||_{2}^{2})<br>$$</p><p>然后设定一个阈值$\epsilon$，筛选掉那么consistency score低于$\epsilon$的邻居。</p><p>对于剩下来的邻居$\tilde {\mathcal {N}} _v$，根据它们的consistency score计算它们被采样的概率：</p><p>$$<br>p^{(l)} {(u;v)} = s^{(l)} {(u,v)} / \sum_{u \in \tilde {\mathcal {N}} _v}  s^{(l)} {(u,v)}<br>$$</p><p>每一层都有重新计算采样概率。</p><h3 id="Relation-Attention"><a href="#Relation-Attention" class="headerlink" title="Relation Attention"></a>Relation Attention</h3><p>我们使用一个向量${\bf {t}} _r$表示关系类型$r$，然后使用这个向量得到每个neighbor的注意力分数：</p><p><img src="https://img-blog.csdnimg.cn/20201215185343957.png"></p><p>其中$r_q$表示第$q$个采样样本连接到当前节点的relation类型。</p><p>得到注意力分数之后就可以直接进行“加权”聚合了：</p><p><img src="https://img-blog.csdnimg.cn/20201215185423793.png"></p><blockquote><p>因此，Relation Attention是寄托在邻居的消息聚合之上的，并不是独立的。</p></blockquote><h2 id="4-Experiments-实验"><a href="#4-Experiments-实验" class="headerlink" title="4. Experiments 实验"></a>4. Experiments 实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>数据集名称：<strong>YelpChi spam review dataset</strong><br>这个数据集包含Yelp上酒店和餐厅的评论</p><p>预处理数据集包括：</p><ul><li>29431 users</li><li>182 products</li><li>45954 reviews</li></ul><h3 id="构图⭐"><a href="#构图⭐" class="headerlink" title="构图⭐"></a>构图⭐</h3><p>review作为图中的node，根据下面三种关系构建node之间的边：</p><ul><li>R-U-R：两条review来自同一个user；</li><li>R-S-R：两条review来自同一product下的同一rating；</li><li>R-T-R：两条review来自同一product下的同一月份;</li></ul><p>node（即review）的初始特征$\bf x$是使用Word2Vec提取出来的100维的vector。</p><h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><p><strong>a spam review classification，一个二分类问题。</strong></p><h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><ul><li>Cora</li><li>PPI</li><li>Reddit</li></ul><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><ul><li>Logistic Regression</li><li>FdGars</li><li>GraphSAGE</li><li>Player2Vec</li></ul><h3 id="度量指标"><a href="#度量指标" class="headerlink" title="度量指标"></a>度量指标</h3><ul><li>F1-score</li><li>AUC</li></ul><h3 id="The-Inconsistency-Problem"><a href="#The-Inconsistency-Problem" class="headerlink" title="The Inconsistency Problem"></a>The Inconsistency Problem</h3><p>下面是关于Benchmark和YelpChi数据集的统计信息，除了统计Nodes和Edges，我们还统计了$\gamma^{(f)}$和$\gamma^{(c)}$。</p><p><img src="https://img-blog.csdnimg.cn/20201215194808619.png"></p><ol><li>context characteristic score</li></ol><p><img src="https://img-blog.csdnimg.cn/20201215194629194.png"></p><ol start="2"><li>feature characteristic score</li></ol><p><img src="https://img-blog.csdnimg.cn/20201215194705239.png"></p><p>Benckmark（Cora，PPI，Reddit）数据集并不是反欺诈数据，所以它们很难体现我们之前说的3种Inconsistency问题。</p><p>使用上面的两个指标，我们可以来讨论在真实的反欺诈研究中，数据集体现的Inconsistency问题：</p><ol><li><strong>Context Inconsistency</strong>：如Table 1所见，除了R-U-R关系集，其他的三个真实数据集都体现了极低的$\gamma^{(c)}$；</li><li><strong>Feature Inconsistency</strong>：观察$\gamma^{(f)}$，发现结果都挺高的；</li><li><strong>Relation Inconsistency</strong>：还是如Table 1所示，不同的Relation导致了不同的$\gamma^{(c)}$和$\gamma^{(f)}$；</li></ol><h3 id="Performance-Evaluation"><a href="#Performance-Evaluation" class="headerlink" title="Performance Evaluation"></a>Performance Evaluation</h3><p><img src="https://img-blog.csdnimg.cn/20201215200126610.png"></p><p>结果分析：</p><ol><li><p>GraphConsis在训练样本为60%和80%的时候，才能体现更优秀的结果；</p></li><li><p>一个更有趣的现象是，传统的LR居然比GNN的性能更好😨为什么捏？</p><p>可能正是因为存在着Inconsistency的问题，导致这些GNN方法在Aggregation的过程中聚合了很多无用的信息；</p></li><li><p>Player2Vec，FdGars和GraphSAGE的结果表明，GraphConsis的neighbor sampling和relation attention的作用。</p></li></ol><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><ol><li><p><strong>可以结合SemiGNN和GraphConsis</strong></p><p>考虑到在SemiGNN的feature work中有提到要考虑不同的relation信息，因此可以引入GraphConsis的思想。关于relation的影响，也可以继续从GNN based HIN中进行探索。</p></li><li><p>GraphConsis中提到了自己的feature work</p></li></ol><ul><li>Future work includes devising an adaptive sampling threshold for each relation to maximize the receptive field of GNNs.（<strong>这个可以考虑GeniePath，使用循环神经网络来构造感受野</strong>）</li><li>Investigating the inconsistency problems under other fraud datasets is another avenue of future research.（比如我的任务——金融反欺诈）</li></ul>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>硬核 | 谈一谈逆势而上的图神经网络技术（附干货资料）</title>
      <link href="/2020/12/14/ying-he-tan-yi-tan-ni-shi-er-shang-de-tu-shen-jing-wang-luo-ji-zhu-fu-gan-huo-zi-liao/"/>
      <url>/2020/12/14/ying-he-tan-yi-tan-ni-shi-er-shang-de-tu-shen-jing-wang-luo-ji-zhu-fu-gan-huo-zi-liao/</url>
      
        <content type="html"><![CDATA[<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9FeFBQS1hnTlZ0ZUJrbkdSd0hMVDBpYzE2ejQ1akN2VGNTRGp1R2RQOVVpYjU0UzB4eGt5aHlpYjJXSW9BUHliQThaaEVOaFI3aWJ6ckJwaGZKd1ZHRW1rNlEvNjQw?x-oss-process=image/format,png" alt="img"></p><p>问一问近几年来逆势而上的技术有什么？相信你一定会说出来一个：<strong>图神经网络</strong>。</p><p>没错，图神经网络就像曾经的微信, python, pytorch, 他们从我们视线的边缘逐渐走向我们视野的中心，并成为很多人的首选。图神经网络目前还没有完全成为各大顶会的焦点，但不可否认，它将会。相信在后面的研究中，图神经网络在人工智能领域会起着举足轻重的作用，2017年你没有关注到图神经网络，那么2020年了就不要错过了。相对于一般的神经网络，图神经网络能够解决更多更有效的问题，图神经网络也可以与自然语言处理，计算机视觉，强化学习，甚至是最基本的缺失值补全这样的机器学习任务结合（见NeurIPS‘20’ Jiaxuan You et.al），简单的说GNN有下面的特点：</p><ul><li>图神经网络能够实现强大的非结构学习的能力：GNN能够从非结构化数据（例如:场景图片、故事片段等）中进行学习和推理。尽管传统的深度学习方法被应用在提取欧氏空间数据的特征方面取得了巨大的成功，但许多实际应用场景中的数据是从非欧式空间生成的，传统的深度学习方法在处理非欧式空间数据上的表现却仍难以使人满意。</li><li>图神经网络可以学习时序以及非时序排序的特征学习：GNN的输出不以节点的输入顺序为转移的。在时空领域的建模，我们既可以关注无先后顺序的空间拓扑结构，也可以建模有顺序的时间维度，多一维度的信息，多一份决策的能力；</li><li>图神经网络可以解决两个节点之间依赖关系的学习：传统的神经网络中，这种依赖关系只能通过节点的特征来体现。图嵌入通过保留图的网络拓扑结构和节点内容信息，将图中顶点表示为低维向量，以便使用简单的机器学习算法（例如，支持向量机分类）进行处理。</li></ul><p><strong>图神经网络在社交网络、广告推荐、搜索推荐、药物生成、智能体交互、高能物理学到社会科学和经济学等领域的复杂关系建模和互动系统构建起到重要作用。</strong>今天，我要给大家介绍一个长期关注图神经网络，干货满满的公众号，<strong>深度学习与图网络。</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9FeFBQS1hnTlZ0ZDFvR3pZRkkyOG1mN3lwZzlLcmljelZablQ2eVRtMkNnbFVYZWQwOHZKNktJT2xGUHc3UWljOWhhWHMzTmhGSWNRaWF4OW9qUHhaZE9PQS82NDA?x-oss-process=image/format,png" alt="img"></p><p>它是由来自中国科学院大学，香港中文大学，上海交通大学等几位科研一线的在读博士研究生生科研之余写的（其中一位同学主要负责，非学校官方账号，科研兴趣），主要关注图神经网络，以及深度学习，机器学习相关内容。该公众号的主要目的是关注图神经网络，机器学习的最新研究进展和动态，偶尔尝试解读一些有意思的研究工作，但水平有限，欢迎来批评指正。在研究方向上， 公众号关注以下几个方面：</p><ul><li><strong>图与自然语言处理</strong></li><li><strong>图与计算机视觉</strong></li><li><strong>图与智慧交通</strong></li><li><strong>图与推荐</strong></li><li><strong>图优化与解决过平滑问题</strong></li><li><strong>图上节点分类</strong></li><li><strong>图池化与分类以及分子图生成</strong></li><li><strong>动态图,时空图嵌入</strong></li></ul><p><strong>在研究动态上</strong>：<strong>公众号关注NeurIPS, ICML, KDD， CVPR等会议论文，TPAMI, TNNLS, TKDE, TKDD等期刊论文，以及关注Arxiv上与图网络相关的研究动态，同时关注GitHub上关于GNN 库.</strong></p><p>如果你也是上面研究方向的同学，欢迎关注与收藏，据了解该公众号<strong>已组建多个图网络学习群</strong>，同时也组织了多次<strong>线上的图论文研讨活动</strong>，<strong>以及CS224W课程学习的活动</strong>。目前正在进行的是IJCAI2020图论文打卡活动，后期会进行KDD2020，NeurIPS2020图论文打卡活动，如果你也有兴趣，欢迎关注一波，后期会在公众号发布相关的信息。</p><p>如果你还没有入门，不用着急，站在他人的肩膀上，你会看的轻松一点，欢迎参考从下面的学习路线</p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><p><strong>1 Tutorial教程合集（入门必读）</strong></p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6b0dcd0de21b80bb4283f39c404d7b37069866a56048d3552ae702fe7140cc14a22e8d29&amp;idx=1&amp;mid=2247484484&amp;scene=21&amp;sn=5eec159195a8df164575d6498cb953d0#wechat_redirect">为什么要进行图嵌入表示?</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6bc5cd0de2d38d29e67b50bb691953693e4147f54bf4c61bf17e40dd94c426c21d08e04e&amp;idx=4&amp;mid=2247484556&amp;scene=21&amp;sn=18852da4be82fb4f701d4f6ba42f5d63#wechat_redirect">图嵌入表示</a>:<a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6bc5cd0de2d38d29e67b50bb691953693e4147f54bf4c61bf17e40dd94c426c21d08e04e&amp;idx=4&amp;mid=2247484556&amp;scene=21&amp;sn=18852da4be82fb4f701d4f6ba42f5d63#wechat_redirect">深度游走（Deepwalk）</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6badcd0de2bb552df2847c4b8760da29c51c50143f704ab53a83ce0498c55dd4ceb08040&amp;idx=1&amp;mid=2247484644&amp;scene=21&amp;sn=01e415014e3ad877a19f67ad629f9e23#wechat_redirect">图系列|从图(Graph)到图卷积: 漫谈图神经网络模型(I)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a41cd0de3576c81426ea124804d0152ae79d2766132ebe7560bc5279379fe6b5c9f9b4d&amp;idx=1&amp;mid=2247484680&amp;scene=21&amp;sn=c196834dc58ea353f3040cc68d62310d#wechat_redirect">图系列|从图(Graph)到图卷积: 漫谈图神经网络模模型(II)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a58cd0de34e44bec470dbf8d04cf141b051ac493b1bf0b7856b55884a3f24d41eedeb93&amp;idx=1&amp;mid=2247484689&amp;scene=21&amp;sn=10d71e5a8f2efc6033288208ad2b8aaf#wechat_redirect">图系列|从图(Graph)到图卷积: 漫谈图神经网络模模型(III)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6ab1cd0de3a7cf2ad132f0c948ec6b28889274ee9e85838998ebc0a59c7771524fd07030&amp;idx=1&amp;mid=2247484920&amp;scene=21&amp;sn=27c661c52789dde2ec7cf40b119372e4#wechat_redirect">图卷积神经网络Graph Convolutional Network(GCN)：从问题到理论分析</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a695fcd0de049f9ac04724a4fe7b25d8a3b51bc6fe4fd155bdc3ee8798523f8835cf53777&amp;idx=1&amp;mid=2247484950&amp;scene=21&amp;sn=88efa0a10fea109676aa314ac8159a62#wechat_redirect">图网络延伸GraphSage: Inductive learning 和 Transductive learning</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6903cd0de0150cce16f4e974eec05caf1d197e90a3ad193e1dc7619ed21688582ef4d61e&amp;idx=1&amp;mid=2247485002&amp;scene=21&amp;sn=27cb326941746ee7c748b3391489da12#wechat_redirect">图注意机制GAT：三种注意力机制在图神经网络中的应用和总结</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a685acd0de14c3059b5279f72bda5a74fd8889c9ce8c2d875ed32cf385f155c01b8911b65&amp;idx=2&amp;mid=2247485203&amp;scene=21&amp;sn=7493fa5a0fc05f443b026a6a902fd555#wechat_redirect">SGC Networks，一种简化的图神经网络=&gt;产生高达两个数量级的加速</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a698acd0de09cbab549858911265493c17ead00152a4f56ba0a81957b9c151e9cf30c9afb&amp;idx=1&amp;mid=2247485123&amp;scene=21&amp;sn=64cff98f39a0f231362ae08975f7aa5b#wechat_redirect">图系列GIN|GNN模型到底有多强呢？Weisfeiler-Leman test来告诉你</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a63cd0de37594971fd35556debb3ea7a9cf64e9fb9f58689b135e8dee18c0ca6a7a7cc0&amp;idx=2&amp;mid=2247484714&amp;scene=21&amp;sn=4278c3414006bc467e191fc3b057a602#wechat_redirect">斯坦福Jure Leskovec——图神经网络研究最新进展（附下载链接）</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6bf8cd0de2ee071da1ac81401607a4a76dc06f891aba38756d94cf68c3881b341415bace&amp;idx=2&amp;mid=2247484593&amp;scene=21&amp;sn=82feac250fc56eb24ee5ce30c7030dff#wechat_redirect">清华大学唐杰——图神经网络 (GNN) 算法及其应用138页PPT（附下载链接）</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a643ccd0ded2a3b9d50f034ef8a8c21f4449e87b4b367a4171d9c5a1032b0dd5a8e8706ba&amp;idx=1&amp;mid=2247486325&amp;scene=21&amp;sn=d28cb627903dc6df23004fca64c762b0#wechat_redirect">斯坦福大学Jure Leskovec—图神经网络GNN研究进展：表达性、预训练、OGB，71页ppt</a></li></ul><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9FeFBQS1hnTlZ0ZjN2WU1pYmJzYXdOQ1dMcXNmSFJpYjg3bklPc2VWcjQ2QVhDdHQ4aWI4TG5tTTJoV1FLa29WMzAzTjdpYU9oc0NpY3hVVmI3a0pTTTdqMW5nLzY0MA?x-oss-process=image/format,png" alt="img"></p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><p><strong>2 深度思考（进阶）</strong></p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a31cd0de3273243088a5428af78c020a687d672612f543156802735ccd4f2934bdfddf9&amp;idx=2&amp;mid=2247484792&amp;scene=21&amp;sn=a1b8562328cdbfbe6c67785e3cf0f5cd#wechat_redirect">GNN需要预训练吗？| GNN教程：图上的预训练任务上篇</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6982cd0de094656ba55fd5305a77cc961526e5fe5e084acba2583b918319b675ae4d2dd6&amp;idx=1&amp;mid=2247485131&amp;scene=21&amp;sn=8056891ddc4075aaebe8211b72adc894#wechat_redirect">GNN需要预训练吗？| GNN教程：图上的预训练任务下篇</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a61bccd0de8aaa1a5995871f59d322299a7cba40fd46b8820fa68160772f3fe4d8acc1ca1&amp;idx=1&amp;mid=2247487221&amp;scene=21&amp;sn=1a1c220385c965d0619bb88b407b6fc6#wechat_redirect">思考：Transformers与图神经网络有什么关系，我们能从transformer学习什么？</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a613dcd0de82be2f3d8cc2b042a2e3bee1430274c997fd9ea664faa9c6e34ae17097448d3&amp;idx=1&amp;mid=2247487092&amp;scene=21&amp;sn=d634e1af0b685c6c508a7799a4db4e4b#wechat_redirect">从Laplacian矩阵开始，理解GCN</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6854cd0de1428e500ed9ce78e79a5906c70ee668f8d2a1bf2147d04fe4383a2e6ead4b60&amp;idx=1&amp;mid=2247485213&amp;scene=21&amp;sn=55db3f92b33fd97f70ea931ce720c9e3#wechat_redirect">如何解决图神经网络(GNN)训练中过度平滑的问题？</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a658bcd0dec9d6a3080ca467759922c0aba695cf77b5be8144fa2bc2d13dd6095ffbd2c70&amp;idx=1&amp;mid=2247486146&amp;scene=21&amp;sn=5ee261f9c390ee3d0d4900111c313bb2#wechat_redirect">图网络GNN(特别篇)：一文遍览图网络中16种典型的图卷积和9种图池化Graph Pooling</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68eccd0de1fade505b2ce0db738fb4cae7fdf6fcc50a2dec4a899472f50295fe8bed2996&amp;idx=2&amp;mid=2247485349&amp;scene=21&amp;sn=2f0434a16a253ccc7e338f09f55be49d#wechat_redirect">从源头深入分析GCN的四个行文思路：重要度度量与累计，注意力，局部一致性，Laplacian变换</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a683bcd0de12da2ce5e106381970eb13d20ce0156c2fe79c56e4897c50216ace6d866042c&amp;idx=2&amp;mid=2247485298&amp;scene=21&amp;sn=50373ec18ed69ab2f9e68668ff4803e5#wechat_redirect">KDD‘18 | 学习任意阶邻近度的Network Embedding</a></li></ul><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9FeFBQS1hnTlZ0ZjN2WU1pYmJzYXdOQ1dMcXNmSFJpYjg3bVk2clFsa0h0UHZ4eW5jaWNaYWQyQ3lVNVg5TlhoRGpRRTY0VDNtU1RSZTlYd1lXSE53dVdHdy82NDA?x-oss-process=image/format,png" alt="img"></p><h2 id="-2"><a href="#-2" class="headerlink" title=""></a></h2><p><strong>3 综述论文</strong></p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6811cd0de107b5d3c24a819551c5c58f909e366b3127ea886855d778dc4acafa217dc9d6&amp;idx=1&amp;mid=2247485272&amp;scene=21&amp;sn=41318f1c9a7a56dcb5e741986df94617#wechat_redirect">清华大学朱文武「基于深度学习的图表示」综述论文，51页pdf（附下载链接）</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a685acd0de14cc88a490265a09e63b6a4b19b4de271fb4bbb40b14d1a5c93600594f8e0ab&amp;idx=1&amp;mid=2247485203&amp;scene=21&amp;sn=d978795214e07c6a11621ef630213129#wechat_redirect">图综述|综述《A Gentle Introduction to Deep Learning for Graphs》</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a653ecd0dec2838a932ddad0107b3ee266297b83392af4916fa0f0d61ea15768c47b3963f&amp;idx=1&amp;mid=2247486071&amp;scene=21&amp;sn=6bfad8168dfd1de20a620f843ca63ecc#wechat_redirect">图网络综述|9篇研究综述看图神经网络GNN的最新研究进展</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a65d1cd0decc79e085b9bcf4139a878aa0a7ba04a4d9a3c684c09c04876fdc140780bd91e&amp;idx=2&amp;mid=2247486104&amp;scene=21&amp;sn=2e4f2a176388d7144b824a52bb3047a8#wechat_redirect">图网络综述|10-异构网络表示学习</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a63afcd0deab987b8f041c29a75bc65816cd18de1d705eeb8ad65a4db641fca5679a47f48&amp;idx=2&amp;mid=2247486694&amp;scene=21&amp;sn=4f6cbe3f77bc41ffbd37b22b4e2f14c5#wechat_redirect">图网络综述|动态图上的表示学习(1)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6248cd0deb5e9ba5f4800dc7705cec5b18f538a978ff20d8660d71e9add8e0d0be9c3075&amp;idx=1&amp;mid=2247486721&amp;scene=21&amp;sn=37f739855cb6dfc4b43086197f5a09ff#wechat_redirect">图文献综述|动态图上的表示学习(2)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a61f6cd0de8e0158a479977a54947d68669cf906155d11d6a5152f40b840c32fc8355f6b7&amp;idx=2&amp;mid=2247487167&amp;scene=21&amp;sn=01c601778ea1405547008f7901e36f03#wechat_redirect">图文献综述|交通领域如何构建图模型进行深度学习</a></li></ul><p>学习一个新的领域最好是从综述开始，因为综述已经非常全面概括了目前该领域的论文，进行了合理的分类。公众号已经整理打包好第3部分全部的综述论文，后期新的论文会加入进去。如果你也需要，在下面公众号回复：<strong>图网络资料</strong>就可以了，内部资料，请勿外传。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9FeFBQS1hnTlZ0ZDFvR3pZRkkyOG1mN3lwZzlLcmljelZablQ2eVRtMkNnbFVYZWQwOHZKNktJT2xGUHc3UWljOWhhWHMzTmhGSWNRaWF4OW9qUHhaZE9PQS82NDA?x-oss-process=image/format,png" alt="img"></p><h2 id="-3"><a href="#-3" class="headerlink" title=""></a></h2><p><strong>4 图相关专题</strong></p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6541cd0dec572a9fb11c5e34b2556f004e2ee819aea294a364843a1865415a2119599a23&amp;idx=1&amp;mid=2247485960&amp;scene=21&amp;sn=66916debf283fcb8cbf07fcdcb342e11#wechat_redirect">图专题｜图神经网络在医学影像中的应用</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a616dcd0de87b005aab0584f6fa2d67a37bf9093694afec006c740d35c1afd5cbb1a172a9&amp;idx=1&amp;mid=2247487012&amp;scene=21&amp;sn=dc538af969299ea1445249c5c4cb0cc8#wechat_redirect">图专题 | 欺诈检测之图算法</a><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a616dcd0de87b005aab0584f6fa2d67a37bf9093694afec006c740d35c1afd5cbb1a172a9&amp;idx=1&amp;mid=2247487012&amp;scene=21&amp;sn=dc538af969299ea1445249c5c4cb0cc8#wechat_redirect">(1)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a605fcd0de949fd1d42c3fffc3944664bbbf1cfa8b43a3fe55fd0bd1434e06a0507f61240&amp;idx=1&amp;mid=2247487254&amp;scene=21&amp;sn=be6d42c652040a8e8b93fdc651845f1c#wechat_redirect">图专题 | 欺诈</a><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a605fcd0de949fd1d42c3fffc3944664bbbf1cfa8b43a3fe55fd0bd1434e06a0507f61240&amp;idx=1&amp;mid=2247487254&amp;scene=21&amp;sn=be6d42c652040a8e8b93fdc651845f1c#wechat_redirect">检测之图算法</a><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a605fcd0de949fd1d42c3fffc3944664bbbf1cfa8b43a3fe55fd0bd1434e06a0507f61240&amp;idx=1&amp;mid=2247487254&amp;scene=21&amp;sn=be6d42c652040a8e8b93fdc651845f1c#wechat_redirect">(2)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a62eecd0debf8cee3719e5b4566fe1cd520ecfe97f380c675021dd33f99c2f21c5e515d54&amp;idx=2&amp;mid=2247486887&amp;scene=21&amp;sn=68b4a0487a93e2c1ebe6496d9b2496b0#wechat_redirect">图专题 | 图网络之核函数</a><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a62eecd0debf8cee3719e5b4566fe1cd520ecfe97f380c675021dd33f99c2f21c5e515d54&amp;idx=2&amp;mid=2247486887&amp;scene=21&amp;sn=68b4a0487a93e2c1ebe6496d9b2496b0#wechat_redirect">(1)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6254cd0deb4251502e6f780a1aee159e049f5c1f456b9431954149c611d6a0d793d0a945&amp;idx=1&amp;mid=2247486749&amp;scene=21&amp;sn=b52297369798585e246b73958a83b3d9#wechat_redirect">图专题 | 图网络之核函数</a><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6254cd0deb4251502e6f780a1aee159e049f5c1f456b9431954149c611d6a0d793d0a945&amp;idx=1&amp;mid=2247486749&amp;scene=21&amp;sn=b52297369798585e246b73958a83b3d9#wechat_redirect">(2)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6471cd0ded674baabcf867aada5830707330a64bc9e0c7e94bb9307f407dd2bfe6201c02&amp;idx=1&amp;mid=2247486264&amp;scene=21&amp;sn=242b7fe066c28d83136a46cb1e58e0d2#wechat_redirect">图专题 | 图上的池化方法</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6417cd0ded01c69500e1a3d94154756b9349bab0e69657b28f70e658005968fe94747f35&amp;idx=1&amp;mid=2247486302&amp;scene=21&amp;sn=ab434bc382029df9547e043ec5f6fbf8#wechat_redirect">图专题 | 图上的生成模型</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a65aecd0decb824ebe43432cc00ac8e91d2d6d52a698a96185e39cc8fca9a1c4fe7fa3000&amp;idx=1&amp;mid=2247486183&amp;scene=21&amp;sn=8fa1926437ac0541ef15a55d5220050d#wechat_redirect">图专题 | 动态图表示学习</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68cbcd0de1dda1dddb4b77982eaba4eed6ff395471dd597273d317151b8e20ea471400b9&amp;idx=2&amp;mid=2247485314&amp;scene=21&amp;sn=b5e095da2df9aba3dd5b86e1d565805b#wechat_redirect">图专题 | 图表示学习方法的鲁棒性研究</a></li></ul><h2 id="-4"><a href="#-4" class="headerlink" title=""></a></h2><p><strong>5 会议论文合集</strong></p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a601fcd0de9091e4d5576d2f940b8c8f24433991095db556fc4bde76d2f8b4340bf3b4f25&amp;idx=1&amp;mid=2247487318&amp;scene=21&amp;sn=c090ea8c0cd7c43f3f202a104b2d24f0#wechat_redirect">ICML20 | 连续图神经网络；常数曲率图神经网络；贝叶斯图神经网络</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68aecd0de1b8eb16f36f87ea8532d430290e1e7596cdc04f306556f1f19dbc71c581ab91&amp;idx=1&amp;mid=2247485415&amp;scene=21&amp;sn=502e76a33a638c7f1bcdadde2b62f174#wechat_redirect">论文汇总 | (全部)AAAI’20图相关论文合集，涉及各个方面</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6866cd0de1707d2fb247d5e2278fd303527eef8ac2a95662cb35eb55e19910d35411c074&amp;idx=3&amp;mid=2247485231&amp;scene=21&amp;sn=3b9c9d04f98adb42efb67945bc1024d2#wechat_redirect">论文汇总 | 15篇ICLR’20与图网络学习有关的论文汇总（第一期）</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6866cd0de1707eaf873e208f54ac8a6a1072cd1e821bec2cabc9bbf47d5bfa28c7e9da4b&amp;idx=2&amp;mid=2247485231&amp;scene=21&amp;sn=b005305abd9a0aeba2b1d9b188b78f59#wechat_redirect">论文汇总 | 14篇ICLR’20与图网络学习有关的论文汇总（第二期）</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6866cd0de170215eab244116bf04f2ee25b4292fd58a838b1717f2e2c31c02c7e1fa30cc&amp;idx=1&amp;mid=2247485231&amp;scene=21&amp;sn=76ab2995a5adc36c76808d66a9dd074d#wechat_redirect">论文汇总 | 14篇ICLR’20与图网络学习有关的论文汇总（第三期）</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a28cd0de33ecd4589fb24fb3d7661402b2e5bd469e7b816f403999f9ad3a77b0a55bf03&amp;idx=2&amp;mid=2247484769&amp;scene=21&amp;sn=3dcd212796787de2ae06a748abafd423#wechat_redirect">论文汇总 | ICCV’19 图相关论文合集</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a28cd0de33e720e78d3ec0040c6cb5aedd16d3105e977cce70a0bcaba5961985da83966&amp;idx=1&amp;mid=2247484769&amp;scene=21&amp;sn=ef997ac2df5ad9ab9ccdf889bc32d123#wechat_redirect">论文汇总 | KDD’19 图相关论文合集</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6b1ecd0de2087fedcb57f54906880ffb3edcd7b351e46bfb2772959e201c1018449c81a2&amp;idx=1&amp;mid=2247484503&amp;scene=21&amp;sn=6e87a721f588e23868684fab95a745ce#wechat_redirect">论文汇总 | NeurIPS’19 图网络相关论文合集</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a697fcd0de06914215987c3e9b33fa56e2bbd7b8e9c864af3777f7dd863a08abde7b93847&amp;idx=1&amp;mid=2247484982&amp;scene=21&amp;sn=44ddbd939bac98e574707f53d923c529#wechat_redirect">论文简讯 | AAAI’20中五篇与图神经网络相关论文，涉及图分类，动态图建模，知识图谱对齐，交通预测等</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6ce0cd0de5f6761f75d3a463557687f354f6fe809af11176b3d3ca22f5ac0858e128080c&amp;idx=1&amp;mid=2247484329&amp;scene=21&amp;sn=2a553b2ffcbcd185f4ece070e4d36785#wechat_redirect">论文简讯 | WWW’19|</a>图相关论文(第一期)</li></ul><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy9FeFBQS1hnTlZ0ZjN2WU1pYmJzYXdOQ1dMcXNmSFJpYjg3cUVMZDcwOHlJbjZpYnJ6QmlhNW9JbzBMeXpkMVlVRk5na2thTTVrQzRiYjBwM0tKeVNRWXRZZkEvNjQw?x-oss-process=image/format,png" alt="img"></p><h2 id="-5"><a href="#-5" class="headerlink" title=""></a></h2><p><strong>6论文快讯</strong></p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a61d7cd0de8c1e1d52efc67c7f20ddbd41e85e3ee875faa7a5fefc8078b8243a36128cf6b&amp;idx=1&amp;mid=2247487134&amp;scene=21&amp;sn=a6df25be27fe01dfea1ef9b81927f8ae#wechat_redirect">图表示学习和几何深度学习workshop(II): node2vec与强化学习；双曲嵌入；Group卷积中的注意力等</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a610fcd0de819d67eea53c88f44e835ffac6c50cbee409da44d707f7da0b8e474d1ca91d5&amp;idx=1&amp;mid=2247487046&amp;scene=21&amp;sn=ee4abc37262525a285efbac3f6737601#wechat_redirect">ELLIS Workshop: April 2020图网络最新研究进展(I)</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68e2cd0de1f4fadb4e269232298632cc8693de6c1e859d4a12bd620af48f9cc8330dd0bf&amp;idx=2&amp;mid=2247485355&amp;scene=21&amp;sn=48d78eaa30ddae7e9b7ad6a3168e6869#wechat_redirect">论文汇讯: 图网络新应用：新燃料发现，有机化学逆合成，多任务学习，单一场景中的检测等等</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6048cd0de95e7c4730e77e212a59cf33676592e259710b4d816d61b0019d5967f6b031dc&amp;idx=2&amp;mid=2247487233&amp;scene=21&amp;sn=726e70b049ef673686dbeacc15fc6131#wechat_redirect">GRAPH-BERT:图表示学习只需要注意力</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a61a2cd0de8b4945d6231012a6adc2378d5ae38e501e2b707cd0c920c6a5e2296594037a2&amp;idx=1&amp;mid=2247487211&amp;scene=21&amp;sn=0d73cbeaedcce9f1ffd946c80d08b528#wechat_redirect">图机器学习-图拉普拉斯算子的离散正则性，141页ppt，Discrete regularity graph Laplacians</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a63f9cd0deaef04e84dd250ab413c7111e37d637460a9d1cdc1b8d2cede2a524efa736a41&amp;idx=2&amp;mid=2247486640&amp;scene=21&amp;sn=cb510a11010f2e4139e89f69e8d47288#wechat_redirect">KDD19开源论文 Heterogeneous Graph Neural Network</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a60ddcd0de9cb8eebacbaab717e5ea626ef1fdb8bb6219ec6ad03276064f79f185811324c&amp;idx=1&amp;mid=2247487380&amp;scene=21&amp;sn=4189a16de06095c5489af4ac7f046d6a#wechat_redirect">KDD2020|混合时空图卷积网络：更精准的时空预测模型</a></li><li>论文快讯: <a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6af9cd0de3ef30e76a1bad24764f5f0603a9e7aeab17817f4e3eb55cb7202ce00a8a7052&amp;idx=2&amp;mid=2247484848&amp;scene=21&amp;sn=a499d1c583cb4ad9405802a6bba26401#wechat_redirect">5篇Graph相关内容：多标签建模；因果结构学习；N-gram Graph等等</a></li><li>论文快讯: <a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a41cd0de3578e22273aad8809795436018dadce85aefcee75747faeb4a84c083a2aa576&amp;idx=2&amp;mid=2247484680&amp;scene=21&amp;sn=7037cb6a2bf20fbe578290bd317df07a#wechat_redirect">一些新图卷积相关方法的提出来啦</a></li><li>论文快讯: <a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6cb2cd0de5a492b23d8e1994cae7ae50dcc84771ac31fb22cab88b3cdcc5f4685219f9cf&amp;idx=1&amp;mid=2247484411&amp;scene=21&amp;sn=d994ad7ab9631d789bee842abd45127b#wechat_redirect">很多新的图学习方法提出，赶紧来看</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6cc5cd0de5d3dafda96d585dea749b28c281f6816941559847823303cc31f7140b723578&amp;idx=2&amp;mid=2247484300&amp;scene=21&amp;sn=8e03021cba912c4da0f6c45aaad664ec#wechat_redirect">论文快讯: 图神经网络的新鲜应用</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6941cd0de057d834f0bf642e98a758c5470c3429a5ab18b58ff22cf8fdea82d0f4d6acc1&amp;idx=1&amp;mid=2247484936&amp;scene=21&amp;sn=f876044c369fc15244e800572c653b4a#wechat_redirect">论文快讯: 图网络相关的最新文献，涉及Graph Embedding 综述，交通预测、停车位可用性预测，图匹配等</a></li></ul><h2 id="-6"><a href="#-6" class="headerlink" title=""></a></h2><p><strong>7相关资料</strong></p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a69bfcd0de0a9060df71e767134bc2eddcf77ec39cbb8e8df2effcfa41417eb8920539f1f&amp;idx=1&amp;mid=2247485174&amp;scene=21&amp;sn=d0c5ba237e90689afcab26d5618591a1#wechat_redirect">资料|一本机器学习与复杂网络相关的书籍《Machine Learning in Complex Networks》</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a69fbcd0de0edb7577a2301bb42ff7fa5fe23c2d41294a4741e453de4e3f4e58879cae4a7&amp;idx=1&amp;mid=2247485106&amp;scene=21&amp;sn=f06bf54907696f87e68acc84785404d3#wechat_redirect">资料 |斯坦福大学2019秋季新课CS224W: Machine Learning with Graphs 课程讲义PPT等</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a61eecd0de8f874727aa04a8d00c12dac3712573cd4e9718019529a42b32de7762e62b7da&amp;idx=2&amp;mid=2247487143&amp;scene=21&amp;sn=14a8a7b0094782388b0ddd5b8f701b0f#wechat_redirect">斯坦福CS224W课程—机器学习与图—课程介绍与资料汇总</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a612fcd0de839dc78c33c3225285fa8f4060ddfb309320abd09dcb3692f0e2fc325ab9b50&amp;idx=1&amp;mid=2247487078&amp;scene=21&amp;sn=8025547f732e00628a0270b93c3867d1#wechat_redirect">【经典】GAT作者Petar剑桥大学博士论文《深层神经网络结构的复兴》147页pdf，附下载链接</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6930cd0de026f89abcc8a33a84d5c40d6943d0f8777d8a6ca247de1828692967d7fa7060&amp;idx=1&amp;mid=2247485049&amp;scene=21&amp;sn=5a18b2508df75359023a8c8f25c21564#wechat_redirect">资料 | 17篇论文，详解图的机器学习趋势 | NeurIPS 2019</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6969cd0de07f5039e9dda663889f5eebc42539ce61891165a44a4ef445d58995e47329c6&amp;idx=2&amp;mid=2247484960&amp;scene=21&amp;sn=9b3d5cc21ac6e4f0c14d8875d986a9d8#wechat_redirect">资料 | NLP 和人文社会学科课程来了：</a><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6969cd0de07f5039e9dda663889f5eebc42539ce61891165a44a4ef445d58995e47329c6&amp;idx=2&amp;mid=2247484960&amp;scene=21&amp;sn=9b3d5cc21ac6e4f0c14d8875d986a9d8#wechat_redirect">斯坦福开年公开课主讲NLP和社交网络应用</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6c99cd0de58f960201e924dc3f39dfb05a900b35958d5e590939641f0fe2b5baca9201bb&amp;idx=1&amp;mid=2247484368&amp;scene=21&amp;sn=8b853a4bede3ee2e7b2fe30d4d8d08d9#wechat_redirect">资料 | Graph embedding|Graph Neural Network 学习资料汇总</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6adbcd0de3cdbd578d603ce09b45a23f0341b4f07629160c237b30adfb1034c0c106a70d&amp;idx=2&amp;mid=2247484818&amp;scene=21&amp;sn=e89065a89ea21f5ad4ff43c865cc0bc1#wechat_redirect">GitHub | Awesome Graph classification</a></li></ul><p>进入一个新的领域如果有一些系统的辅助的资料就不会被带偏，迷失方向。博客公众号，论文相当于完整的书籍还是缺乏系统性，为此公众号系统收集了图网络相关的资料，书籍等。如果你也需要，在下面的公众号，回复：<strong>图网络资料</strong>就可以领取，内部资料请勿外传~</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9FeFBQS1hnTlZ0ZDFvR3pZRkkyOG1mN3lwZzlLcmljelZablQ2eVRtMkNnbFVYZWQwOHZKNktJT2xGUHc3UWljOWhhWHMzTmhGSWNRaWF4OW9qUHhaZE9PQS82NDA?x-oss-process=image/format,png" alt="img"></p><p><strong>8 论文解读合集</strong></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a60ddcd0de9cb8eebacbaab717e5ea626ef1fdb8bb6219ec6ad03276064f79f185811324c&amp;idx=1&amp;mid=2247487380&amp;scene=21&amp;sn=4189a16de06095c5489af4ac7f046d6a#wechat_redirect">深度解读：KDD2020|混合时空图卷积网络：更精准的时空预测模型</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6016cd0de900d42b9c3e0e0c349f6aa37afbeb5b6ca5d03d885fa310365deb30c54890b5&amp;idx=2&amp;mid=2247487327&amp;scene=21&amp;sn=22bb888386815ecf7d682d138b80d837#wechat_redirect">深度解读：【GNN】VGAE：利用变分自编码器完成图重构</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a601fcd0de9092a3609ebb972cac1b336747b0a8313baa627add0476ac5bf372858a2255e&amp;idx=2&amp;mid=2247487318&amp;scene=21&amp;sn=ed90b395f9e9f049d537b872fb39d6b7#wechat_redirect">深度解读：互信息及其在图表示学习的应用</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a61a2cd0de8b4d7c23df0f2480f103e140432066c937215b009411bac7dc0f97cfaf7591a&amp;idx=2&amp;mid=2247487211&amp;scene=21&amp;sn=f83e90d64c2f240a9a488844b070f5dd#wechat_redirect">深度解读：基于图神经网络的聚类研究与应用</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6185cd0de893316eabf99af23e60d34c667d1890fff4dbaf1f602509adfbfc7d98df28fe&amp;idx=1&amp;mid=2247487180&amp;scene=21&amp;sn=020ca04e699b59c8a19269cb6544f62f#wechat_redirect">深度解读：Transformer for Graph Classification:无监督的学习下图分类(代码已经公开)</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a63bfcd0deaa98d532764ec158aaefb1f8d20ebafcec09953a9b9f0c7855a373c9e72e775&amp;idx=1&amp;mid=2247486710&amp;scene=21&amp;sn=78f9fdb33584dc5021daeaf6f027a7ca#wechat_redirect">深度解读：图系列|三篇图层次化表示学习(Hierarchical GNN)：图分类以及节点分类</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a606ecd0de978cb1521dfa29e6445e68f9e8493214eaa23ad9d728f4bdee5c859bc9780cd&amp;idx=2&amp;mid=2247487271&amp;scene=21&amp;sn=f5869a1fddd8a90727dd5285433f8acf#wechat_redirect">深度解读：团队新作 | 多尺度图卷积神经网络：有效统一三维形状离散化特征表示</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6185cd0de8937a89084da3380cf9b63fe76ed7ece7a612fe507dd09f68ebc3ef32d97012&amp;idx=2&amp;mid=2247487180&amp;scene=21&amp;sn=c537be21bbf0b8e4b3cc2c0017cc6d2a#wechat_redirect">深度解读：ICLR20|CUHK及NUS提出两个指标度量与提升图网络消息传递</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6892cd0de184256a249edbf84060ff47428909bdfe41ab28ffefd7d409657f3b5728a86e&amp;idx=1&amp;mid=2247485403&amp;scene=21&amp;sn=2b945d9f2afd372523378de61b13ef00#wechat_redirect">论文解读：KDD’18异质信息网络嵌入学习—HEER模型结构，解决存在多决种关系的问题</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6892cd0de184f0a113f577b69b739b546512815a00b3c60f32b92daedba4add33db04b36&amp;idx=2&amp;mid=2247485403&amp;scene=21&amp;sn=9268405fcb5e730baa13cbee48cd0a27#wechat_redirect">论文解读：KDD’19 Deep Learning on Graph 最佳论文：自然语言生成（NLG）任务</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6883cd0de19579e1a89216e5728e49e687425baa830ef1328bb0b5e1e26f2ffabab4e737&amp;idx=2&amp;mid=2247485386&amp;scene=21&amp;sn=ae8abe8fac84e9865db6b8b8b5a49b99#wechat_redirect">论文解读：图表示学习中的对抗与攻击KDD’18 Best Paper: Adversarial Attacks</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a684ccd0de15a1633bbb0c67363a4a173bcd6b438abe42f88bbebc2289ca6fdf2dc559bb1&amp;idx=2&amp;mid=2247485189&amp;scene=21&amp;sn=11f10f4ca11227917982eb2f8d3d2d6e#wechat_redirect">论文解读：GMNN：图马尔可夫网络，关系数据进行建模，变分EM框架训练模型</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a69c3cd0de0d5e11e5454e0847711995adf2da5b99b3b5848f0387f69158341a469349410&amp;idx=1&amp;mid=2247485066&amp;scene=21&amp;sn=75a82b6955fabf37c2a89cb87f025abf#wechat_redirect">论文解读：普林斯顿高研院, 浙大, CMU和MIT联合提出图核函数与图神经网络的融合方法</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6917cd0de00177867601fd08f107c5c0125525f2ee98936fdac453f20ba63aa7e6b8171f&amp;idx=1&amp;mid=2247485022&amp;scene=21&amp;sn=e829f7a4984cf00647fd999f564182bb#wechat_redirect">论文解读：NeurIPS’20|Hyperbolic Graph Convolutional Neural Networks</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6905cd0de013f825e2e735649267cd8d5ac34c610d34283a1c00f950d76414c1f7890a35&amp;idx=1&amp;mid=2247485004&amp;scene=21&amp;sn=0cc6b22eea2929261bf0d63957be7ba2#wechat_redirect">论文解读：ICCV’19 Workshop 论文解读：用图神经网络改善视频的多标签分类</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6953cd0de045758104e1b31bdcca1039e89ed2c4cbf3b443019f738261c48596099a6e5e&amp;idx=1&amp;mid=2247484954&amp;scene=21&amp;sn=adfc8e0b42b0df5d07394c5b435b7d14#wechat_redirect">方案解读：CIKM’19 挑战杯「用户行为预测」冠军方案分享：「初筛-精排」两阶求解框架</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6ac3cd0de3d54998693a9dcb041f7c7c24e4723099ef9c23f6ee7a09a04643949a79af38&amp;idx=2&amp;mid=2247484810&amp;scene=21&amp;sn=55965f5a552005defdd5a50c71664da4#wechat_redirect">论文解读：KDD’19 | 图神经网络预测知识图谱中的节点重要性</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68c4cd0de1d2a1a7f3d0b93accd6b927df445370b30552985a9e069abe5e7e4f70c76a76&amp;idx=1&amp;mid=2247485325&amp;scene=21&amp;sn=480459ea0296e3c6ddd1374e2540c89c#wechat_redirect">论文解读：图系列|图神经常微分方程，如何让 GNN 在连续深度域上大显身手？</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6b1ecd0de208dc64cf8f1248cf66cf8f1e19547362fb1ea8a8b543b8087b90783ea89757&amp;idx=4&amp;mid=2247484503&amp;scene=21&amp;sn=541368c53b1771b6d7bf3800c66aecf5#wechat_redirect">论文解读：ACL’19论文| 为知识图谱添加注意力机制</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6b0dcd0de21bbdeae8b9fb618e39902af49972ed16a33c17869292b383243aae1df7f79f&amp;idx=2&amp;mid=2247484484&amp;scene=21&amp;sn=a6106a43387b83d54ca8458d71063203#wechat_redirect">论文解读：基于 GCN 的反垃圾评价系统，闲鱼已经用上了！</a></p><h2 id="-7"><a href="#-7" class="headerlink" title=""></a></h2><p><strong>9动态时空图专题</strong></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68e9cd0de1ffbc5f17f03d74d483b29ce723a8f0ec160e28782f5ac85eaca7c66ca97e69&amp;idx=1&amp;mid=2247485344&amp;scene=21&amp;sn=f297392eea97c6daad3f3b36243ca126#wechat_redirect">动态图系列|动态图嵌入与表示:8篇必读论文(第1期)</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68d0cd0de1c6c60a70303316a60f009def232ba60238ee2b8cddbe5c667f30525e100718&amp;idx=1&amp;mid=2247485337&amp;scene=21&amp;sn=77d935de399466f106068ee47b8d0775#wechat_redirect">动态图系列|动态知识图谱预测与补全上必读6篇论文</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6b1ecd0de20823056318629db123c831b7295390d549793a7dfc78f37fd2b47553922fb2&amp;idx=3&amp;mid=2247484503&amp;scene=21&amp;sn=4724428823d432b1def9a6b5fc9ae34c#wechat_redirect">动态图系列|动态图的研究渐渐多起来</a></p><h2 id="-8"><a href="#-8" class="headerlink" title=""></a></h2><p><strong>10 动作识别专题</strong></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68c4cd0de1d20334803f53b6efe974ca020d49b17a7eef8624194af1e6550a825e22f776&amp;idx=3&amp;mid=2247485325&amp;scene=21&amp;sn=a8cff9e8921f038428903db9a75f697b#wechat_redirect">图系列|全面详解！图卷积在动作识别方向的应用（下）</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6822cd0de1348de542ceca991a04b6b74a47ee10ab23edb27be64785fd1e35dc1a470a14&amp;idx=2&amp;mid=2247485291&amp;scene=21&amp;sn=0c7c8f45d2fe8c053e6b2e68d5e175d5#wechat_redirect">图系列|全面详解！图卷积在动作识别方向的应用（上）</a></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6811cd0de107f476067c5555c5b3cec8d4281287df99d21990c8c69605d267dbb0a8aea2&amp;idx=3&amp;mid=2247485272&amp;scene=21&amp;sn=265d04add9407ed493b484afc8410efd#wechat_redirect">图系列|通过神经网络搜索(NAS)图卷积网络进行人体骨骼的动作识别</a></p><h2 id="-9"><a href="#-9" class="headerlink" title=""></a></h2><p><strong>11 代码实战教程</strong></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a68d0cd0de1c6bd9c55fd9e92dbb5b63d986af7cb2a8ed89e1bc7f28cb540f890d27ed2e1&amp;idx=3&amp;mid=2247485337&amp;scene=21&amp;sn=44ddeb2446c5ddd8d39307aa4014b9b1#wechat_redirect">图代码实战|Amazon图神经网络库DGL零基础上手指南-以节点分类为例</a></p><p>图代码<a href="http://mp.weixin.qq.com/s?__biz=MzUyNzcyNzE0Mg==&amp;chksm=fa7a6a31cd0de327923487c4fa07d9c73a39800bf9c51b1b2583e382534891c28b4cbb47b351&amp;idx=3&amp;mid=2247484792&amp;scene=21&amp;sn=c6b535e7df656a0d4a59bcf61a09d663#wechat_redirect">实战|在PyTorch框架下使用PyG和networkx对Graph进行可视化</a></p><p>上面的内容有点多，是不是看的有点懵，建议收藏一下。后期可以按照<strong>关键词</strong>在深度学习与图网络公众号历史消息中进行搜索就可以了。</p><p>最后在说明一下哈，如果你也需要第三部分和第七部分的资料，在下面的公众号领取。暗号：<strong>图网络资料，</strong>拒绝任何套路，纯干货分享。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9FeFBQS1hnTlZ0ZDFvR3pZRkkyOG1mN3lwZzlLcmljelZablQ2eVRtMkNnbFVYZWQwOHZKNktJT2xGUHc3UWljOWhhWHMzTmhGSWNRaWF4OW9qUHhaZE9PQS82NDA?x-oss-process=image/format,png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】10. Deep Generative Models for Graphs</title>
      <link href="/2020/12/13/si-tan-fu-cs224w-tu-ji-qi-xue-xi-10-deep-generative-models-for-graphs/"/>
      <url>/2020/12/13/si-tan-fu-cs224w-tu-ji-qi-xue-xi-10-deep-generative-models-for-graphs/</url>
      
        <content type="html"><![CDATA[<p><font color="red">核心思想是将<strong>图生成问题</strong>转变成<strong>序列生成问题</strong>。</font></p><blockquote><p>转载自<a href="https://blog.csdn.net/Jenny_oxaza/article/details/107554165">cs224w 图神经网络 学习笔记（十）Deep Generative Models for Graphs</a></p></blockquote><h2 id="Recap-回顾"><a href="#Recap-回顾" class="headerlink" title="Recap: 回顾"></a>Recap: 回顾</h2><p>在上节课中GNN的学习中，我们从空域的角度理解GNN的基本思想就是<strong>邻居聚合</strong>：<br><img src="https://img-blog.csdnimg.cn/20201213162633826.png"></p><blockquote><p>邻居聚合的优点在于结合了<strong>Feature</strong>和<strong>Graph Structure</strong></p></blockquote><p>用数学公式来表示一次<strong>邻居聚合</strong>就是：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glmkmxyyiuj20e601l748.jpg" alt="消息聚合公式"></p><p>经典的GNN算法有GCN和GraphSAGE两种，GraphSAGE相比于GCN的优点之一在于<strong>其有更多的不同的Aggregation Function</strong>。</p><p>今天讲解的内容是如何生成图，即：<br><strong>input</strong>：a set of obeserved graphs<br><strong>output</strong>：new graphs generated by model</p><h2 id="Problem-of-Graph-Generation"><a href="#Problem-of-Graph-Generation" class="headerlink" title="Problem of Graph Generation"></a>Problem of Graph Generation</h2><h3 id="图生成算法的重要性"><a href="#图生成算法的重要性" class="headerlink" title="图生成算法的重要性"></a>图生成算法的重要性</h3><p><img src="https://img-blog.csdnimg.cn/20201213164127247.png"></p><h3 id="图生成算法有两大任务"><a href="#图生成算法有两大任务" class="headerlink" title="图生成算法有两大任务"></a>图生成算法有两大任务</h3><ol><li><strong>Realistic graph generation</strong><br>Generate graphs that are similar to a given set of graphs（本节课的重点）</li><li><strong>Goal-directed graph generation</strong><br>Generate graphs that optimize given objectives/constraints（Drug molecule generation/optimization）</li></ol><h3 id="图生成算法的难点"><a href="#图生成算法的难点" class="headerlink" title="图生成算法的难点"></a>图生成算法的难点</h3><ol><li><strong>Large and variable output space</strong><br><img src="https://img-blog.csdnimg.cn/20201213164853330.png"></li><li><strong>Non-unique representations</strong><br>即存在<strong>图同构</strong>这种性质<br><img src="https://img-blog.csdnimg.cn/20201213165030404.png"></li><li><strong>Complex dependencies</strong><br>以下图的例子来看，在边的生成的过程中，存在着前后依赖关系：<br><img src="https://img-blog.csdnimg.cn/2020121316531251.png"><h2 id="Recap-ML-Basics-for-Graph-Generation"><a href="#Recap-ML-Basics-for-Graph-Generation" class="headerlink" title="Recap: ML Basics for Graph Generation"></a>Recap: ML Basics for Graph Generation</h2>基于上面讨论的图生成算法的难点，我们可以通过下面讲解的机器学习知识将其成功解决。</li></ol><p>首先，从机器学习的角度明确Graph Generative Model的输入和输出：<br><img src="https://img-blog.csdnimg.cn/20201213165808720.png"><br>我们假设现实生活中的图，是由“上帝之手”锻造的，它有一个潜在的<strong>概率分布</strong>$p_{data}(G)$，而这个<strong>概率分布</strong>正是我们需要去学习的。</p><p>回顾一下，传统的 <strong>生成模型（Generative Models）</strong> 的含义：<br><img src="https://img-blog.csdnimg.cn/20201213170152364.png"><br>我们的目标有两个：<strong>得到一个近似的distribution（由参数$\theta$确定）</strong> 以及 <strong>能够从这个distribution里面采样出新的数据（终极目标）</strong>。</p><blockquote><p>需要理解的是，这里的distribution是概率分布的意思，讲得实际一点，比如数据服从正态分布、数据服从伯努利分布等等。只有有了概率分布之后，我们才能在这个概率分布中采样出无穷无尽的数据，这也是为什么第一个目标是构造概率分布的原因。</p></blockquote><p>定义了两个目标之后，我们就来看看怎么实现这两个目标：</p><h3 id="Goal-1：Make-p-model-x-theta-close-to-p-data-x"><a href="#Goal-1：Make-p-model-x-theta-close-to-p-data-x" class="headerlink" title="Goal 1：Make $p_{model}(x; \theta)$ close to $p_{data}(x)$"></a>Goal 1：Make $p_{model}(x; \theta)$ close to $p_{data}(x)$</h3><p>关键技术：<font color="red"><strong>极大似然估计</strong></font><br><img src="https://img-blog.csdnimg.cn/20201213171503144.png"></p><blockquote><p>简单回顾一下概率论中关于极大似然估计的基本思想：使当前被发现的样本的概率最大化。</p></blockquote><h3 id="Goal-2：Sample-from-p-model-x-theta"><a href="#Goal-2：Sample-from-p-model-x-theta" class="headerlink" title="Goal 2：Sample from $p_{model}(x;\theta)$"></a>Goal 2：Sample from $p_{model}(x;\theta)$</h3><p><img src="https://img-blog.csdnimg.cn/20201213172158498.png"><br>这里不太理解，据说是生成模型的常用做法。</p><p>关于生成模型的种类，老师给出了下面这张图：<br><img src="https://img-blog.csdnimg.cn/20201213172413331.png"><br>本节课使用的生成模型叫做：<font color="red"><strong>Auto-regressive models</strong></font>，它的核心思想是<strong>predicts future behavior based on past behavior.</strong></p><h3 id="Auto-regressive-models"><a href="#Auto-regressive-models" class="headerlink" title="Auto-regressive models"></a>Auto-regressive models</h3><p>Auto-regressive models具有这样一种特征：$p_{model}(x;\theta)$ is used for both <strong>density estimation</strong> and <strong>sampling</strong> (from the probability density)</p><blockquote><p>而对于Variational Auto Encoders（VAEs），Generative Adversarial Nets（GATs）等模型，它们是分2个或者多个模型来分别完成上面两个Goals的。</p></blockquote><p>Auto-regressive model里面用到了概率论中的Chain Relu：<br><img src="https://img-blog.csdnimg.cn/20201213192840646.png"></p><blockquote><ul><li>Chain Rule揭示了一个简单的事实：后面的状态由前面的状态决定。</li><li>后面讲的GraphRNN的思想就源于Chain Rule，学习的时候记得将GraphRNN类比到Chain Rule~</li></ul></blockquote><h2 id="GraphRNN：Generating-Realistic-Graphs"><a href="#GraphRNN：Generating-Realistic-Graphs" class="headerlink" title="GraphRNN：Generating Realistic Graphs"></a>GraphRNN：Generating Realistic Graphs</h2><h3 id="Model-Graph-as-Sequences-将图的生成问题转换成序列生成问题"><a href="#Model-Graph-as-Sequences-将图的生成问题转换成序列生成问题" class="headerlink" title="Model Graph as Sequences 将图的生成问题转换成序列生成问题"></a>Model Graph as Sequences 将图的生成问题转换成序列生成问题</h3><p>网络的生成是通过不断地增加节点和边来实现的。<br><img src="https://img-blog.csdnimg.cn/20201213193937952.png"><br>对应一个确定的节点顺序$\pi$，图$G$可以表示为节点和边的序列$S^{\pi}$：</p><p><img src="https://img-blog.csdnimg.cn/20200805220551591.png#pic_center"><br>$S^{\pi}$实际上是序列的序列，里面包含了序列$S_{i}^{\pi}$，它有两个层次的操作：</p><ul><li><strong>节点操作</strong>——增加节点，每个$S_{i}^{\pi}$对应一个；<br><img src="https://img-blog.csdnimg.cn/20201213194509203.png"></li><li><strong>边的操作</strong>——序列 $S_i^{\pi}$中的每一步表示是否要与已有的节点增加一条边：<br><img src="https://img-blog.csdnimg.cn/2020121319473133.png"><br>那么，我们就可以将图的生成问题转换成一个<strong>序列的生成问题</strong>。这样一来，（对于无向图来说），我们只需要保存邻接矩阵的一半就行了。</li></ul><p>综上所述：<br><strong>A graph + a node ordering = A sequence of sequences!</strong><br><img src="https://img-blog.csdnimg.cn/2020121319502638.png"><br>那么接下来，我们需要解决的就是两个过程：</p><ul><li>新的节点的生成——节点序列的生成</li><li>对于新生成的节点，生成其相关联的边——边序列的生成</li></ul><p><strong>而解决序列问题，我们自然而然地就能想到利用RNN来实现。</strong></p><h3 id="GraphRNN"><a href="#GraphRNN" class="headerlink" title="GraphRNN"></a>GraphRNN</h3><p>GraphRNN包括两个部分：</p><p>node-level RNN——生成edge-level RNN的初始状态<br>edge-level RNN——为新节点创建相关的边，并将结果更新到node-level RNN的状态中。<br><img src="https://img-blog.csdnimg.cn/20201213204549171.png"></p><blockquote><p>其实从上面的图中可以看出，每一步是先求解边，然后再加入点的。</p></blockquote><p>那么，我们怎样利用RNN来生成序列呢？</p><p>首先来看看RNN的定义，对于一个RNN单元来说，有状态$s_t$，输入$x_t$，输出$y_t$。<br><img src="https://img-blog.csdnimg.cn/2020121320501081.png"><br>对于序列的表示，可以将RNN单元重复连接。序列的开始和结束都定义为特殊的标识符，开始的标示符$s_0 = SOS$，结束的标识符$y_T = EOS$（此时序列$S^{\pi}$的被定义成了$\lbrace SOS，S_1^{\pi}，….，EOS \rbrace$）。而上一个状态的输出就是下一个状态的输入，即$x_{t+1} = y_{t}$。如下图所示：<br><img src="https://img-blog.csdnimg.cn/20201213205954553.png"><br>在上述模型的基础上，我们需要给RNN模型增加随机性。首先，我们要明确的是，我们的目标是使用RNN来估计 $\prod_{t=1}^n p_{model}(x_t|x_1, \cdots, x_{t-1}; \theta)$。我们定义$y_{t} = p_{model}(x_{t}|x_1,\cdots,x_{t-1}|\theta)$，那么$x_{t+1}$是在$y_{t}$上采样的结果，即$x_{t+1} \sim y_{t}$。<br><img src="https://img-blog.csdnimg.cn/20201213210700763.png"><br><strong>RNN每一步的输出是一个概率向量，下一个状态的输入时基于该概率向量的一个取样。</strong></p><p><strong>模型的测试</strong><br><img src="https://img-blog.csdnimg.cn/20201213210959608.png"><br>假设我们有一个已经训练好的模型，$y$服从伯努利分布，$y_1=0.9$表示有0.9的概率生成1，即有边连接；有 $1-0.9=0.1$的概率生成0，即没有边连接。<br><strong>模型训练</strong></p><blockquote><p>那么怎么去训练模型呢？我们知道训练模型肯定是需要定义损失函数的，那么怎么定义随时函数呢？</p><p><img src="https://img-blog.csdnimg.cn/20201213211224369.png"><br>在进行模型训练的时候，有一个原则——Teacher Forcing，也就是加入我们检测到真实的边的序列为 [1,0,…]，在训练时我们用真实的这个序列作为输出。</p></blockquote><p>损失函数定义为$y^{<em>}$和$y$之间的Binary cross entropy，训练目标是使损失函数最小化：<br>$$L=-[y_1^</em> \log(y_1)+(1-y_1^*)\log (1-y_1)]$$</p><p>公式的解释如下图：<br><img src="https://img-blog.csdnimg.cn/20201213212341574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ljX2N5MTk5OQ==,size_16,color_FFFFFF,t_70"><br>下图是一个小例子：<br><img src="https://img-blog.csdnimg.cn/20201213212856969.png"><br>从上面这个例子可以很清晰地看到，我们是分了两个维度来进行RNN，因此在Node Level上，我们有$SOS$，在每个Edge Level上，我们也有对应的$SOS$和$EOS$。</p><h3 id="模型优化——图节点的编号策略"><a href="#模型优化——图节点的编号策略" class="headerlink" title="模型优化——图节点的编号策略"></a>模型优化——图节点的编号策略</h3><p>然而，我们还是面临一个问题，因为每一个新生成的点都有可能和之前的点进行关联，也就是说，当图的节点数量很大时，我需要记住很长的依赖关系来实现边的生成。<br><img src="https://img-blog.csdnimg.cn/2020121321321342.png"><br>我们的解决方案，就是在数据预处理阶段利用BFS（广度优先搜索）来给图的节点编号。<br><img src="https://img-blog.csdnimg.cn/20201213213416680.png"><br>使用广度优先搜索进行编号，有两个优点：</p><ul><li>Reduce possible node orderings</li><li>Reduce steps for edge generation</li></ul><p><img src="https://img-blog.csdnimg.cn/20201213213502964.png"></p><h2 id="Application-and-Open-Question"><a href="#Application-and-Open-Question" class="headerlink" title="Application and Open Question"></a>Application and Open Question</h2><p>前面讲的是Graph Generation的Task 1：Generating Realistic Graphs，这一小节通过一个药物发现的例子讲解Task 2：Goal-directed graph generation。</p><p>这里我就也不看了。</p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【AI研习社】KDD 2020丨欺诈检测研究现状以及欺诈者对抗行为建模</title>
      <link href="/2020/12/12/ai-yan-xi-she-kdd-2020-gun-qi-zha-jian-ce-yan-jiu-xian-zhuang-yi-ji-qi-zha-zhe-dui-kang-xing-wei-jian-mo/"/>
      <url>/2020/12/12/ai-yan-xi-she-kdd-2020-gun-qi-zha-jian-ce-yan-jiu-xian-zhuang-yi-ji-qi-zha-zhe-dui-kang-xing-wei-jian-mo/</url>
      
        <content type="html"><![CDATA[<h2 id="直播介绍"><a href="#直播介绍" class="headerlink" title="直播介绍"></a>直播介绍</h2><p><a href="https://live.yanxishe.com/room/865">https://live.yanxishe.com/room/865</a></p><p><img src="https://mooc.yanxishe.com/files/course/2020/08-20/1025062a5a1f089725.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glle91arawj20o00dijxu.jpg"></p><p>Review 1和Review 3是水军，其中Review 1是由GAN自动生成的，实在是牛逼。</p><h2 id="1-Background-a-history-of-fraud"><a href="#1-Background-a-history-of-fraud" class="headerlink" title="1. Background: a history of fraud"></a>1. Background: a history of fraud</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllecjkkr2j20nc0dfadv.jpg"></p><p>欺诈的演变，哪里有钱，哪里就有欺诈，现在互联网上的金融有钱，所以出现了金融欺诈。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllejzsgnoj20zb0ink08.jpg"></p><p>欺诈者存在欺骗行为，而黑客不一定存在欺诈。</p><p>是异常而不是欺诈。</p><p>欺诈和异常的区别，异常是客观的，是分布上的，比如离群点，而欺诈并不是这样子的，它是主观的，具有领域知识。</p><p>交叉领域。</p><h2 id="2-Background-fraud-type-and-fraud-detectors"><a href="#2-Background-fraud-type-and-fraud-detectors" class="headerlink" title="2. Background: fraud type and fraud detectors"></a>2. Background: fraud type and fraud detectors</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllenuk5exj20ys0j2n6p.jpg"></p><p>要做欺诈检测（Fraud Detection），首先要知道什么叫做欺诈（Fraud），在当今社会，按照欺诈发生的领域，主要有3类：</p><ol><li>Social Network<ul><li><strong>Fake Reviews</strong> –&gt; GraphConsis</li><li>Social Bots</li><li>Misinformation（错误信息）</li><li>Disinformation（假情报）</li><li>Fake Accounts</li><li>Social Sybils（社会女巫？）</li><li>Link Advertising</li></ul></li><li><strong>Finance</strong>（蚂蚁金服在做）<ul><li>Insurance Fraud</li><li><strong>Loan Defaulter</strong> –&gt; SemiGNN </li><li>Money Laundering（洗钱）</li><li><strong>Malicious Account</strong>（恶意账户） –&gt; GEM、GeniePath</li><li>Transaction Fraud</li><li>Cash-out User（现金不足用户）</li><li>Credit Card Fraud（信用卡欺诈）</li></ul></li><li>Others<ul><li>Advertisement（虚假CTR）</li><li>Mobile Apps（下载量造假）</li><li>Ecommerce（薅羊毛）</li><li>Crowdturfing（有不良商家就雇人给自己的商品写正面评论，甚至败坏竞争对手的名声）</li><li>Bitcoin Fraud</li><li>Game</li><li>Email，SMS，Phones</li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glleuo69bej20y10i5qbf.jpg" alt=")$65KW4K04FJGPS~2D0KJQJ.png"></p><p><strong>模态角度分类</strong>：</p><p>Content-based Detectors：比如虚假评论，就是看虚假评论内容进行分辨</p><p>Behavior-based Detectors：基于它在时间维度上的变化，进行分辨</p><p>Graph-based Detectors</p><p><strong>技术角度分类</strong>：</p><p>Ruled-based Detectors：专家知识</p><p>Feature-based Detectors：逻辑回归，决策树</p><p>Deep learning-based Detectors：端到端</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllevfl01ej20y60hg45j.jpg" alt="7QPSQQ6[W5@DE~74]P1AGC1.png"></p><p>![DE6B~GAL7QMD`Z4ITDM)}ZP.png](<a href="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllf2w4ue4j20vo0kr7k6.jpg">http://ww1.sinaimg.cn/large/9b63ed6fgy1gllf2w4ue4j20vo0kr7k6.jpg</a>)</p><p>应用宝和咸鱼</p><p>复杂的对抗行为，给你买好评。</p><h2 id="3-Resource：dataset-toolbox-paper-survey-company-etc"><a href="#3-Resource：dataset-toolbox-paper-survey-company-etc" class="headerlink" title="3. Resource：dataset,toolbox,paper,survey,company,etc."></a>3. Resource：dataset,toolbox,paper,survey,company,etc.</h2><h3 id="SafeGraph"><a href="#SafeGraph" class="headerlink" title="SafeGraph"></a>SafeGraph</h3><blockquote><p><a href="https://github.com/safe-graph">https://github.com/safe-graph</a></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glml5cwvvwj20lc093goj.jpg" alt="开源的GitHub仓库"></p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><h4 id="OODS-dataset（Outlier-Detection-DataSets）"><a href="#OODS-dataset（Outlier-Detection-DataSets）" class="headerlink" title="OODS dataset（Outlier Detection DataSets）"></a>OODS dataset（Outlier Detection DataSets）</h4><blockquote><p> <a href="http://odds.cs.stonybrook.edu/">http://odds.cs.stonybrook.edu/</a></p></blockquote><ul><li>包含五种类别的数据：<ol><li><strong>Multi-dimensional point datasets</strong>: There is one record per data point, and each record contains several attributes.</li><li><strong>Time series graph datasets for event detection</strong>: Temporal graph data where the graph changes dynamically over time in which new nodes and edges arrive or existing nodes and edges disappear.</li><li><strong>Time series point datasets (Multivariate/Univariate)</strong>: Temporal point data where each point has one or more attributes and the attributes change over time.</li><li><strong>Adversarial/Attack scenario and security datasets</strong>: Opinion fraud detection data from online review system. Cyber security data, e.g. intrusion detection with DoS, DDoS etc. attack scenario. </li><li><strong>Crowded scene video data for anomaly detection</strong>: Video clips acquired with camera.</li></ol></li></ul><h4 id="Bitcoin-dataset❗"><a href="#Bitcoin-dataset❗" class="headerlink" title="Bitcoin dataset❗"></a>Bitcoin dataset❗</h4><blockquote><p><a href="https://www.kaggle.com/ellipticco/elliptic-data-set">https://www.kaggle.com/ellipticco/elliptic-data-set</a></p></blockquote><h5 id="Description"><a href="#Description" class="headerlink" title="Description"></a><strong>Description</strong></h5><p>将比特币交易作为实体节点，通过机器学习的方法将比特币交易分类成合法和不合法两个类别。<strong>这是一个Node Level的二分类问题。</strong></p><p>The Elliptic Data Set maps Bitcoin transactions to real entities belonging to licit categories (exchanges, wallet providers, miners, licit services, etc.) versus illicit ones (scams, malware, terrorist organizations, ransomware, Ponzi schemes, etc.). The task on the dataset is to classify the illicit and licit nodes in the graph.</p><h5 id="Content"><a href="#Content" class="headerlink" title="Content"></a><strong>Content</strong></h5><p>This anonymized data set is a transaction graph collected from the Bitcoin blockchain. <strong>A node</strong> in the graph represents a transaction, <strong>an edge</strong> can be viewed as a flow of Bitcoins between one transaction and the other. Each node has 166 features and has been labeled as being created by a “licit”, “illicit” or “unknown” entity.</p><p>Nodes and edges</p><ul><li>The graph is made of <strong>203,769 nodes</strong> and <strong>234,355 edges</strong>. </li><li><strong>Two percent (4,545)</strong> of the nodes are labelled class1 (illicit). </li><li><strong>Twenty-one percent (42,019)</strong> are labelled class2 (licit). </li><li>The remaining transactions are not labelled with regard to licit versus illicit.</li></ul><p>Features</p><ul><li><p>There are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. </p></li><li><p>There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. The time steps, running from 1 to 49, are evenly spaced with an interval of about two weeks. Each time step contains a single connected component of transactions that appeared on the blockchain within less than three hours between each other; there are no edges connecting the different time steps.</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gln58563h9j20v20abwez.jpg" alt="time_step"></p></li><li><p>The first 94 features represent local information about the transaction – including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs.The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).</p></li></ul><blockquote><p>使用到这个数据集的论文有：</p><ol><li><a href="https://arxiv.org/abs/1902.10191">EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</a></li></ol></blockquote><h4 id="Yelp-and-Amazon"><a href="#Yelp-and-Amazon" class="headerlink" title="Yelp and Amazon"></a>Yelp and Amazon</h4><blockquote><p><a href="https://github.com/safe-graph/DGFraud">https://github.com/safe-graph/DGFraud</a></p></blockquote><p>For <a href="https://arxiv.org/abs/2005.00625">GraphConsis</a>, we preprocessed <a href="http://odds.cs.stonybrook.edu/yelpchi-dataset/">Yelp Spam Review Dataset</a> with reviews as nodes and three relations as edges.</p><p>The dataset with <code>.mat</code> format is located at <code>/dataset/YelpChi.zip</code>. The <code>.mat</code> file includes:</p><ul><li><code>net_rur, net_rtr, net_rsr</code>: three sparse matrices representing three homo-graphs defined in <a href="https://arxiv.org/abs/2005.00625">GraphConsis</a>paper;</li><li><code>features</code>: a sparse matrix of 100-dimension Bag-of-words features;</li><li><code>label</code>: a numpy array with the ground truth of nodes. <code>1</code> represents spam and <code>0</code> represents benign.</li></ul><p>To get the complete metadata of the Yelp dataset, please send an email to <a href="mailto:ytongdou@gmail.com">ytongdou@gmail.com</a> for inquiry.</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gln61ay6pzj20j704laa7.jpg"></p><h4 id="百度飞桨数据集"><a href="#百度飞桨数据集" class="headerlink" title="百度飞桨数据集"></a>百度飞桨数据集</h4><p>Code &amp; Toolbox</p><ul><li>PyOD<ul><li><a href="https://github.com/yzhao062/pyod">https://github.com/yzhao062/pyod</a></li></ul></li><li>PyoDD<ul><li><a href="https://github.com/datamllab/pyodds">https://github.com/datamllab/pyodds</a></li></ul></li><li>Paper &amp; Code list<ul><li><a href="https://github.com/safe-graph/graph-fraud-detection-papers">https://github.com/safe-graph/graph-fraud-detection-papers</a></li></ul></li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glmldfqtqsj20la0bvn3n.jpg" alt="重要资源"></p><ul><li><strong>KDD 2020 TrueFact Workshop: Making a Credible Web for Tomorrow</strong><ul><li><a href="https://www.microsoft.com/en-us/research/event/kdd-2020-truefact-workshop-making-a-credible-web-for-tomorrow/#lprogram-schedule">https://www.microsoft.com/en-us/research/event/kdd-2020-truefact-workshop-making-a-credible-web-for-tomorrow/#lprogram-schedule</a></li></ul></li><li><strong>KDD 2020 Workshop on Machine Learning in Finance</strong><ul><li><a href="https://sites.google.com/view/kdd-mlf-2020/home">https://sites.google.com/view/kdd-mlf-2020/home</a></li></ul></li><li><strong>KDD 2020 Deep Anomaly Detection Tutorial</strong><ul><li><a href="https://youtu.be/-KTJ841Pz1A">https://youtu.be/-KTJ841Pz1A</a></li></ul></li><li><strong>AI for Anti-Money Laundering Blog</strong><ul><li><a href="https://www.markrweber.com/graph-deep-learning">https://www.markrweber.com/graph-deep-learning</a></li></ul></li><li><strong>Awesome Fraud Detection Research Papers.</strong><ul><li><a href="https://github.com/benedekrozemberczki/awesome-fraud-detection-papers">https://github.com/benedekrozemberczki/awesome-fraud-detection-papers</a></li></ul></li></ul><h3 id="Scholar"><a href="#Scholar" class="headerlink" title="Scholar"></a>Scholar</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glmleroisyj20ks089n06.jpg" alt="重要人物"></p><blockquote><p>这些学者都有和工业界的合作</p></blockquote><table><thead><tr><th>Name</th><th>School</th><th>Lab Link</th><th>Overseas/Domestic</th></tr></thead><tbody><tr><td>Christos Faloutsos</td><td>CMU</td><td></td><td>Overseas</td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table><p>Company</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glmlhi6kqtj20j6055t9x.jpg" alt="重要人物"></p><h2 id="Summary-and-Q-amp-A"><a href="#Summary-and-Q-amp-A" class="headerlink" title="Summary and Q&amp;A"></a>Summary and Q&amp;A</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glmlub4iyaj20hf0aztax.jpg"></p><p>好发论文的点：</p><ul><li>发现新的欺诈类型（有一个新问题，将传统的方法做一些改变和适应就能够解决）</li><li>提高模型的效率（GNN的效率没有传统的模型高的）</li><li>Model Ensemble（模型集成和融合）</li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glmltmmw7oj20e700vglm.jpg"></p><p>工业界：</p><ul><li>Define problem clearly,find appropriate model定义问题明确（水军是适合规则？图？Feature来建模？）</li><li>采样很重要（特别针对GNN来说，闲鱼的一篇论文可以）</li><li>Cost &amp; return trade off</li><li>Old but good（XGBoost）</li><li>Early detection is a challenge（Early Detection很难做）</li></ul><blockquote><p>一些数据集都是社交网络的。</p></blockquote><h2 id="KDD20-spammer-adversarial-behavior-and-spamming-practical-effect"><a href="#KDD20-spammer-adversarial-behavior-and-spamming-practical-effect" class="headerlink" title="KDD20: spammer adversarial behavior and spamming practical effect"></a>KDD20: spammer adversarial behavior and spamming practical effect</h2><blockquote><p>非深度学习</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllfy4j5b8j20y10f4dnr.jpg"></p><p>动态博弈建模水军：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllfzoc3m2j20y80j7123.jpg"></p><p>新的衡量指标：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllg259qhkj20wk0k412o.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllg3koe51j20xq0i1n87.jpg" alt="P~5$58%FX(QC~3AN37I02{5.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllg49wpcpj20xq0jak0f.jpg" alt="LG~.png"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllg5p8956j20y80iuk10.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllg75svfcj20vp0fc45u.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllga5v7ckj20u10j57ew.jpg"></p><p>![5Q64V(LB$YHMW@AJ@`F0)T1.png](<a href="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllgba1z6nj20y20k511g.jpg">http://ww1.sinaimg.cn/large/9b63ed6fgy1gllgba1z6nj20y20k511g.jpg</a>)</p><p><img src="%E3%80%90AI%E7%A0%94%E4%B9%A0%E7%A4%BE%E3%80%91KDD-2020%E4%B8%A8%E6%AC%BA%E8%AF%88%E6%A3%80%E6%B5%8B%E7%A0%94%E7%A9%B6%E7%8E%B0%E7%8A%B6%E4%BB%A5%E5%8F%8A%E6%AC%BA%E8%AF%88%E8%80%85%E5%AF%B9%E6%8A%97%E8%A1%8C%E4%B8%BA%E5%BB%BA%E6%A8%A1/image-20201212224120421.png" alt="image-20201212224120421"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllgd2yj4nj20uv0in7d1.jpg"></p><p>看不懂，直接跳过噜</p><h2 id="SIGIR20-amp-CIKM20-how-to-apply-GNN-to-fraud-detection-problems"><a href="#SIGIR20-amp-CIKM20-how-to-apply-GNN-to-fraud-detection-problems" class="headerlink" title="SIGIR20&amp;CIKM20: how to apply GNN to fraud detection problems"></a>SIGIR20&amp;CIKM20: how to apply GNN to fraud detection problems</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllgi4ll79j20xe0hd7fc.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllgizybupj20uq0h87e2.jpg"></p><p>camouflage /ˈkæməflɑːʒ/ v. 伪装</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllgkofo3lj20z10k9tjz.jpg"></p><p>混合的模型，就是模型里面啥都有。</p><p>IP地址不再适用，通过代理等等方式，改变大家之间的relation，从而成功伪装。这些伪装给GNN带来挑战。</p><p>设计模型的目标就是为了去克服这些困难。</p><p>三个模块来解决问题。</p><p>第一个模块：</p><p>必须要引入一些外部只是来进行辅助训练。</p><p>第二个模块：</p><p>不用学习relation</p><p>注意力机制训练慢</p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Semi-supervised Graph Attentive Network for Financial Fraud Detection</title>
      <link href="/2020/12/12/a-semi-supervised-graph-attentive-network-for-financial-fraud-detection/"/>
      <url>/2020/12/12/a-semi-supervised-graph-attentive-network-for-financial-fraud-detection/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考文章：<a href="https://blog.csdn.net/weixin_40505645/article/details/105124245">https://blog.csdn.net/weixin_40505645/article/details/105124245</a></p></blockquote><h2 id="0-原作信息"><a href="#0-原作信息" class="headerlink" title="0. 原作信息"></a>0. 原作信息</h2><pre><code class="text">@inproceedings{wang2019semi,  title={A Semi-supervised Graph Attentive Network for Financial Fraud Detection},  author={Wang, Daixin and Lin, Jianbin and Cui, Peng and Jia, Quanhui and Wang, Zhen and Fang, Yanming and Yu, Quan and Zhou, Jun and Yang, Shuang and Qi, Yuan},  booktitle={2019 IEEE International Conference on Data Mining (ICDM)},  pages={598--607},  year={2019},  organization={IEEE}}</code></pre><h2 id="1-Contribution-本文贡献"><a href="#1-Contribution-本文贡献" class="headerlink" title="1. Contribution 本文贡献"></a>1. Contribution 本文贡献</h2><ul><li><p><strong>提出了第一个用于金融反欺诈的semi-supervised GNN</strong>，能够同时利用有标签数据和无标签数据（通过设计两个$Loss Function$进行实现）。</p></li><li><p>聚合了异构信息。</p></li><li><p><strong>提出了一个Hierarchical Attention机制</strong>，该机制先在一个user的各个view上实行注意力机制，再在各个view之间实行注意力机制，增强了模型可解释性。</p></li><li><p>在花呗的两个任务上，结果体现得很work。</p></li></ul><h2 id="2-Background-背景知识"><a href="#2-Background-背景知识" class="headerlink" title="2. Background 背景知识"></a>2. Background 背景知识</h2><ul><li><p>任务: <strong>欺诈检测,Fraud Detection</strong></p><p>可视作分类任务,由于异常点的采集,标注极其耗费人力物力,数据集常为<strong>少量有标签</strong>数据(labeled data),及<strong>大量无标签</strong>数据(unlabeled data).</p><p>模型需要具备<strong>解释性</strong>, 能向部署方, 被检测者说明检测的判别标准。</p></li><li><p>过往检测方式</p><ul><li>基于规则，Rule-based<ul><li>原理: 假定欺诈活动基于一定可观测的模式</li><li>缺点: 耗费人力手工设定, 依赖专家知识; 容易被攻击</li></ul></li><li>基于统计特征的机器学习模型<ul><li>如SVM，树模型等等</li><li>缺点：只考虑了实体特征，没有考虑实体之间的关系</li></ul></li><li>最近的图表示学习的尝试<ul><li>没有利用无标签数据，模型缺乏可解释性</li></ul></li></ul></li><li><p>综上，欺诈检测建模有如下<strong>灵魂三问</strong>：</p><ol><li><p><strong>How to bridge the labeled data with the unlabeled data？</strong></p><p>如何处理不平衡的数据（labeled的数据少，unlabeled的数据多），利用无标签的数据？（Semi-supervised，使用social relation graph将labeled data和unlabeled data联系起来）</p></li><li><p><strong>How to model the data heterogeneity？</strong></p><p>如何聚合用户的来自不同view的异质信息？</p></li><li><p><strong>How to learn an interpretable model?</strong></p><p>怎么保证模型的可解释性？</p></li></ol></li></ul><h2 id="3-The-Model-Semi-GNN-模型"><a href="#3-The-Model-Semi-GNN-模型" class="headerlink" title="3. The Model Semi-GNN 模型"></a>3. The Model Semi-GNN 模型</h2><h3 id="3-1-Problem-Definition-and-Notations-问题定义及数学符号含义"><a href="#3-1-Problem-Definition-and-Notations-问题定义及数学符号含义" class="headerlink" title="3.1 Problem Definition and Notations 问题定义及数学符号含义"></a>3.1 Problem Definition and Notations 问题定义及数学符号含义</h3><ul><li><p>Multiple View的构建</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gll31rl6iij20j30gd0vy.jpg"></p><ul><li>重点研究<em>User</em>用户类型节点, 图中有标签数据&lt;&lt;无标签数据</li><li>异构图（文中记为multiview graph=a collection of view-specific graph），$G^v = {U \cup S^v},v \in \lbrace 1,…,m \rbrace$，$v$就是view-specific graph，$U = U_L \cup U_{UL}$</li><li>view由具体业务确定, 本文中部分view是从用户特征处理产生的, 以增加结构信息. (详见实验部分数据集处理)</li></ul></li><li><p>数学符号含义</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gll36gujw4j20jo07ewfd.jpg"></p></li></ul><h3 id="3-2-整体框架"><a href="#3-2-整体框架" class="headerlink" title="3.2 整体框架"></a>3.2 整体框架</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gll39hxz1zj20yz0k1dhz.jpg" alt="model framework"></p><p>上面的架构图回答了下面的三个问题：</p><ol><li>In each view-specific graph,how to assemble the user’s neighbors?</li><li>How to assemble multiview data to obtain user embedding?</li><li>How to model the labeled and unlabeled user simultaneously?</li></ol><p><strong>node-level</strong>（聚合各个view-specific graph内部的信息）→ <strong>view-level</strong>（聚合所有view-specific graph的信息）→ <strong>supervised classification loss 和 unsupervised graph-based loss</strong>（前者重点考虑feature带来的信息，后者考虑图结构带来的信息）。</p><h3 id="3-3-Node-Level"><a href="#3-3-Node-Level" class="headerlink" title="3.3 Node Level"></a>3.3 Node Level</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gll44v3x29j20t60cejs1.jpg" alt="node level"></p><p>对于一个user $u$，它的邻居结点$i$，给它传递的特征表示设为$e^{v}_{ui}$。</p><p>∵ 不同的邻居节点对节点的$u$的影响不同，引入Attention来自适应学习邻居节点的权重：<br>$$<br>\alpha_{ui}^{v} = \frac { \exp (e_{ui}^{v} · H_{ui}^{v}) }   {\sum_{k \in \cal N_u^v} \exp (e^v_{uk} · H^v_{uk}) }<br>$$<br> ，其中$H_{u}^{v}$是可以学习到的参数矩阵。</p><p>∴ 对于节点$u$在view $v$下的embedding，可以表示成：<br>$$<br>h_{u}^{v} = \sum_{k \in \cal {N_u^v}} \alpha_{uk}^v e_{uk}^{v}<br>$$</p><h3 id="3-4-View-Level"><a href="#3-4-View-Level" class="headerlink" title="3.4 View Level"></a>3.4 View Level</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gll4y4xeiij20kh0bnjrz.jpg" alt="view level"></p><ol><li><p>MLP</p><p>由node-level得到的$h_u^v$是属于各自view内(异构)的embedding, 为了聚合各个view时能更好的利用相互关联性（capture the multiview correlations）, 首先用MLP将之映射到高维的语义空间（high-level semantic space）。（low-level space → high-level space），即有：<br>$$<br>h_u^{v(l)} = Relu (h_u^{v(l-1)}W_l^{v}+b_l^{v}),v \in \lbrace 2,…,m \rbrace<br>$$</p></li><li><p>View-Attention</p><p>不同view对User的embedding影响也不同，同样，引入Attention来自适应：</p></li></ol><p>$$<br>\alpha_u^v = \frac<br>{\exp (h_{u}^{v(L)}·\phi_u^v)}<br>{\sum_{k \in \lbrace 1,…,m \rbrace} \exp(h_u^{k(L)}·\phi_u^{k})} v \in \lbrace1,…,m\rbrace<br>$$</p><p>最后, 连接在attention加权后各view节点表示,得到User的embedding.<br>$$<br>h_u = ||_{v=1}^{m}(\alpha_u^v·h_u^{v(L)})<br>$$<br>$m$为view个数，$||$为连接.</p><h3 id="3-5-Loss损失函数设计"><a href="#3-5-Loss损失函数设计" class="headerlink" title="3.5 Loss损失函数设计"></a>3.5 Loss损失函数设计</h3><ol><li><p>标签数据</p><p>使用softmax函数处理multi-classification问题即可。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gll5miqq8cj20h403kaa9.jpg" alt="loss for labeled data"></p></li><li><p>无标签数据</p><p>现象：<strong>欺诈常呈团伙聚集, 标注为负样本的用户, 其邻居节点也可疑.</strong></p><p>基于上述假设, <strong>受DeepWalk启发</strong>, 作者设计的<em>Loss</em>希望：邻近节点的表示是相似的, 同时不同节点的表示差异较大. （encourages nearby nodes having similar representaions while makes the representations of disparate nodes distinct）</p><p>所以无标签数据的损失函数就和DeepWalk里面的损失函数长得一样了：</p></li></ol><p>$$<br>\cal{L_{graph}} = \sum_{u \in \bf U}<br>                (<br>                \sum_{v \in \cal{N_v} \cup {Neg_u} }<br>                (<br>                -log(\sigma(a_u^{T}a_v)-Q·E_{q \sim P_{neg(u)}}log(\sigma(a_u^{T}a_q)))<br>                )<br>                )<br>$$</p><p>其中，$\cal{N_u}$ 为节点$u$的邻居，$\cal{Neg_u}$为节点$u$的负采样邻居，$\sigma$为sigmoid函数，$P_{neg}(u) \propto d_{u}^{0.75} $为负采样的分布，$Q$为负采样的采样数量（本文取3）。</p><p>综上所述，我们可以得到整个模型的Loss：<br>$$<br>\cal{L_{semiGNN}} = \alpha·\cal{L_{sup}} + (1-\alpha)·\cal{L_{graph}} + \lambda \cal{L_{reg}}<br>$$<br>其中，$\alpha$用于权衡监督和无监督之间的重要性，${\mathcal L}_{reg}$则是正则化系数。</p><h3 id="3-6-模型分析"><a href="#3-6-模型分析" class="headerlink" title="3.6 模型分析"></a>3.6 模型分析</h3><p>模型主要特点：</p><ol><li>综合使用异构的数据（multiview）</li><li>可能有较好的解释性（attention）</li><li>SemiGNN是inductive method，能够运用到从未见过的样本上（inductive）</li><li>可并行训练的。从之前的伪代码和framework图可以总结出，时间复杂度为$O(I·|E|·d·m)$，然后模型的训练是以边为单位的，所以可以实现并行处理。</li></ol><h3 id="3-7-算法伪代码"><a href="#3-7-算法伪代码" class="headerlink" title="3.7 算法伪代码"></a>3.7 算法伪代码</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gll9sa9smzj20kq0n5tcx.jpg" alt="pseudo code"></p><h2 id="4-Experiment-实验"><a href="#4-Experiment-实验" class="headerlink" title="4. Experiment 实验"></a>4. Experiment 实验</h2><p>实验结果需要能够验证四件事情：</p><ul><li>Can SemiGNN learn better embeddings for fraud detection compared with existing methods?</li><li>Whether the proposed hierarchical attention mechanism can help improve the model’s performance and give interpretable results?</li><li>Whether the performance of our model can benefit from the unlabeled data?</li><li>Is SemiGNN sensitive to the parameters and how the performance will be affected?</li></ul><h3 id="4-1-Dataset-数据集"><a href="#4-1-Dataset-数据集" class="headerlink" title="4.1 Dataset 数据集"></a>4.1 Dataset 数据集</h3><p>花呗内部数据: 大约4,000,000 有标签用户，总共100, 000, 000用户。address的大小为300k，nicks的大小为500k，apps的大小为20k。</p><p>预处理后, 保留有约90% 的用户, 这些用户的相关view数据确实较少。</p><p>预处理后构建了如下4个图（其中有3种view）：</p><ol><li>user-relation（用户间关系）</li><li>user-app（二部图）</li><li>user-nick（二部图）</li><li>user-address（二部图）</li></ol><h3 id="4-2-实验任务"><a href="#4-2-实验任务" class="headerlink" title="4.2 实验任务"></a>4.2 实验任务</h3><ol><li>user default prediction 用户违约预测（default有“违约”的意思，活久见）</li><li>user attributes prediction 用户特征预测 （本文为用户职业预测）</li></ol><h3 id="4-3-实验结果"><a href="#4-3-实验结果" class="headerlink" title="4.3 实验结果"></a>4.3 实验结果</h3><p>超越baseline小几个点.</p><p><strong>1）user default prediction</strong> :</p><p><img src="https://img-blog.csdnimg.cn/20200326175303237.png#pic_center"></p><p><strong>2）user attributes prediction</strong> :</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllb0m4epoj21cy0e6409.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllayff3srj20hr0a8myr.jpg"></p><p><strong>3) The effect of different views</strong> :</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllb1hh5gjj20j606umy4.jpg"></p><p>验证不同的view的影响，$SemiGNN_x$代表只使用$x$这一种view。从实验结果也可以看出每种view对两个任务的重要程度还是不一样的。</p><h3 id="4-4-模型可解释性"><a href="#4-4-模型可解释性" class="headerlink" title="4.4 模型可解释性"></a>4.4 模型可解释性</h3><p>根据任务的特点，为<strong>用户违约预测任务</strong>选择注意力在top 15的<strong>app</strong>，为<strong>用户职业预测——医生</strong>选择注意力在top 15的<strong>nicks</strong>和<strong>addresses</strong>。结果如下表：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllbgrub3uj215e0gadms.jpg"></p><h3 id="4-5-参数敏感性-Parameter-Sensitivity"><a href="#4-5-参数敏感性-Parameter-Sensitivity" class="headerlink" title="4.5 参数敏感性 Parameter Sensitivity"></a>4.5 参数敏感性 Parameter Sensitivity</h3><p>深度学习重在调参，看看参数的论文中提到的参数对<strong>用户违约预测任务（二分类问题）</strong>的影响吧：</p><blockquote><p>二分类问题使用AUC作为度量指标没问题，而且这里AUC应该比P-R曲线更有说服力。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gllbkzvkzij213x0cz40b.jpg"></p><ol><li>Dimension of Final Embedding：在达到最高值之后，会随着维度增加而减少，原因在于维度维度越大，越容易出现overfitting和redundancy；</li><li>Dimension of the initial node embedding：变化并不明显；</li><li>The value of $\alpha$：可以看出supervised的重要性更高，但是如果过度重视supervised，而忽视unsupervised，则会导致效果的急速下降。</li></ol><h2 id="5-我的思考"><a href="#5-我的思考" class="headerlink" title="5. 我的思考"></a>5. 我的思考</h2><ol><li><p><strong>本文关于异构信息的处理</strong></p><p>实质上异构数据是单独处理, 再通过Attention连接来做embedding, 从结果来尚算喜人。</p></li><li><p><strong>模型的可解释性仍是薄弱</strong></p><p>文中对模型可解释性的分析， <strong>仅是基于对若干只采用单一view的模型性能做分析, 进而比较attention系数的差异。</strong></p></li><li><p><strong>为什么node-level之后还需要经过L层的multiple layer perceptron之后才能聚合？</strong></p><p><strong>多模态的知识</strong>。文章的大意是刚刚出来的每个node level的信息都是相关性极其弱的，因此，我们使用需要经过MLP，来使得它们具有相关性，也正是有了相关性，我们的view-level的注意力机制才有意义！否则，对于没有相关性的数据进行注意力，可能效果不好的（不知注意力机制的论文有没有提到这一点）</p><p>Ngiam, Jiquan, et al. ”Multimodal deep learning.” Proceedings of the 28th international conference on machine learning (ICML-11). 2011.</p></li><li><p><strong>为什么要分view来讨论特征？</strong></p><p>首先，文章有提到，你完全可以将所有view组合在一起作为特征vector（如Baseline中的XGBoost）。但是，作<strong>者认为如果建模不同任务和不同顶点对不同view的偏好，将会达到更好的效果。</strong></p></li><li><p>回归淳朴，回到了最初的<strong>DeepWalk</strong>的思想，使用跟它们一样的损失函数，同时解决了node embedding这种无监督学习的缺陷，感觉更像是对node embedding的改进！！！为它引入supervised的信息。同时，输入DeepWalk模型的不再是embedding look-up，而是$a_u$（从view-specific graphs中提取出来的特征）</p></li><li><p>度量指标<strong>KS</strong>是什么东西？</p><p>Friedman, Jerome H. ”A recursive partitioning decision rule for nonparametric classification.” IEEE Transactions on Computers 4 (1977): 404-408.</p></li><li><p>论文可以改进的地方：</p><p><strong>论文作者自己说以后要在relation graph的relation type上再做区分（家人，朋友，同事）。</strong></p></li><li><p>hexo公式解析有坑，得对于$x_{y}^{z}$，等写成x_{y}^{z}，先下后上。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 金融反欺诈 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多分类，多标签模型的Accuracy, Precision, Recall和F1-score</title>
      <link href="/2020/12/11/duo-fen-lei-mo-xing-accuracy-precision-recall-he-f1-score-de-chao-ji-wu-di-shen-ru-tan-tao/"/>
      <url>/2020/12/11/duo-fen-lei-mo-xing-accuracy-precision-recall-he-f1-score-de-chao-ji-wu-di-shen-ru-tan-tao/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>众所周知，机器学习分类模型常用评价指标有Accuracy, Precision, Recall和F1-score，而回归模型最常用指标有MAE和RMSE。但是我们真正了解这些评价指标的意义吗？</p><p><strong>在具体场景（如不均衡多分类）中到底应该以哪种指标为主要参考呢？多分类模型和二分类模型的评价指标有啥区别？多分类问题中，为什么Accuracy = micro precision = micro recall = micro F1-score? 什么时候用macro, weighted, micro precision/ recall/ F1-score?</strong></p><p>这几天为了回复严谨（划去: 刁难）的reviewer，我查阅了一些文章，总算是梳理清楚啦。在这里分享给大家，权当做个总结。今天要讲的主要分为以下两点：</p><ul><li><strong>二分类</strong>模型的常见指标<strong>快速回顾</strong></li><li><strong>多分类</strong>模型的常见指标<strong>详细解析</strong></li><li><strong>多标签</strong>模型的常见指标<strong>详细解析</strong></li></ul><p>在探讨这些问题前，让我们先回顾一下最常见的指标Accuracy到底有哪些不足。</p><p>Accuracy是分类问题中最常用的指标，<strong>它计算了分类正确的预测数与总预测数的比值</strong>。但是，<font color="red"><strong>对于不平衡数据集而言，Accuracy并不是一个好指标</strong></font>。为啥？</p><p>假设我们有100张图片，其中91张图片是「狗」，5张是「猫」，4张是「猪」，我们希望训练一个三分类器，能正确识别图片里动物的类别。其中，狗这个类别就是大多数类 (majority class)。当大多数类中样本（狗）的数量远超过其他类别（猫、猪）时，如果采用Accuracy来评估分类器的好坏，那么即便模型性能很差 (如无论输入什么图片，都预测为「狗」)，也可以得到较高的Accuracy Score（如91%）。此时，虽然Accuracy Score很高，但是意义不大。<strong>当数据异常不平衡时，Accuracy评估方法的缺陷尤为显著。</strong></p><p>因此，我们需要引入Precision （精准度），Recall （召回率）和F1-score评估指标。考虑到二分类，多分类，多标签模型中，评估指标的计算方法<strong>略有不同</strong>，我们将其分开讨论。</p><h2 id="二分类模型的常见指标快速回顾"><a href="#二分类模型的常见指标快速回顾" class="headerlink" title="二分类模型的常见指标快速回顾"></a>二分类模型的常见指标快速回顾</h2><p>在二分类问题中，假设该样本一共有两种类别：Positive和Negative。当分类器预测结束，我们可以绘制出混淆矩阵（confusion matrix）。其中分类结果分为如下几种：</p><p><img src="https://pic2.zhimg.com/80/v2-0af2bd2decf49bbb45e31ef7091625e9_720w.jpg"></p><ul><li>True Positive (TP): 把正样本成功预测为正。</li><li>True Negative (TN)：把负样本成功预测为负。</li><li>False Positive (FP)：把负样本错误地预测为正。</li><li>False Negative (FN)：把正样本错误的预测为负。</li></ul><p><img src="https://pic1.zhimg.com/80/v2-763b2bc2e358ead002eca8d94e104db4_720w.jpg"></p><p>在二分类模型中，Accuracy，Precision，Recall和F1 score的定义如下：</p><p><img src="https://www.zhihu.com/equation?tex=Accuracy+=+%5Cfrac%7BTP+TN%7D%7BTP+TN+FP+FN%7D"></p><p><img src="https://www.zhihu.com/equation?tex=Precision+=+%5Cfrac%7BTP%7D%7BTP+FP%7D"></p><p><img src="https://www.zhihu.com/equation?tex=Recall+=+%5Cfrac%7BTP%7D%7BTP+FN%7D"></p><p><img src="https://www.zhihu.com/equation?tex=F1%5Ctext%7B-%7Dscore+=+%5Cfrac%7B2%5Ctimes+%5Ctext%7BPrecision%7D+%5Ctimes+%5Ctext%7BRecall%7D%7D%7B+%5Ctext%7BPrecision%7D+%5Ctext%7BRecall%7D%7D"></p><blockquote><p>F1-score的计算方式在数学中称为：调和平均。</p></blockquote><p>其中，Precision着重评估<strong>在预测为Positive的所有数据中，真实Positve的数据到底占多少？</strong>Recall着重评估：<strong>在所有的Positive数据中，到底有多少数据被成功预测为Positive?</strong></p><p>举个例子，一个医院新开发了一套癌症AI诊断系统，想评估其性能好坏。我们把病人得了癌症定义为Positive，没得癌症定义为Negative。那么， 到底该用什么指标进行评估呢？</p><p>如用Precision对系统进行评估，那么其回答的问题就是：</p><pre><code class="text">在诊断为癌症的一堆人中，到底有多少人真得了癌症？</code></pre><p>如用Recall对系统进行评估，那么其回答的问题就是：</p><pre><code class="text">在一堆得了癌症的病人中，到底有多少人能被成功检测出癌症？</code></pre><p>如用Accuracy对系统进行评估，那么其回答的问题就是：</p><pre><code class="text">在一堆癌症病人和正常人中，有多少人被系统给出了正确诊断结果（患癌或没患癌）？</code></pre><p><strong>OK，那啥时候应该更注重Recall而不是Precision呢？</strong></p><blockquote><p>当False Negative (FN)的成本代价很高 (后果很严重)，希望尽量避免产生FN时，应该着重考虑提高Recall指标。</p></blockquote><p>在上述例子里，False Negative是得了癌症的病人没有被诊断出癌症，这种情况是最应该避免的。我们宁可把健康人误诊为癌症 (FP)，也不能让真正患病的人检测不出癌症 (FN) 而耽误治疗离世。在这里，癌症诊断系统的目标是：尽可能提高Recall值，哪怕牺牲一部分Precision。</p><p><strong>那啥时候应该更注重Precision而不是Recall呢？</strong></p><blockquote><p>当False Positive (FP)的成本代价很高 (后果很严重)时，即期望尽量避免产生FP时，应该着重考虑提高Precision指标。</p></blockquote><p>以垃圾邮件屏蔽系统为例，垃圾邮件为Positive，正常邮件为Negative，False Positive是把正常邮件识别为垃圾邮件，这种情况是最应该避免的（你能容忍一封重要工作邮件直接进了垃圾箱，被不知不觉删除吗？）。我们宁可把垃圾邮件标记为正常邮件 (FN)，也不能让正常邮件直接进垃圾箱 (FP)。在这里，垃圾邮件屏蔽系统的目标是：尽可能提高Precision值，哪怕牺牲一部分recall。</p><p>而F1-score是Precision和Recall两者的综合。</p><p>举个更有意思的例子（我拍脑袋想出来的，绝对原创哈），假设检察机关想将罪犯捉拿归案，需要对所有人群进行分析，以判断某人犯了罪（Positive），还是没犯罪（Negative）。显然，检察机关希望不漏掉一个罪人（提高recall），也不错怪一个好人（提高precision），所以就需要同时权衡recall和precision两个指标。</p><p>尤其在上个世纪，中国司法体制会更偏向Recall，即「天网恢恢，疏而不漏，任何罪犯都插翅难飞」。而西方司法系统会更偏向Precision，即「绝不冤枉一个好人，但是难免有罪犯成为漏网之鱼，逍遥法外」。到底是哪种更好呢？显然，极端并不可取。Precision和Recall都应该越高越好，也就是F1应该越高越好。</p><p>呼，二分类问题的常见指标和试用场景终于讲完了。咦，说好的快速回顾呢？</p><h2 id="多分类模型的常见指标详细解析"><a href="#多分类模型的常见指标详细解析" class="headerlink" title="多分类模型的常见指标详细解析"></a>多分类模型的常见指标详细解析</h2><p>在多分类（大于两个类）问题中，假设我们要开发一个动物识别系统，来区分输入图片是猫，狗还是猪。给定分类器一堆动物图片，产生了如下结果混淆矩阵。</p><p><img src="https://pic4.zhimg.com/80/v2-e1cf922d05b7e1bf266620577e6fd253_720w.jpg"></p><p>在混淆矩阵中，正确的分类样本（Actual label = Predicted label）分布在左上到右下的对角线上。其中，Accuracy的定义为分类正确（对角线上）的样本数与总样本数的比值。<font color="red"><strong>Accuracy度量的是全局样本预测情况。而对于Precision和Recall而言，每个类都需要单独计算其Precision和Recall</strong></font>。</p><p><img src="https://pic4.zhimg.com/v2-77e85aad7db38c1f4a2116af5fb10a5b_r.jpg"></p><p>比如，对类别「猪」而言，其Precision和Recall分别为:</p><p><img src="https://www.zhihu.com/equation?tex=Precision+=+%5Cfrac%7BTP%7D%7BTP+FP%7D+=+%5Cfrac%7B20%7D%7B20+50%7D+=+2/7"></p><p><img src="https://www.zhihu.com/equation?tex=Recall+=+%5Cfrac%7BTP%7D%7BTP+FN%7D+=+%5Cfrac%7B20%7D%7B20+10%7D+=+2/3"></p><p>也就是，</p><p><img src="https://www.zhihu.com/equation?tex=P_%7Bcat%7D=8/15,+P_%7Bdog%7D+=+17/23,P_%7Bpig%7D=2/7"> （P代表Precision）</p><p><img src="https://www.zhihu.com/equation?tex=R_%7Bcat%7D=4/7,+R_%7Bdog%7D+=+17/32,R_%7Bpig%7D=2/3"> （R代表Recall）</p><p>如果想评估该识别系统的总体功能，必须考虑猫、狗、猪三个类别的综合预测性能。那么，<strong>到底要怎么综合这三个类别的Precision呢？</strong>是简单加起来做平均吗？通常来说， 我们有如下几种解决方案（也可参考<a href="https://link.zhihu.com/?target=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">scikit-learn官网</a>）：</p><h3 id="1-Macro-average方法"><a href="#1-Macro-average方法" class="headerlink" title="1. Macro-average方法"></a><strong>1. Macro-average方法</strong></h3><p>该方法最简单，直接将不同类别的评估指标（Precision/ Recall/ F1-score）加起来求平均，给所有类别相同的权重。<strong>该方法能够平等看待每个类别，但是它的值会受稀有类别影响。</strong></p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BMacro-Precision%7D+=+%5Cfrac%7B%7BP%7D_%7Bcat%7D++P_%7Bdog%7D+++P_%7Bpig%7D+%7D%7B3%7D+=+0.5194"></p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BMacro-Recall%7D+=+%5Cfrac%7BR_%7Bcat%7D+++R_%7Bdog%7D+++R_%7Bpig%7D+%7D%7B3%7D+=+0.5898"></p><h3 id="2-Weighted-average方法"><a href="#2-Weighted-average方法" class="headerlink" title="2. Weighted-average方法"></a><strong>2. Weighted-average方法</strong></h3><p>该方法给不同类别不同权重（权重根据该类别的真实分布比例确定），每个类别乘权重后再进行相加。<strong>该方法考虑了类别不平衡情况，它的值更容易受到常见类（majority class）的影响</strong>。</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BW%7D_%7Bcat%7D+:+%5Ctext%7BW%7D_%7Bdog%7D+:+%5Ctext%7BW%7D_%7Bpig%7D+=+%5Ctext%7BN%7D_%7Bcat%7D+:+%5Ctext%7BN%7D_%7Bdog%7D+:%5Ctext%7BN%7D_%7Bpig%7D++=+%5Cfrac%7B7%7D%7B26%7D+:+%5Cfrac%7B16%7D%7B26%7D:+%5Cfrac%7B3%7D%7B26%7D"> (W代表权重，N代表样本在该类别下的真实数目)</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BWeighted-Precision%7D+=+%7BP%7D_%7Bcat%7D%5Ctimes+W_%7Bcat%7D++++%7BP%7D_%7Bdog%7D%5Ctimes+W_%7Bdog%7D++++%7BP%7D_%7Bpig%7D%5Ctimes+W_%7Bpig%7D+=+0.6314"></p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BWeighted-Recall%7D+=+%7BR%7D_%7Bcat%7D%5Ctimes+W_%7Bcat%7D++++%7BR%7D_%7Bdog%7D%5Ctimes+W_%7Bdog%7D++++%7BR%7D_%7Bpig%7D%5Ctimes+W_%7Bpig%7D+=+0.5577"></p><h3 id="3-Micro-average方法"><a href="#3-Micro-average方法" class="headerlink" title="3. Micro-average方法"></a><strong>3. Micro-average方法</strong></h3><p>该方法把每个类别的TP, FP, FN先相加之后，在根据二分类的公式进行计算。</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BMicro-Precision%7D+=+%5Cfrac%7B%7BTP%7D_%7Bcat%7D+++%7BTP%7D_%7Bdog%7D+++%7BTP%7D_%7Bpig%7D%7D%7B+%7BTP%7D_%7Bcat%7D+++%7BTP%7D_%7Bdog%7D+++%7BTP%7D_%7Bpig%7D+++%7BFP%7D_%7Bcat%7D+++%7BFP%7D_%7Bdog%7D+++%7BFP%7D_%7Bpig%7D%7D+=+0.5577+"></p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BMicro-Recall%7D+=+%5Cfrac%7B%7BTP%7D_%7Bcat%7D+++%7BTP%7D_%7Bdog%7D+++%7BTP%7D_%7Bpig%7D%7D%7B+%7BTP%7D_%7Bcat%7D+++%7BTP%7D_%7Bdog%7D+++%7BTP%7D_%7Bpig%7D+++%7BFN%7D_%7Bcat%7D+++%7BFN%7D_%7Bdog%7D+++%7BFN%7D_%7Bpig%7D%7D=0.5577"></p><p>其中，特别有意思的是，<strong>Micro-precision和Micro-recall竟然始终相同！</strong>这是为啥呢？</p><p>这是因为在某一类中的False Positive样本，一定是其他某类别的False Negative样本。听起来有点抽象？举个例子，比如说系统错把「狗」预测成「猫」，那么对于狗而言，其错误类型就是False Negative，对于猫而言，其错误类型就是False Positive。于此同时，Micro-precision和Micro-recall的数值都等于Accuracy，因为它们计算了对角线样本数和总样本数的比值，总结就是：</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BMicro-Precision%7D+=+%5Ctext%7BMicro-Recall%7D+=+%5Ctext%7BMicro-F1+score%7D+=+%5Ctext%7BAccuracy%7D+"></p><p>最后，我们运行一下代码，检验手动计算结果是否和Sklearn包结果一致：</p><pre><code class="python">import numpy as npimport seaborn as snsfrom sklearn.metrics import confusion_matriximport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score# create confusion matrixy_true = np.array([-1]*70 + [0]*160 + [1]*30)y_pred = np.array([-1]*40 + [0]*20 + [1]*20 +                   [-1]*30 + [0]*80 + [1]*30 +                   [-1]*5 + [0]*15 + [1]*20)cm = confusion_matrix(y_true, y_pred)conf_matrix = pd.DataFrame(cm, index=['Cat','Dog','Pig'], columns=['Cat','Dog','Pig'])# plot size settingfig, ax = plt.subplots(figsize = (4.5,3.5))sns.heatmap(conf_matrix, annot=True, annot_kws={"size": 19}, cmap="Blues")plt.ylabel('True label', fontsize=18)plt.xlabel('Predicted label', fontsize=18)plt.xticks(fontsize=18)plt.yticks(fontsize=18)plt.savefig('confusion.pdf', bbox_inches='tight')plt.show()</code></pre><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glk4z653u6j20ci09qdg9.jpg"></p><pre><code class="python">print('-----Macro-----')print('Macro precision',precision_score(y_true,y_pred,average='macro'))print('Macro recall',recall_score(y_true,y_pred,average='macro'))print('Macro f1-score',f1_score(y_true,y_pred,average='macro'))print('-----Weighted-----')print('Weighted precision',precision_score(y_true,y_pred,average='weighted'))print('Weighted recall',recall_score(y_true,y_pred,average='weighted'))print('Weighted f1-score',f1_score(y_true,y_pred,average='weighted'))print('-----Micro-----')print('Micro precision',precision_score(y_true,y_pred,average='micro'))print('Micro recall',recall_score(y_true,y_pred,average='micro'))print('Micro f1-score',f1_score(y_true,y_pred,average='micro'))</code></pre><pre><code class="text">-----Macro-----Macro precision 0.5193926846100759Macro recall 0.589781746031746Macro f1-score 0.5233019853709507-----Weighted-----Weighted precision 0.6314062748845358Weighted recall 0.5576923076923077Weighted f1-score 0.575114540631782-----Micro-----Micro precision 0.5576923076923077Micro recall 0.5576923076923077Micro f1-score 0.5576923076923077</code></pre><p>运算结果完全一致。</p><h2 id="多标签模型的常见指标详细解析"><a href="#多标签模型的常见指标详细解析" class="headerlink" title="多标签模型的常见指标详细解析"></a>多标签模型的常见指标详细解析</h2><p>前面讲解了二分类和多分类模型的评价指标，那么对于多标签分类模型，它的评价指标又是怎么计算的呢？</p><p>在针对multilabel分类计算F score的时候，通常有macro和micro两种average的方法。Python的scikit-learn库在计算f1 score也提供了micro和macro两种选择，具体在multilabel的情况下，怎么计算F1-score，在网上查阅了很多博客和资料都没有给出一个明确的用列子解释的步骤，这边我自己通过整合资料代码验证出了macro和micro两种F1-score的计算方法。</p><p>请看下面的简单例子：</p><p>让$N=3,L=3$，一共有3个data，每个data有3个预备分类。</p><pre><code class="python">y_gt = np.array([[1,0,1],[0,1,1],[0,1,0]])y_pred = np.array([[0,0,1],[1,1,1],[1,1,1]])print("Macro f1-score:",f1_score(y_gt,y_pred,average='macro'))print('Micro f1-score:',f1_score(y_gt,y_pred,average='micro'))</code></pre><pre><code class="text">Macro f1-score: 0.6Micro f1-score: 0.6666666666666666</code></pre><p>对于每一个class，我们都需要先算一个2x2的confusion matrix，里面分别标明了对于这一个class的TP，FP，FN和TN。<font color="red"><strong>其实做法和多分类是一样的。</strong></font></p><h3 id="1-Macro-average方法-1"><a href="#1-Macro-average方法-1" class="headerlink" title="1. Macro-average方法"></a>1. Macro-average方法</h3><p>对于macro，我们通过每一个class的confusion matrix算出它的precision和recall，并计算出对与那个class的F1 score，最后通过平均所有class的F1 score得到F1 macro</p><p><strong>Class 0：</strong></p><table><thead><tr><th align="center">gt\pred</th><th align="center">1</th><th align="center">0</th></tr></thead><tbody><tr><td align="center"><strong>1</strong></td><td align="center">0</td><td align="center">1</td></tr><tr><td align="center"><strong>0</strong></td><td align="center">2</td><td align="center">0</td></tr></tbody></table><p>$P_0 = \frac{0}{0+2}=0, R_0 = \frac{0}{0+1}=0$</p><p>$F1_0=\frac{2\times0\times0}{0+0}=0$</p><p><strong>Class 1：</strong></p><table><thead><tr><th align="center">gt\pred</th><th align="center">1</th><th align="center">0</th></tr></thead><tbody><tr><td align="center"><strong>1</strong></td><td align="center">2</td><td align="center">0</td></tr><tr><td align="center"><strong>0</strong></td><td align="center">0</td><td align="center">1</td></tr></tbody></table><p>$P_1 = \frac{2}{2+0}=1, R_1 = \frac{2}{2+0}=1$</p><p>$F1_1=\frac{2\times1\times1}{1+1}=1$</p><p><strong>Class 2:</strong></p><table><thead><tr><th align="center">gt\pred</th><th align="center">1</th><th align="center">0</th></tr></thead><tbody><tr><td align="center"><strong>1</strong></td><td align="center">2</td><td align="center">0</td></tr><tr><td align="center"><strong>0</strong></td><td align="center">1</td><td align="center">0</td></tr></tbody></table><p>$P_2 = \frac{2}{2+1}=\frac{2}{3}, R_2 = \frac{2}{2+0}=1$</p><p>$F1_2=\frac{2\times{\frac{2}{3}}\times1}{\frac{2}{3}+1}=\frac{4}{5}$</p><p>$F1_{macro}=\frac{1}{N}(F1_0+ F1_1+…+F1_N)=\frac{1}{3}(F1_0+F1_1+F1_2)=\frac{1}{3}(0+1+\frac{4}{5})=0.6$</p><h3 id="2-Micro-average方法"><a href="#2-Micro-average方法" class="headerlink" title="2. Micro-average方法"></a>2. Micro-average方法</h3><p>对于micro，我们把所有class的binary confusion matrix整合成一个大的2x2confusion matrix，然后并对于整合成的confusion matrix算出一个precision和recall值（$P_{comb}$ and $R_{comb}$），最后通过公式得到$F1_{micro}$ score。</p><p><strong>Combined all classes:</strong></p><table><thead><tr><th align="center">gt\pred</th><th align="center">1</th><th align="center">0</th></tr></thead><tbody><tr><td align="center"><strong>1</strong></td><td align="center">4</td><td align="center">1</td></tr><tr><td align="center"><strong>0</strong></td><td align="center">3</td><td align="center">1</td></tr></tbody></table><p>$P_{comb}=\frac{4}{4+3}=\frac{4}{7}, R_{comb}=\frac{4}{4+1}=\frac{4}{5}$</p><p>$F1_{micro}=\frac{2P_{comb}R_{comb}}{P_{comb}+R_{comb}}=\frac{2\times\frac{4}{7}\times\frac{4}{5}}{\frac{4}{7}+\frac{4}{5}}=0.66666666…$</p><p><font color="red">注意区别在多标签模型中，其Micro-average得到的precision，recall和F1-score并不是相等的。</font></p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol><li><a href="https://www.cnblogs.com/fledlingbird/p/10675922.html">多分类模型Accuracy, Precision, Recall和F1-score的超级无敌深入探讨</a></li><li><a href="https://www.cnblogs.com/fledlingbird/p/10675922.html">Multilabel(多标签分类)metrics：hamming loss，F score</a></li><li><a href="https://link.zhihu.com/?target=https://lawtomated.com/accuracy-precision-recall-and-f1-scores-for-lawyers/">4 Things You Need to Know about AI: Accuracy, Precision, Recall and F1 scores</a></li><li><a href="https://link.zhihu.com/?target=https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2">Multi-Class Metrics Made Simple, Part I: Precision and Recall</a></li><li><a href="https://link.zhihu.com/?target=https://tech.labs.oliverwyman.com/blog/2019/10/17/accuracy-precision-recal-elixir/">Accuracy, Precision and Recall: Multi-class Performance Metrics for Supervised Learning</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详解深度学习中的Normalization，BN/LN/WN</title>
      <link href="/2020/12/10/xiang-jie-shen-du-xue-xi-zhong-de-normalization-bn-ln-wn/"/>
      <url>/2020/12/10/xiang-jie-shen-du-xue-xi-zhong-de-normalization-bn-ln-wn/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文地址：<a href="https://zhuanlan.zhihu.com/p/33173246">https://zhuanlan.zhihu.com/p/33173246</a></p></blockquote><p>深度神经网络模型训练之难众所周知，其中一个重要的现象就是 Internal Covariate Shift. Batch Norm 大法自 2015 年由Google 提出之后，就成为深度学习必备之神器。自 BN 之后， Layer Norm / Weight Norm / Cosine Norm 等也横空出世。本文从 Normalization 的背景讲起，用一个公式概括 Normalization 的基本思想与通用框架，将各大主流方法一一对号入座进行深入的对比分析，并从参数和数据的伸缩不变性的角度探讨 Normalization 有效的深层原因。</p><p>本文继续沿袭上一个专题（<a href="https://zhuanlan.zhihu.com/p/32230623">Adam那么棒，为什么还对SGD念念不忘 —— 一个框架看懂优化算法</a>）的写作思路，先提炼一个通用的分析框架，然后再对号入座式地梳理各大主流算法，便于我们分析比较。</p><p><strong>目录：</strong></p><p><strong>1. 为什么需要 Normalization</strong></p><p>——深度学习中的 Internal Covariate Shift 问题及其影响</p><p><strong>2. Normalization 的通用框架与基本思想</strong></p><p>——从主流 Normalization 方法中提炼出的抽象框架</p><p><strong>3. 主流 Normalization 方法梳理</strong></p><p>——结合上述框架，将 BatchNorm / LayerNorm / WeightNorm / CosineNorm 对号入座，各种方法之间的异同水落石出。</p><p><strong>4. Normalization 为什么会有效？</strong></p><p>——从参数和数据的伸缩不变性探讨Normalization有效的深层原因。</p><p>以下是正文，enjoy.</p><h2 id="1-为什么需要-Normalization"><a href="#1-为什么需要-Normalization" class="headerlink" title="1. 为什么需要 Normalization"></a><strong>1. 为什么需要 Normalization</strong></h2><p><strong>1.1 独立同分布与白化</strong></p><p>机器学习界的炼丹师们最喜欢的数据有什么特点？窃以为，莫过于“<strong>独立同分布</strong>”了，即<em>independent and identically distributed</em>，简称为 <em>i.i.d.</em> 独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 和 神经网络 则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。</p><p>因此，在把数据喂给机器学习模型之前，“<strong>白化（whitening）</strong>”是一个重要的数据预处理步骤。白化一般包含两个目的：</p><p>（1）<em>去除特征之间的相关性</em> —&gt; 独立；</p><p>（2）<em>使得所有特征具有相同的均值和方差</em> —&gt; 同分布。</p><p>白化最典型的方法就是PCA，可以参考阅读 <a href="https://link.zhihu.com/?target=http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/">PCAWhitening</a>。</p><p><strong>1.2 深度学习中的 Internal Covariate Shift</strong></p><p>深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p><p>Google 将这一现象总结为 Internal Covariate Shift，简称 ICS. 什么是 ICS 呢？<a href="https://www.zhihu.com/people/b716bc76c2990cd06dae2f9c1f984e6d">@魏秀参</a> 在<a href="https://www.zhihu.com/question/38102762/answer/85238569">一个回答</a>中做出了一个很好的解释：</p><blockquote><p>大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有<img src="https://www.zhihu.com/equation?tex=x%5Cin+%5Cmathcal%7BX%7D" alt="[公式]">,<img src="https://www.zhihu.com/equation?tex=P_s(Y%7CX=x)=P_t(Y%7CX=x)%5C%5C" alt="[公式]">但是<img src="https://www.zhihu.com/equation?tex=P_s(X)%5Cne+P_t(X)%5C%5C" alt="[公式]">大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p></blockquote><p><strong>1.3 ICS 会导致什么问题？</strong></p><p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p><p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</p><p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</p><p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p><hr><h2 id="2-Normalization-的通用框架与基本思想"><a href="#2-Normalization-的通用框架与基本思想" class="headerlink" title="2. Normalization 的通用框架与基本思想"></a><strong>2. Normalization 的通用框架与基本思想</strong></h2><p>我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量 </p><p><img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D=(x_1,+x_2,+%5Ccdots,+x_d)%5C%5C"></p><p>通过某种运算后，输出一个标量值：</p><p><img src="https://www.zhihu.com/equation?tex=y=f(%5Cbold%7Bx%7D)%5C%5C"></p><p>由于 ICS 问题的存在， <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D" alt="[公式]"> 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。</p><p>因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D" alt="[公式]"> 送给神经元之前，先对其做<strong>平移和伸缩变换</strong>， 将 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D" alt="[公式]"> 的分布规范化成在固定区间范围的标准分布。</p><p>通用变换框架就如下所示：</p><p><img src="https://www.zhihu.com/equation?tex=h=f%5Cleft(%5Cbold%7Bg%7D%5Ccdot%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D+%5Cbold%7Bb%7D%5Cright)%5C%5C" alt="[公式]"></p><p>我们来看看这个公式中的各个参数。</p><p>（1） <img src="https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D" alt="[公式]"> 是<strong>平移参数</strong>（shift parameter）， <img src="https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D" alt="[公式]"> 是<strong>缩放参数</strong>（scale parameter）。通过这两个参数进行 shift 和 scale 变换： <img src="https://www.zhihu.com/equation?tex=%5Cbold%7B%5Chat%7Bx%7D%7D=%5Cfrac%7B%5Cbold%7Bx%7D-%5Cbold%7B%5Cmu%7D%7D%7B%5Cbold%7B%5Csigma%7D%7D%5C%5C" alt="[公式]">得到的数据符合均值为 0、方差为 1 的标准分布。</p><p>（2） <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D" alt="[公式]"> 是<strong>再平移参数</strong>（re-shift parameter）， <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D" alt="[公式]"> 是<strong>再缩放参数</strong>（re-scale parameter）。将 上一步得到的 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7B%5Chat%7Bx%7D%7D" alt="[公式]"> 进一步变换为： <img src="https://www.zhihu.com/equation?tex=%5Cbold%7By%7D=%5Cbold%7Bg%7D%5Ccdot+%5Cbold%7B%5Chat%7Bx%7D%7D+++%5Cbold%7Bb%7D%5C%5C" alt="[公式]"></p><p>最终得到的数据符合均值为 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D" alt="[公式]"> 、方差为 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2" alt="[公式]"> 的分布。</p><p>奇不奇怪？奇不奇怪？</p><p>说好的处理 ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？</p><p>答案是——<strong>为了保证模型的表达能力不因为规范化而下降</strong>。</p><p>我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，<strong>其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围</strong>。</p><p>沮不沮丧？沮不沮丧？</p><p>难道我们底层神经元人民就在做无用功吗？</p><p>所以，为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D" alt="[公式]"> 、方差为 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2" alt="[公式]"> ）。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。</p><p>除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。</p><p>那么问题又来了——</p><p><strong>经过这么的变回来再变过去，会不会跟没变一样？</strong></p><p>不会。因为，再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中， <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bx%7D" alt="[公式]"> 的均值取决于下层神经网络的复杂关联；但在新参数中， <img src="https://www.zhihu.com/equation?tex=%5Cbold%7By%7D=%5Cbold%7Bg%7D%5Ccdot+%5Cbold%7B%5Chat%7Bx%7D%7D+++%5Cbold%7Bb%7D" alt="[公式]"> 仅由 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D" alt="[公式]"> 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。</p><p>那么还有一个问题——</p><p><strong>这样的 Normalization 离标准的白化还有多远？</strong></p><p>标准白化操作的目的是“独立同分布”。独立就不说了，暂不考虑。变换为均值为 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D" alt="[公式]"> 、方差为 <img src="https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D%5E2" alt="[公式]"> 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已。（所以，这个坑还有得研究呢！）</p>]]></content>
      
      
      
        <tags>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch: weight decay 和 dropout</title>
      <link href="/2020/12/10/pytorch-weight-decay-he-dropout/"/>
      <url>/2020/12/10/pytorch-weight-decay-he-dropout/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图信号处理</title>
      <link href="/2020/12/10/tu-xin-hao-chu-li/"/>
      <url>/2020/12/10/tu-xin-hao-chu-li/</url>
      
        <content type="html"><![CDATA[<p>频谱（spectrum）：图信号所有的傅里叶系数合在一起称为该信号的频谱。</p><blockquote><p>对于一张给定的图（即图的拓扑结构不变），图信号的频谱等价于一种身份ID，给定了频谱，我们就可以推导出空域中的图信号。</p></blockquote><p>频谱完整的描述了图信号的频域特性，为接下来图信号的采样，滤波，重构等信号处理工作创造了条件。</p><p>频谱上的傅里叶系数反映了两个方面的信息：</p><ol><li>图信号本身的大小；</li><li>图的结构信息。</li></ol><h2 id="图滤波器"><a href="#图滤波器" class="headerlink" title="图滤波器"></a>图滤波器</h2><p>图滤波器：对给定图信号的频谱中的各个频率分量的强度进行增强或衰减的操作。</p><p>定义可能很难读懂，但是看了下面图滤波器的表达式就很清晰了：<br>$$<br>H=V \begin{bmatrix} h(\lambda_1) &amp; &amp; &amp; \  &amp; h(\lambda_2) \ &amp; &amp;… \&amp; &amp; &amp; h(\lambda_N) \ \end{bmatrix} V^{T} = V\Lambda_hV^{T}<br>$$<br>上式中的$\Lambda_h$称为图滤波器$H$的频率响应矩阵（Frequency Response Matrix），对应的函数$h(\lambda)$为$H$的频率响应函数（Frequency Response Function）。</p><p>根据泰勒展开——多项式逼近函数去近似任意函数，我们可以定义$h(\lambda)$为：<br>$$<br>h(\lambda) = \sum^{\infty}<em>{k=0}h_k\lambda^{k}<br>$$<br>对应的图滤波器$H$则为：<br>$$<br>H = \sum^{\infty}</em>{k=0} h_kS^{k}<br>$$</p><p>对于图滤波器，我们可以像图信号一样，从空域和频域两个视角来理解：</p><h3 id="空域角度"><a href="#空域角度" class="headerlink" title="空域角度"></a>空域角度</h3>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DGFraud工具分析</title>
      <link href="/2020/12/07/dgfraud-gong-ju-fen-xi/"/>
      <url>/2020/12/07/dgfraud-gong-ju-fen-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="DBLP"><a href="#DBLP" class="headerlink" title="DBLP"></a>DBLP</h3><p>We uses the pre-processed DBLP dataset from <a href="https://github.com/Jhy1993/HAN">Jhy1993/HAN</a> You can run the <strong>FdGars, Player2Vec, GeniePath and GEM</strong> based on the DBLP dataset. Unzip the archive before using the dataset:</p><pre><code>cd datasetunzip DBLP4057_GAT_with_idx_tra200_val_800.zip</code></pre><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glfhfm3ovcj20mu02cdgr.jpg" alt="数据集">We extract a subset of DBLP which contains 14328 papers (P), 4057 authors (A), 20 conferences (C), 8789 terms (T). The authors are divided into four areas: <strong>database, data mining, machine learning, information retrieval</strong>. Also, we label each author’s research area according to the conferences they submitted. Author features are the elements of a bag-of-words represented of keywords. </p><blockquote><p>里面包含论文，作者，会议和专业术语这几种实体</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1glfhqcdkmrj20f406g74n.jpg" alt="mat文件内部情况"></p><ul><li>label表示的是作者是哪个领域的，是我们多分类的目标。</li></ul><h3 id="Example-dataset"><a href="#Example-dataset" class="headerlink" title="Example dataset"></a>Example dataset</h3><p>We implement example graphs for <strong>SemiGNN, GAS and GEM</strong> in <code>data_loader.py</code>. Because those models require unique graph structures or node types, which cannot be found in opensource datasets.</p><h3 id="Yelp-dataset"><a href="#Yelp-dataset" class="headerlink" title="Yelp dataset"></a>Yelp dataset</h3><p>For <a href="https://arxiv.org/abs/2005.00625">GraphConsis</a>, we preprocessed <a href="http://odds.cs.stonybrook.edu/yelpchi-dataset/">Yelp Spam Review Dataset</a> with reviews as nodes and three relations as edges.</p><p>The dataset with <code>.mat</code> format is located at <code>/dataset/YelpChi.zip</code>. The <code>.mat</code> file includes:</p><ul><li><code>net_rur, net_rtr, net_rsr</code>: three sparse matrices representing three homo-graphs defined in <a href="https://arxiv.org/abs/2005.00625">GraphConsis</a> paper;</li><li><code>features</code>: a sparse matrix of 100-dimension Bag-of-words features;</li><li><code>label</code>: a numpy array with the ground truth of nodes. <code>1</code> represents spam and <code>0</code>represents benign.</li></ul><p>To get the complete metadata of the Yelp dataset, please send an email to <a href="mailto:ytongdou@gmail.com">ytongdou@gmail.com</a> for inquiry.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How to Read a Paper</title>
      <link href="/2020/12/06/how-to-read-a-paper/"/>
      <url>/2020/12/06/how-to-read-a-paper/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="2-“三步走”方法"><a href="#2-“三步走”方法" class="headerlink" title="2. “三步走”方法"></a>2. “三步走”方法</h2><p>读论文不是一蹴而就的，拿到一篇论文，直接一股脑读下去是不明智的！读论文应该分“三步走”~每一步都有自己的目标，都是建立在前一步的基础上的：</p><ol><li>第一步：了解论文的idea是什么？</li><li>第二步：理解论文的内容（但不是论文的细节）</li><li>第三步：深度理解论文</li></ol><h3 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h3><p>第一步有点“上帝视角”的意思，从大的框架上看待论文，了解到论文的idea，以决定需不需要再进行阅读。这个过程对于英语母语者为5~10分钟，建议分以下小步进行：</p><ol><li>仔细研读论文的title，abstract和introduction</li><li>阅读每个section和sub-section的标题（但是要忽视里面的内容）</li><li>瞟一眼里面的数学公式来了解论文用到了哪些理论知识</li><li>阅读conclusion</li><li>看论文引用，找出你读过的文献</li></ol><p>在做完上面这些步骤后，你需要能够回答下面的5个$C$：</p><table><thead><tr><th></th></tr></thead><tbody><tr><td>Category</td></tr><tr><td>Context</td></tr><tr><td>Correctness</td></tr><tr><td>Contributions</td></tr><tr><td>Clarity</td></tr></tbody></table><table><thead><tr><th>Category</th><th></th></tr></thead><tbody><tr><td>What type of paper is this?</td><td></td></tr></tbody></table><table><thead><tr><th>Context</th><th></th></tr></thead><tbody><tr><td>Which other papers is it related to?</td><td></td></tr><tr><td>Which theoretical bases were used to analyze the problem?</td><td></td></tr></tbody></table><table><thead><tr><th>Correctness</th><th></th></tr></thead><tbody><tr><td>Do the assumptions appear to be valid?</td><td></td></tr></tbody></table><table><thead><tr><th>Contributions</th><th></th></tr></thead><tbody><tr><td>What are the paper’s main contributions?</td><td></td></tr></tbody></table><table><thead><tr><th>Clarity</th><th></th></tr></thead><tbody><tr><td>Is the paper well written?</td><td></td></tr></tbody></table><h2 id="3-如何探索一个新的领域？"><a href="#3-如何探索一个新的领域？" class="headerlink" title="3. 如何探索一个新的领域？"></a>3. 如何探索一个新的领域？</h2><p>第一步：使用文献搜索引擎（如Google Scholar，CiteSeer等）<strong>搜索关键字</strong>，找到近3-5年来，引用数量高的论文。对于每篇论文，采取first-pass的阅读方式了解大意，同时留意其“Related Work”部分，</p><blockquote><p>第一步下来，通过阅读“Related Work”部分，你会得到大量的相关论文，如果足够幸运，你可能在这个过程中得到一篇很好的综述！如果你找到了一篇极好的综述，那么你就可以直接读综述，读完综述，也算是入了一个门了。</p></blockquote><p>第二步：如果运气并不是很好，没有找到综述。此时，你可以留意每篇论文参考文献，看看它们是否共同引用了某些优质论文或者某些作者。这些作者很可能是领域的大牛。对于优质论文，下载到自己的“小金库”中，而对于大牛，直接浏览他的学术首页，看看他最近在哪些会议，期刊上发文章，这可以让你找到这个领域的顶会和顶刊~（大牛只会发顶会和顶刊(lll￢ω￢)）</p><p>第三步：去顶会和顶刊的网站，看看它们最近几年的动向。同时快速扫一遍它的论文库，寻找里面的优秀论文（比如引用数多啊），将它们也下载到自己的“小金库”。</p><p>综上，你的“小金库”将会非常充实！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>【李宏毅-深度学习】26. RNN Part2</title>
      <link href="/2020/12/05/li-hong-yi-shen-du-xue-xi-26-rnn-part2/"/>
      <url>/2020/12/05/li-hong-yi-shen-du-xue-xi-26-rnn-part2/</url>
      
        <content type="html"><![CDATA[<h1 id="Recurrent-Neural-Network-Ⅱ"><a href="#Recurrent-Neural-Network-Ⅱ" class="headerlink" title="Recurrent Neural Network(Ⅱ)"></a>Recurrent Neural Network(Ⅱ)</h1><blockquote><p>上一篇文章介绍了RNN的基本架构，像这么复杂的结构，我们该如何训练呢？</p></blockquote><h4 id="Learning-Target"><a href="#Learning-Target" class="headerlink" title="Learning Target"></a>Learning Target</h4><h5 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h5><p>依旧是Slot Filling的例子，我们需要把model的输出$y^i$与映射到slot的reference vector求交叉熵，比如“Taipei”对应到的是“dest”这个slot，则reference vector在“dest”位置上值为1，其余维度值为0</p><p>RNN的output和reference vector的cross entropy之和就是损失函数，也是要minimize的对象</p><p>需要注意的是，word要依次输入model，比如“arrive”必须要在“Taipei”前输入，不能打乱语序</p><p><a href="https://camo.githubusercontent.com/0eb839883a13dfa33763447eed83aa4ce4ca14748ccf2cdb7821e176a8c61425/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e2e706e67"><img src="https://camo.githubusercontent.com/0eb839883a13dfa33763447eed83aa4ce4ca14748ccf2cdb7821e176a8c61425/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e2e706e67" alt="img"></a></p><h5 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h5><p>有了损失函数后，训练其实也是用梯度下降法，为了计算方便，这里采取了反向传播(Backpropagation)的进阶版，Backpropagation through time，简称BPTT算法</p><p>BPTT算法与BP算法非常类似，只是多了一些时间维度上的信息，这里不做详细介绍</p><p><a href="https://camo.githubusercontent.com/c7e24b9ea1ac909dc658beec61c62c664478fde9cccb5b4811533f15684637a7/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e322e706e67"><img src="https://camo.githubusercontent.com/c7e24b9ea1ac909dc658beec61c62c664478fde9cccb5b4811533f15684637a7/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e322e706e67" alt="img"></a></p><p>不幸的是，RNN的训练并没有那么容易</p><p>我们希望随着epoch的增加，参数的更新，loss应该要像下图的蓝色曲线一样慢慢下降，但在训练RNN的时候，你可能会遇到类似绿色曲线一样的学习曲线，loss剧烈抖动，并且会在某个时刻跳到无穷大，导致程序运行失败</p><p><a href="https://camo.githubusercontent.com/040d1315ec1e68f2327ed1e0c2bff83446e0be2f05f1a89526371c28c6bfac2e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e332e706e67"><img src="https://camo.githubusercontent.com/040d1315ec1e68f2327ed1e0c2bff83446e0be2f05f1a89526371c28c6bfac2e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e332e706e67" alt="img"></a></p><h5 id="Error-Surface"><a href="#Error-Surface" class="headerlink" title="Error Surface"></a>Error Surface</h5><p>分析可知，RNN的error surface，即loss由于参数产生的变化，是非常陡峭崎岖的</p><p>下图中，$z$轴代表loss，$x$轴和$y$轴代表两个参数$w_1$和$w_2$，可以看到loss在某些地方非常平坦，在某些地方又非常的陡峭</p><p>如果此时你的训练过程类似下图中从下往上的橙色的点，它先经过一块平坦的区域，又由于参数的细微变化跳上了悬崖，这就会导致loss上下抖动得非常剧烈</p><p>如果你的运气特别不好，一脚踩在悬崖上，由于之前一直处于平坦区域，gradient很小，你会把参数更新的步长(learning rate)调的比较大，而踩到悬崖上导致gradient突然变得很大，这会导致参数一下子被更新了一个大步伐，导致整个就飞出去了，这就是学习曲线突然跳到无穷大的原因</p><p><a href="https://camo.githubusercontent.com/92d3323f10ace032fde287d2ce3e33e8fe44a1889d983cc36435617127eefd87/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e342e706e67"><img src="https://camo.githubusercontent.com/92d3323f10ace032fde287d2ce3e33e8fe44a1889d983cc36435617127eefd87/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6c6561726e342e706e67" alt="img"></a></p><p>想要解决这个问题，就要采用Clipping方法，当gradient即将大于某个threshold的时候，就让它停止增长，比如当gradient大于15的时候就直接让它等于15</p><p>为什么RNN会有这种奇特的特性呢？下图给出了一个直观的解释：</p><p>假设RNN只含1个neuron，它是linear的，input和output的weight都是1，没有bias，从当前时刻的memory值接到下一时刻的input的weight是$w$，按照时间点顺序输入[1, 0, 0, 0, …, 0]</p><p>当第1个时间点输入1的时候，在第1000个时间点，RNN输出的$y^{1000}=w^{999}$，想要知道参数$w$的梯度，只需要改变$w$的值，观察对RNN的输出有多大的影响即可：</p><ul><li>当$w$从1-&gt;1.01，得到的$y^{1000}$就从1变到了20000，这表示$w$的梯度很大，需要调低学习率</li><li>当$w$从0.99-&gt;0.01，则$y^{1000}$几乎没有变化，这表示$w$的梯度很小，需要调高学习率</li><li>从中可以看出gradient时大时小，error surface很崎岖，尤其是在$w=1$的周围，gradient几乎是突变的，这让我们很难去调整learning rate</li></ul><p><a href="https://camo.githubusercontent.com/b46cfe1c1755746d12cdff20effe7615b96bda792d083d8df41400b71ad88c9e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d7768792e706e67"><img src="https://camo.githubusercontent.com/b46cfe1c1755746d12cdff20effe7615b96bda792d083d8df41400b71ad88c9e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d7768792e706e67" alt="img"></a></p><p>因此我们可以解释，RNN训练困难，是由于它把同样的操作在不断的时间转换中重复使用</p><p>从memory接到neuron输入的参数$w$，在不同的时间点被反复使用，$w$的变化有时候可能对RNN的输出没有影响，而一旦产生影响，==<strong>经过长时间的不断累积，该影响就会被放得无限大</strong>==，因此RNN经常会遇到这两个问题：</p><ul><li>梯度消失(gradient vanishing)，一直在梯度平缓的地方停滞不前</li><li>梯度爆炸(gradient explode)，梯度的更新步伐迈得太大导致直接飞出有效区间</li></ul><h4 id="Help-Techniques"><a href="#Help-Techniques" class="headerlink" title="Help Techniques"></a>Help Techniques</h4><p>有什么技巧可以帮我们解决这个问题呢？LSTM就是最广泛使用的技巧，它会把error surface上那些比较平坦的地方拿掉，从而解决梯度消失(gradient vanishing)的问题，但它无法处理梯度崎岖的部分，因而也就无法解决梯度爆炸的问题(gradient explode)</p><p>但由于做LSTM的时候，大部分地方的梯度变化都很剧烈，因此训练时可以放心地把learning rate设的小一些</p><p>Q：为什么要把RNN换成LSTM？A：LSTM可以解决梯度消失的问题</p><p>Q：为什么LSTM能够解决梯度消失的问题？</p><p>A：RNN和LSTM对memory的处理其实是不一样的：</p><ul><li>在RNN中，每个新的时间点，memory里的旧值都会被新值所覆盖</li><li>在LSTM中，每个新的时间点，memory里的值会乘上$f(g_f)$与新值相加</li></ul><p>对RNN来说，$w$对memory的影响每次都会被清除，而对LSTM来说，除非forget gate被打开，否则$w$对memory的影响就不会被清除，而是一直累加保留，因此它不会有梯度消失的问题</p><p><a href="https://camo.githubusercontent.com/fd15ffcc34dc36f6a7fb7d98d0a37df414db66b55ae9e0d3a0ff5333d8816680/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d746563682e706e67"><img src="https://camo.githubusercontent.com/fd15ffcc34dc36f6a7fb7d98d0a37df414db66b55ae9e0d3a0ff5333d8816680/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d746563682e706e67" alt="img"></a></p><p>另一个版本GRU (Gated Recurrent Unit)，只有两个gate，需要的参数量比LSTM少，鲁棒性比LSTM好，不容易过拟合，它的基本精神是旧的不去，新的不来，GRU会把input gate和forget gate连起来，当forget gate把memory里的值清空时，input gate才会打开，再放入新的值</p><p>此外，还有很多技术可以用来处理梯度消失的问题，比如Clockwise RNN、SCRN等</p><p><a href="https://camo.githubusercontent.com/8a7269fa063df42594bb7b406035b3f512f43deba2c6e60d85a2506ceb79e8b3/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d74656368322e706e67"><img src="https://camo.githubusercontent.com/8a7269fa063df42594bb7b406035b3f512f43deba2c6e60d85a2506ceb79e8b3/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d74656368322e706e67" alt="img"></a></p><h4 id="More-Applications"><a href="#More-Applications" class="headerlink" title="More Applications"></a>More Applications</h4><p>在Slot Filling中，我们输入一个word vector输出它的label，除此之外RNN还可以做更复杂的事情</p><ul><li>多对一</li><li>多对多</li></ul><h5 id="Sentiment-Analysis"><a href="#Sentiment-Analysis" class="headerlink" title="Sentiment Analysis"></a>Sentiment Analysis</h5><p>语义情绪分析，我们可以把某影片相关的文章爬下来，并分析其正面情绪or负面情绪</p><p>RNN的输入是字符序列，在不同时间点输入不同的字符，并在最后一个时间点输出该文章的语义情绪</p><p><a href="https://camo.githubusercontent.com/227617ac75e29333298f35a0dd207ce07d38a0f5f2130418ef920611e13ff4b6/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6170702e706e67"><img src="https://camo.githubusercontent.com/227617ac75e29333298f35a0dd207ce07d38a0f5f2130418ef920611e13ff4b6/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6170702e706e67" alt="img"></a></p><h5 id="Key-term-Extraction"><a href="#Key-term-Extraction" class="headerlink" title="Key term Extraction"></a>Key term Extraction</h5><p>关键词分析，RNN可以分析一篇文章并提取出其中的关键词，这里需要把含有关键词标签的文章作为RNN的训练数据</p><p><a href="https://camo.githubusercontent.com/71c60ba7fccc900f4f252cd894a549b0bb67331a2ddd6f1d5ea9ce6e6d74ac43/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070322e706e67"><img src="https://camo.githubusercontent.com/71c60ba7fccc900f4f252cd894a549b0bb67331a2ddd6f1d5ea9ce6e6d74ac43/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070322e706e67" alt="img"></a></p><h5 id="Output-is-shorter"><a href="#Output-is-shorter" class="headerlink" title="Output is shorter"></a>Output is shorter</h5><p>如果输入输出都是sequence，且输出的sequence比输入的sequence要短，RNN可以处理这个问题</p><p>以语音识别为例，输入是一段声音信号，每隔一小段时间就用1个vector来表示，因此输入为vector sequence，而输出则是character vector</p><p>如果依旧使用Slot Filling的方法，只能做到每个vector对应1个输出的character，识别结果就像是下图中的“好好好棒棒棒棒棒”，但这不是我们想要的，可以使用Trimming的技术把重复内容消去，剩下“好棒”</p><p><a href="https://camo.githubusercontent.com/7c9f77b80fd0900243ecd90d73df9ffda53175e26f7191b557de386cf4b6ebcc/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070332e706e67"><img src="https://camo.githubusercontent.com/7c9f77b80fd0900243ecd90d73df9ffda53175e26f7191b557de386cf4b6ebcc/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070332e706e67" alt="img"></a></p><p>但“好棒”和“好棒棒”实际上是不一样的，如何区分呢？</p><p>需要用到CTC算法，它的基本思想是，输出不只是字符，还要填充NULL，输出的时候去掉NULL就可以得到连词的效果</p><p><a href="https://camo.githubusercontent.com/fbb19d677cf89cf0e36dedc629d81b4e4f1c8fa181efc59b9d6c8b7bf82991ff/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070342e706e67"><img src="https://camo.githubusercontent.com/fbb19d677cf89cf0e36dedc629d81b4e4f1c8fa181efc59b9d6c8b7bf82991ff/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070342e706e67" alt="img"></a></p><p>下图是CTC的示例，RNN的输出就是英文字母+NULL，google的语音识别系统就是用CTC实现的</p><p><a href="https://camo.githubusercontent.com/1c789a5344b594a6b94bb8d4c4d2bf609ef6e2d78957d413b80414945594b7dd/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070352e706e67"><img src="https://camo.githubusercontent.com/1c789a5344b594a6b94bb8d4c4d2bf609ef6e2d78957d413b80414945594b7dd/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070352e706e67" alt="img"></a></p><h5 id="Sequence-to-Sequence-Learning"><a href="#Sequence-to-Sequence-Learning" class="headerlink" title="Sequence to Sequence Learning"></a>Sequence to Sequence Learning</h5><p>在Seq2Seq中，RNN的输入输出都是sequence，但是长度不同</p><p>在CTC中，input比较长，output比较短；而在Seq2Seq中，并不确定谁长谁短</p><p>比如现在要做机器翻译，将英文的word sequence翻译成中文的character sequence</p><p>假设在两个时间点分别输入“machine”和“learning”，则在最后1个时间点memory就存了整个句子的信息，接下来让RNN输出，就会得到“机”，把“机”当做input，并读取memory里的值，就会输出“器”，依次类推，这个RNN甚至会一直输出，不知道什么时候会停止</p><p><a href="https://camo.githubusercontent.com/dbb79f07c02da593cf6d7b3ef47d9f61266b094e6258311fd86e687ff0f252c8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070362e706e67"><img src="https://camo.githubusercontent.com/dbb79f07c02da593cf6d7b3ef47d9f61266b094e6258311fd86e687ff0f252c8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070362e706e67" alt="img"></a></p><p>怎样才能让机器停止输出呢？</p><p>可以多加一个叫做“断”的symbol “===”，当输出到这个symbol时，机器就停止输出</p><p><a href="https://camo.githubusercontent.com/6e4d4760384824826f4bdcf593d8be0c83a8b0869f1a16a59f3c3e09fbfc7bab/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070372e706e67"><img src="https://camo.githubusercontent.com/6e4d4760384824826f4bdcf593d8be0c83a8b0869f1a16a59f3c3e09fbfc7bab/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070372e706e67" alt="img"></a></p><p>具体的处理技巧这里不再详述</p><h5 id="Seq2Seq-for-Syntatic-Parsing"><a href="#Seq2Seq-for-Syntatic-Parsing" class="headerlink" title="Seq2Seq for Syntatic Parsing"></a>Seq2Seq for Syntatic Parsing</h5><p>Seq2Seq还可以用在句法解析上，让机器看一个句子，它可以自动生成树状的语法结构图</p><p><a href="https://camo.githubusercontent.com/766bb3facb832ba7e7dfdbb88958e0012ae14cf7b494f94e4cabdb221e84cedb/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070382e706e67"><img src="https://camo.githubusercontent.com/766bb3facb832ba7e7dfdbb88958e0012ae14cf7b494f94e4cabdb221e84cedb/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070382e706e67" alt="img"></a></p><h5 id="Seq2Seq-for-Auto-encoder-Text"><a href="#Seq2Seq-for-Auto-encoder-Text" class="headerlink" title="Seq2Seq for Auto-encoder Text"></a>Seq2Seq for Auto-encoder Text</h5><p>如果用bag-of-word来表示一篇文章，就很容易丢失词语之间的联系，丢失语序上的信息</p><p>比如“白血球消灭了感染病”和“感染病消灭了白血球”，两者bag-of-word是相同的，但语义却是完全相反的</p><p><a href="https://camo.githubusercontent.com/6c4527fad8fcf5123940a5fad4f6f965cca097cb08e88cc77d52469dc3bde21f/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070392e706e67"><img src="https://camo.githubusercontent.com/6c4527fad8fcf5123940a5fad4f6f965cca097cb08e88cc77d52469dc3bde21f/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d617070392e706e67" alt="img"></a></p><p>这里就可以使用Seq2Seq Autoencoder，在考虑了语序的情况下，把文章编码成vector，只需要把RNN当做编码器和解码器即可</p><p>我们输入word sequence，通过RNN变成embedded vector，再通过另一个RNN解压回去，如果能够得到一模一样的句子，则压缩后的vector就代表了这篇文章中最重要的信息</p><p><a href="https://camo.githubusercontent.com/d608b716e9f992a22e726537cc06d193b42c8cbd487aef6565a279b039bee378/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031302e706e67"><img src="https://camo.githubusercontent.com/d608b716e9f992a22e726537cc06d193b42c8cbd487aef6565a279b039bee378/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031302e706e67" alt="img"></a></p><p>这个结构甚至可以被层次化，我们可以对句子的几个部分分别做vector的转换，最后合并起来得到整个句子的vector</p><p><a href="https://camo.githubusercontent.com/4254b8d5e93a5508e7fa8627edf87b47f84438ea4d220d79dff01531c223396e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031312e706e67"><img src="https://camo.githubusercontent.com/4254b8d5e93a5508e7fa8627edf87b47f84438ea4d220d79dff01531c223396e/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031312e706e67" alt="img"></a></p><h5 id="Seq2Seq-for-Auto-encoder-Speech"><a href="#Seq2Seq-for-Auto-encoder-Speech" class="headerlink" title="Seq2Seq for Auto-encoder Speech"></a>Seq2Seq for Auto-encoder Speech</h5><p>Seq2Seq autoencoder还可以用在语音处理上，它可以把一段语音信号编码成vector</p><p>这种方法可以把声音信号都转化为低维的vecotr，并通过计算相似度来做语音搜索</p><p><a href="https://camo.githubusercontent.com/4e621987c15342a785cfb650e2e4ea707376711bb6688acdf8fc2971bb7c7ee7/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031322e706e67"><img src="https://camo.githubusercontent.com/4e621987c15342a785cfb650e2e4ea707376711bb6688acdf8fc2971bb7c7ee7/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031322e706e67" alt="img"></a></p><p>先把声音信号转化成声学特征向量(acoustic features)，再通过RNN编码，最后一个时间点存在memory里的值就代表了整个声音信号的信息</p><p>为了能够对该神经网络训练，还需要一个RNN作为解码器，得到还原后的$y_i$，使之与$x_i$的差距最小</p><p><a href="https://camo.githubusercontent.com/f9d13e1a3e3a6356036ca8521fb695e6467a38184c6657e7e1b22570dda78abb/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031332e706e67"><img src="https://camo.githubusercontent.com/f9d13e1a3e3a6356036ca8521fb695e6467a38184c6657e7e1b22570dda78abb/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031332e706e67" alt="img"></a></p><h5 id="Attention-based-Model"><a href="#Attention-based-Model" class="headerlink" title="Attention-based Model"></a>Attention-based Model</h5><p>除了RNN之外，Attention-based Model也用到了memory的思想</p><p>机器会有自己的记忆池，神经网络通过操控读写头去读或者写指定位置的信息，这个过程跟图灵机很像，因此也被称为neural turing machine</p><p><a href="https://camo.githubusercontent.com/01923f5bab179a855813ac893cd985f28a9412edb04b6995ecb92e2674041ef6/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031342e706e67"><img src="https://camo.githubusercontent.com/01923f5bab179a855813ac893cd985f28a9412edb04b6995ecb92e2674041ef6/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d61707031342e706e67" alt="img"></a></p><p>这种方法通常用在阅读理解上，让机器读一篇文章，再把每句话的语义都存到不同的vector中，接下来让用户向机器提问，神经网络就会去调用读写头的中央处理器，取出memory中与查询语句相关的信息，综合处理之后，可以给出正确的回答</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>【李宏毅-深度学习】26. RNN Part1</title>
      <link href="/2020/12/05/li-hong-yi-shen-du-xue-xi-26-rnn-part1/"/>
      <url>/2020/12/05/li-hong-yi-shen-du-xue-xi-26-rnn-part1/</url>
      
        <content type="html"><![CDATA[<h1 id="Recurrent-Neural-Network-Ⅰ"><a href="#Recurrent-Neural-Network-Ⅰ" class="headerlink" title="Recurrent Neural Network(Ⅰ)"></a>Recurrent Neural Network(Ⅰ)</h1><blockquote><p>RNN，或者说最常用的LSTM，一般用于记住之前的状态，以供后续神经网络的判断，它由input gate、forget gate、output gate和cell memory组成，每个LSTM本质上就是一个neuron，特殊之处在于有4个输入：$z$和三门控制信号$z_i$、$z_f$和$z_o$，每个时间点的输入都是由当前输入值+上一个时间点的输出值+上一个时间点cell值来组成</p></blockquote><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><h5 id="Slot-Filling"><a href="#Slot-Filling" class="headerlink" title="Slot Filling"></a>Slot Filling</h5><p>在智能客服、智能订票系统中，往往会需要slot filling技术，它会分析用户说出的语句，将时间、地址等有效的关键词填到对应的槽上，并过滤掉无效的词语</p><p>词汇要转化成vector，可以使用1-of-N编码，word hashing或者是word vector等方式，此外我们可以尝试使用Feedforward Neural Network来分析词汇，判断出它是属于时间或是目的地的概率</p><p>但这样做会有一个问题，该神经网络会先处理“arrive”和“leave”这两个词汇，然后再处理“Taipei”，这时对NN来说，输入是相同的，它没有办法区分出“Taipei”是出发地还是目的地</p><p>这个时候我们就希望神经网络是有记忆的，如果NN在看到“Taipei”的时候，还能记住之前已经看过的“arrive”或是“leave”，就可以根据上下文得到正确的答案</p><p><a href="https://camo.githubusercontent.com/2b9a21b472f782cbd6c99301752fdcc69f87fe1cdd32c8391bdeb3df93c94f5a/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6578616d706c652e706e67"><img src="https://camo.githubusercontent.com/2b9a21b472f782cbd6c99301752fdcc69f87fe1cdd32c8391bdeb3df93c94f5a/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6578616d706c652e706e67" alt="img"></a></p><p>这种有记忆力的神经网络，就叫做Recurrent Neural Network(RNN)</p><p>在RNN中，hidden layer每次产生的output $a_1$、$a_2$，都会被存到memory里，下一次有input的时候，这些neuron就不仅会考虑新输入的$x_1$、$x_2$，还会考虑存放在memory中的$a_1$、$a_2$</p><p>注：在input之前，要先给内存里的$a_i$赋初始值，比如0</p><p><a href="https://camo.githubusercontent.com/4824d7458180c379b6012379921cf8824edce6109d846f92e8063b0fdb14b9f6/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2e706e67"><img src="https://camo.githubusercontent.com/4824d7458180c379b6012379921cf8824edce6109d846f92e8063b0fdb14b9f6/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2e706e67" alt="img"></a></p><p>注意到，每次NN的输出都要考虑memory中存储的临时值，而不同的输入产生的临时值也尽不相同，因此改变输入序列的顺序会导致最终输出结果的改变(Changing the sequence order will change the output)</p><h5 id="Slot-Filling-with-RNN"><a href="#Slot-Filling-with-RNN" class="headerlink" title="Slot Filling with RNN"></a>Slot Filling with RNN</h5><p>用RNN处理Slot Filling的流程举例如下：</p><ul><li>“arrive”的vector作为$x^1$输入RNN，通过hidden layer生成$a^1$，再根据$a^1$生成$y^1$，表示“arrive”属于每个slot的概率，其中$a^1$会被存储到memory中</li><li>“Taipei”的vector作为$x^2$输入RNN，此时hidden layer同时考虑$x^2$和存放在memory中的$a^1$，生成$a^2$，再根据$a^2$生成$y^2$，表示“Taipei”属于某个slot的概率，此时再把$a^2$存到memory中</li><li>依次类推</li></ul><p><a href="https://camo.githubusercontent.com/ce346860ed9f6ae5c9fe309f65b11d59224cbca598f6bcbea28dfe79922c4fde/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6578616d706c65322e706e67"><img src="https://camo.githubusercontent.com/ce346860ed9f6ae5c9fe309f65b11d59224cbca598f6bcbea28dfe79922c4fde/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6578616d706c65322e706e67" alt="img"></a></p><p>注意：上图为同一个RNN在三个不同时间点被分别使用了三次，并非是三个不同的NN</p><p>这个时候，即使输入同样是“Taipei”，我们依旧可以根据前文的“leave”或“arrive”来得到不一样的输出</p><p><a href="https://camo.githubusercontent.com/e4a2ae83cb0f77a47f8d6eb38ef445bbc06ad06819883c3f37813e8ca12aa539/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6578616d706c65332e706e67"><img src="https://camo.githubusercontent.com/e4a2ae83cb0f77a47f8d6eb38ef445bbc06ad06819883c3f37813e8ca12aa539/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d6578616d706c65332e706e67" alt="img"></a></p><h5 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h5><p>RNN有不同的变形：</p><ul><li>Elman Network：将hidden layer的输出保存在memory里</li><li>Jordan Network：将整个neural network的输出保存在memory里</li></ul><p>由于hidden layer没有明确的训练目标，而整个NN具有明确的目标，因此Jordan Network的表现会更好一些</p><p><a href="https://camo.githubusercontent.com/9dc9b147c70879900b322ff54642f14cd78cf5b5a64b9dbd084adbf5110f52d8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d747970652e706e67"><img src="https://camo.githubusercontent.com/9dc9b147c70879900b322ff54642f14cd78cf5b5a64b9dbd084adbf5110f52d8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d747970652e706e67" alt="img"></a></p><h5 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h5><p>RNN 还可以是双向的，你可以同时训练一对正向和反向的RNN，把它们对应的hidden layer $x^t$拿出来，都接给一个output layer，得到最后的$y^t$</p><p>使用Bi-RNN的好处是，NN在产生输出的时候，它能够看到的范围是比较广的，RNN在产生$y^{t+1}$的时候，它不只看了从句首$x^1$开始到$x^{t+1}$的输入，还看了从句尾$x^n$一直到$x^{t+1}$的输入，这就相当于RNN在看了整个句子之后，才决定每个词汇具体要被分配到哪一个槽中，这会比只看句子的前一半要更好</p><p><a href="https://camo.githubusercontent.com/1d0deff740a918bfb7f2c48fa913ae5080369096e146f330aec0890066de23a0/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d62692e706e67"><img src="https://camo.githubusercontent.com/1d0deff740a918bfb7f2c48fa913ae5080369096e146f330aec0890066de23a0/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f726e6e2d62692e706e67" alt="img"></a></p><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>前文提到的RNN只是最简单的版本，并没有对memory的管理多加约束，可以随时进行读取，而现在常用的memory管理方式叫做长短期记忆(Long Short-term Memory)，简称LSTM</p><p>冷知识：可以被理解为比较长的短期记忆，因此是short-term，而非是long-short term</p><h5 id="Three-gate"><a href="#Three-gate" class="headerlink" title="Three-gate"></a>Three-gate</h5><p>LSTM有三个gate：</p><ul><li>当某个neuron的输出想要被写进memory cell，它就必须要先经过一道叫做<strong>input gate</strong>的闸门，如果input gate关闭，则任何内容都无法被写入，而关闭与否、什么时候关闭，都是由神经网络自己学习到的</li><li>output gate决定了外界是否可以从memory cell中读取值，当<strong>output gate</strong>关闭的时候，memory里面的内容同样无法被读取</li><li>forget gate则决定了什么时候需要把memory cell里存放的内容忘记清空，什么时候依旧保存</li></ul><p><a href="https://camo.githubusercontent.com/29385e85f0c291c0f4b04519fb97509a4a9189fe01438f6469f1c062c3466a7a/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d2e706e67"><img src="https://camo.githubusercontent.com/29385e85f0c291c0f4b04519fb97509a4a9189fe01438f6469f1c062c3466a7a/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d2e706e67" alt="img"></a></p><p>整个LSTM可以看做是4个input，1个output：</p><ul><li>4个input=想要被存到memory cell里的值+操控input gate的信号+操控output gate的信号+操控forget gate的信号</li><li>1个output=想要从memory cell中被读取的值</li></ul><h5 id="Memory-Cell"><a href="#Memory-Cell" class="headerlink" title="Memory Cell"></a>Memory Cell</h5><p>如果从表达式的角度看LSTM，它比较像下图中的样子</p><ul><li>$z$是想要被存到cell里的输入值</li><li>$z_i$是操控input gate的信号</li><li>$z_o$是操控output gate的信号</li><li>$z_f$是操控forget gate的信号</li><li>$a$是综合上述4个input得到的output值</li></ul><p><a href="https://camo.githubusercontent.com/24bb5f15c2892bbd2734c8eaaf1319a97ed6f8f6a24984bc2b4e150a8c52f6a8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d322e706e67"><img src="https://camo.githubusercontent.com/24bb5f15c2892bbd2734c8eaaf1319a97ed6f8f6a24984bc2b4e150a8c52f6a8/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d322e706e67" alt="img"></a></p><p>把$z$、$z_i$、$z_o$、$z_f$通过activation function，分别得到$g(z)$、$f(z_i)$、$f(z_o)$、$f(z_f)$</p><p>其中对$z_i$、$z_o$和$z_f$来说，它们通过的激活函数$f()$一般会选sigmoid function，因为它的输出在0~1之间，代表gate被打开的程度</p><p>令$g(z)$与$f(z_i)$相乘得到$g(z)\cdot f(z_i)$，然后把原先存放在cell中的$c$与$f(z_f)$相乘得到$cf(z_f)$，两者相加得到存在memory中的新值$c’=g(z)\cdot f(z_i)+cf(z_f)$</p><ul><li>若$f(z_i)=0$，则相当于没有输入，若$f(z_i)=1$，则相当于直接输入$g(z)$</li><li>若$f(z_f)=1$，则保存原来的值$c$并加到新的值上，若$f(z_f)=0$，则旧的值将被遗忘清除</li></ul><p>从中也可以看出，forget gate的逻辑与我们的直觉是相反的，控制信号打开表示记得，关闭表示遗忘</p><p>此后，$c’$通过激活函数得到$h(c’)$，与output gate的$f(z_o)$相乘，得到输出$a=h(c’)f(z_o)$</p><h5 id="LSTM-Example"><a href="#LSTM-Example" class="headerlink" title="LSTM Example"></a>LSTM Example</h5><p>下图演示了一个LSTM的基本过程，$x_1$、$x_2$、$x_3$是输入序列，$y$是输出序列，基本原则是：</p><ul><li>当$x_2=1$时，将$x_1$的值写入memory</li><li>当$x_2=-1$时，将memory里的值清零</li><li>当$x_3=1$时，将memory里的值输出</li><li>当neuron的输入为正时，对应gate打开，反之则关闭</li></ul><p><a href="https://camo.githubusercontent.com/6ce4940591d6e7e31d088d04e66395099af7b0c2820754bf33505701fef985d1/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d332e706e67"><img src="https://camo.githubusercontent.com/6ce4940591d6e7e31d088d04e66395099af7b0c2820754bf33505701fef985d1/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d332e706e67" alt="img"></a></p><h5 id="LSTM-Structure"><a href="#LSTM-Structure" class="headerlink" title="LSTM Structure"></a>LSTM Structure</h5><p>你可能会觉得上面的结构与平常所见的神经网络不太一样，实际上我们只需要把LSTM整体看做是下面的一个neuron即可</p><p><a href="https://camo.githubusercontent.com/f8e356b17d4433f4728c39665dd8d622c35ead0ff75291b2c063bd08bba3573a/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d342e706e67"><img src="https://camo.githubusercontent.com/f8e356b17d4433f4728c39665dd8d622c35ead0ff75291b2c063bd08bba3573a/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d342e706e67" alt="img"></a></p><p>假设目前我们的hidden layer只有两个neuron，则结构如下图所示：</p><ul><li>输入$x_1$、$x_2$会分别乘上四组不同的weight，作为neuron的输入以及三个状态门的控制信号</li><li>在原来的neuron里，1个input对应1个output，而在LSTM里，4个input才产生1个output，并且所有的input都是不相同的</li><li>从中也可以看出LSTM所需要的参数量是一般NN的4倍</li></ul><p><a href="https://camo.githubusercontent.com/433d36a5210c5d676a7095aa7a9f75c1c634c74e3acb14f57505749b39df1344/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d352e706e67"><img src="https://camo.githubusercontent.com/433d36a5210c5d676a7095aa7a9f75c1c634c74e3acb14f57505749b39df1344/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d352e706e67" alt="img"></a></p><h5 id="LSTM-for-RNN"><a href="#LSTM-for-RNN" class="headerlink" title="LSTM for RNN"></a>LSTM for RNN</h5><p>从上图中你可能看不出LSTM与RNN有什么关系，接下来我们用另外的图来表示它</p><p>假设我们现在有一整排的LSTM作为neuron，每个LSTM的cell里都存了一个scalar值，把所有的scalar连接起来就组成了一个vector $c^{t-1}$</p><p>在时间点$t$，输入了一个vector $x^t$，它会乘上一个matrix，通过转换得到$z$，而$z$的每个dimension就代表了操控每个LSTM的输入值，同理经过不同的转换得到$z^i$、$z^f$和$z^o$，得到操控每个LSTM的门信号</p><p><a href="https://camo.githubusercontent.com/ceb6257e3be6ae1920fb3fd4c6572db853ef38773f837b34d088e06c64b8f2ae/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d362e706e67"><img src="https://camo.githubusercontent.com/ceb6257e3be6ae1920fb3fd4c6572db853ef38773f837b34d088e06c64b8f2ae/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d362e706e67" alt="img"></a></p><p>下图是单个LSTM的运算情景，其中LSTM的4个input分别是$z$、$z^i$、$z^f$和$z^o$的其中1维，每个LSTM的cell所得到的input都是各不相同的，但它们却是可以一起共同运算的，整个运算流程如下图左侧所示：</p><p>$f(z^f)$与上一个时间点的cell值$c^{t-1}$相乘，并加到经过input gate的输入$g(z)\cdot f(z^i)$上，得到这个时刻cell中的值$c^t$，最终再乘上output gate的信号$f(z^o)$，得到输出$y^t$</p><p><a href="https://camo.githubusercontent.com/cfc4fb0ad5b3b8c5c4d01e7a35013fefcdc725582b2aa9f42f137e2728384618/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d372e706e67"><img src="https://camo.githubusercontent.com/cfc4fb0ad5b3b8c5c4d01e7a35013fefcdc725582b2aa9f42f137e2728384618/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d372e706e67" alt="img"></a></p><p>上述的过程反复进行下去，就得到下图中各个时间点上，LSTM值的变化情况，其中与上面的描述略有不同的是，这里还需要把hidden layer的最终输出$y^t$以及当前cell的值$c^t$都连接到下一个时间点的输入上</p><p>因此在下一个时间点操控这些gate值，不只是看输入的$x^{t+1}$，还要看前一个时间点的输出$h^t$和cell值$c^t$，你需要把$x^{t+1}$、$h^t$和$c^t$这3个vector并在一起，乘上4个不同的转换矩阵，去得到LSTM的4个输入值$z$、$z^i$、$z^f$、$z^o$，再去对LSTM进行操控</p><p>注意：下图是<strong>同一个</strong>LSTM在两个相邻时间点上的情况</p><p><a href="https://camo.githubusercontent.com/5b72766f863503b30ed1bc0bb38d62e4277a41592e8595865eb1e49271dc02c9/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d382e706e67"><img src="https://camo.githubusercontent.com/5b72766f863503b30ed1bc0bb38d62e4277a41592e8595865eb1e49271dc02c9/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d382e706e67" alt="img"></a></p><p>上图是单个LSTM作为neuron的情况，事实上LSTM基本上都会叠多层，如下图所示，左边两个LSTM代表了两层叠加，右边两个则是它们在下一个时间点的状态</p><p><a href="https://camo.githubusercontent.com/0a71cec2b222e9aa8cb0d9ee2595d14d02379a87f102dc07d97a767e9481dcce/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d392e706e67"><img src="https://camo.githubusercontent.com/0a71cec2b222e9aa8cb0d9ee2595d14d02379a87f102dc07d97a767e9481dcce/68747470733a2f2f67697465652e636f6d2f53616b7572612d67682f4d4c2d6e6f7465732f7261772f6d61737465722f696d672f6c73746d392e706e67" alt="img"></a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度长文: 通过最近两年的8篇论文全面分析和总结基于图神经网络GNN的欺诈检测</title>
      <link href="/2020/12/05/shen-du-chang-wen-tong-guo-zui-jin-liang-nian-de-8-pian-lun-wen-quan-mian-fen-xi-he-zong-jie-ji-yu-tu-shen-jing-wang-luo-gnn-de-qi-zha-jian-ce/"/>
      <url>/2020/12/05/shen-du-chang-wen-tong-guo-zui-jin-liang-nian-de-8-pian-lun-wen-quan-mian-fen-xi-he-zong-jie-ji-yu-tu-shen-jing-wang-luo-gnn-de-qi-zha-jian-ce/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章转载自<a href="https://mp.weixin.qq.com/s/JFU0A1kBIqF-5qPeXieUZw">深度长文: 通过最两年的8篇论文全面分析和总结基于图神经网络GNN 欺诈检测</a></p></blockquote><p>[TOC]</p><p>欺诈检测（风控）是一个近两年业界和学界颇受关注的话题，其应用方向包括<strong>金融欺诈检测</strong>，<strong>风险控制</strong>，<strong>网络水军识别</strong>，<strong>黑产灰产识别</strong>等。</p><p>本公众号之前曾经介绍了一篇基于图模型的欺诈检测<a href="http://mp.weixin.qq.com/s?__biz=MzU1Mjc5NTg5OQ==&amp;mid=2247483854&amp;idx=1&amp;sn=ca1dbcf4fac78752db8824da1247a2f4&amp;chksm=fbfdea5acc8a634ca58309a95915ebaad275cdc6467aa11ac9c28d687893608121e1407b7322&amp;scene=21#wechat_redirect">文章</a>。随着近两年图神经网络（GNN）广受欢迎，基于图神经网络的欺诈检测研究也随之出现。</p><p>本文针对近两年内发表的<strong>近 8 篇论文</strong>进行分析，希望给读者一个 GNN 欺诈检测研究的宏观图景。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/EvHCAT1VCBMEZSUr17LLdbHXUgRWncbsoAhY8N1aW8KGaF0J48ibtkOMEjHuvSyiapmhibYian6TgAAUDYcWdmGeVA/640"></p><p>关于基于图数据的欺诈检测研究最新的进展，可以参考下面的 GitHub 链接：<a href="https://github.com/safe-graph/graph-fraud-detection-papers">https://github.com/safe-graph/graph-fraud-detection-papers</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/EvHCAT1VCBMHufvGspQTkiboMYDGQrpjZS1QpaBd0n2SQPyAom8BsDr5oQenibdvWtoZf5frnV2y05Bq4STibvticw/640" alt="img"></p><p>本文介绍的大部分模型的代码都已复现，具体可以参考下面的 GitHub 链接：<a href="https://github.com/safe-graph/DGFraud">https://github.com/safe-graph/DGFraud</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/EvHCAT1VCBMHufvGspQTkiboMYDGQrpjZ6qWiaCZbyicGL0n2icUAIffFZpqSxgmcAQOV696aRDQHa2IoQABjlHDeg/640" alt="img"></p><p>本文将从两个角度对相关工作进行介绍：</p><ul><li>我们先从不同论文的应用方向入手，对相关工作进行分类介绍，主要介绍<font color="red"><strong>每个应用方向的特点</strong>。</font></li><li>因为在 GNN 的应用中，构图和邻居信息聚合是最重要的两个步骤。本文第二个角度是从不同论文的方法入手，总结<font color="red"><strong>其构图，邻居信息聚合等方法的异同</strong>。</font></li></ul><p>随后，本文还介绍了应用 GNN 到欺诈检测问题中的<strong>关键步骤</strong>。文末对现存的挑战和未来方向进行了总结。</p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>目前已经发表的工作，根据其应用场景，我们可以将8篇论文分为三大类：</p><p><strong>金融欺诈识别</strong></p><blockquote><p>① A Semi-supervised Graph Attentive Network for Financial Fraud Detection. ICDM 2019.</p></blockquote><blockquote><p>② Geniepath: Graph Neural Networks with Adaptive Receptive Paths. AAAI 2019.</p></blockquote><blockquote><p>③ Heterogeneous Graph Neural Networks for Malicious Account Detection. CIKM 2018.</p></blockquote><p><strong>评论水军识别</strong></p><blockquote><p>④ Fdgars: Fraudster Detection via Graph Convolutional Networks in Online App Review System. WWW 2019.</p></blockquote><blockquote><p>⑤ ASA: Adversary Situation Awareness via Heterogeneous Graph Convolutional Networks. WWW 2020.</p></blockquote><blockquote><p>⑥ Spam Review Detection with Graph Convolutional Networks. CIKM 2019.</p></blockquote><blockquote><p>⑦ Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection. SIGIR 2020.</p></blockquote><p><strong>网络犯罪识别</strong></p><blockquote><p>⑧ Key Player Identification in Underground Forums over Attributed Heterogeneous Information Network Embedding Framework. CIKM 2019.</p></blockquote><p>下面我们分别介绍这三个方向的论文及其应用特点。</p><p>金融欺诈研究的论文全部来自于<strong>蚂蚁金服</strong>，其中：</p><ul><li>论文①是对<strong>蚂蚁花呗用户还款能力和信用分进行预测</strong>。相对于欺诈检测，这篇文章更倾向于风险的评估。（因为是一个预测的问题，所以偏风险评估）</li><li>论文②和论文③都是针对<strong>支付宝欺诈账户的检测</strong>，其利用了支付宝账号在不同设备的登录和活动信息。</li></ul><p>评论水军识别的研究已经有十年时间，之前有关于 Amazon，大众点评，Yelp 和 Google Play 评论水军的研究。</p><p>近两年 GNN 水军识别文章主要来自阿里和腾讯，其中论文④和论文⑤是研究腾讯应用宝的水军，论文⑥是研究阿里闲鱼 APP 评论区的水军。论文⑦基于 Yelp 水军数据对现有GNN 模型进行了改进。</p><p>网络犯罪识别是计算机安全的重要研究方向，论文⑧用图模型和 GNN 来检测暗网论坛中的关键用户。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>从方法角度，我们可以将上述论文提出的模型按照<font color="red"><strong>构图方式，邻居聚合方式，特有模块</strong></font>这三个类别进行总结。下面我们将对每篇论文从这三个角度进行介绍。</p><h3 id="①-A-Semi-supervised-Graph-Attentive-Network-for-Financial-Fraud-Detection-ICDM-2019"><a href="#①-A-Semi-supervised-Graph-Attentive-Network-for-Financial-Fraud-Detection-ICDM-2019" class="headerlink" title="① A Semi-supervised Graph Attentive Network for Financial Fraud Detection. ICDM 2019."></a>① A Semi-supervised Graph Attentive Network for Financial Fraud Detection. ICDM 2019.</h3><p>该文章构建了多视角图（Multi-view Graph），即从现有实体和特征的类型出发，将属于同一类型的实体连接起来，构成一个图。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/EvHCAT1VCBMHufvGspQTkiboMYDGQrpjZWrnic9B0iabxRwMQBfCXhGxbKQQG3GIat5sUicrpKpqp4uicUEVLWXPUVw/640" alt="论文①构建的多视角图"></p><p>举例来说，属于同一账户的设备可以构成一个<strong>设备图</strong>，节点是设备，有无边代表两个设备是否属于相同账户。同理，拥有共同好友关系的账户可以构成<strong>好友图</strong>，节点是账户，有无边代表是否存在共同好友。 </p><p>该文章提出了分层级注意力网络，<strong>先在单个视角下对邻居进行信息聚合，然后在多视角层面对同一节点来自不同视角的信息进行聚合。</strong>两次聚合都采用了<strong>注意力机制</strong>。</p><p>该文章还有个特点，其引入了<strong>无监督图嵌入（Network Embedding）模型</strong>（例如 <code>DeepWalk</code> 和 <code>LINE</code>） 的思想，除了<strong>对有标签和特征的部分节点使用 GNN 编码</strong>外，它<strong>对整体图结构采用了 DeepWalk 的无监督编码方式</strong>，从而在结构上进一步捕捉可疑用户和正常用户的区别。</p><h3 id="②-Geniepath-Graph-Neural-Networks-with-Adaptive-Receptive-Paths-AAAI-2019"><a href="#②-Geniepath-Graph-Neural-Networks-with-Adaptive-Receptive-Paths-AAAI-2019" class="headerlink" title="② Geniepath: Graph Neural Networks with Adaptive Receptive Paths. AAAI 2019."></a>② Geniepath: Graph Neural Networks with Adaptive Receptive Paths. AAAI 2019.</h3><p>该论文提出了适应性的图卷积网络，其构图形式为<strong>基于支付宝账户的异质图</strong>。</p><p>在邻居维度，其采用了<strong>注意力机制</strong>对邻居重要程度进行编码。</p><p>在网络深度维度，其采用了 <strong>LSTM 网络对最优的的网络深度进行学习</strong>。</p><h3 id="③-Heterogeneous-Graph-Neural-Networks-for-Malicious-Account-Detection-CIKM-2018"><a href="#③-Heterogeneous-Graph-Neural-Networks-for-Malicious-Account-Detection-CIKM-2018" class="headerlink" title="③ Heterogeneous Graph Neural Networks for Malicious Account Detection. CIKM 2018."></a>③ Heterogeneous Graph Neural Networks for Malicious Account Detection. CIKM 2018.</h3><p>这篇文章针对支付宝欺诈账户进行检测。其节点为账户和设备，其中账户的特征为账户活动情况和设备信息，如果账户和设备有过交互行为，账户节点和设备节点之间会有一条边。</p><p>例如，某账户在一个 IP 地址上登录过，那么这个账户节点和 IP 地址节点之间会有一条边。</p><p>在聚合信息时，对于一个账户节点，该文章<strong>先聚合其同一设备子图下的邻居</strong>，然后<strong>再聚合不同子图的信息</strong>。不同于论文①，该文章采用<font color="red"><strong>权重参数来编码不同设备子图的重要性，而不是使用注意力机制。</strong></font></p><p><font color="red"><strong>该文章是最早发表的基于 GNN 的欺诈检测论文，目前也有比较高的影响力。其文中有许多工业部署的实际经验的技巧值得借鉴。</strong></font></p><h3 id="④-Fdgars-Fraudster-Detection-via-Graph-Convolutional-Networks-in-Online-App-Review-System-WWW-2019"><a href="#④-Fdgars-Fraudster-Detection-via-Graph-Convolutional-Networks-in-Online-App-Review-System-WWW-2019" class="headerlink" title="④ Fdgars: Fraudster Detection via Graph Convolutional Networks in Online App Review System. WWW 2019."></a>④ Fdgars: Fraudster Detection via Graph Convolutional Networks in Online App Review System. WWW 2019.</h3><p>该文章是最早用 GCN 研究评论水军的文章，它针对腾讯应用宝水军<strong>行为和语义特点</strong>，<strong>设计了若干特征。</strong></p><p>在构图时使用了经典的 co-review 关系，即如果两个账户评论过同一个 APP，它们之间会有一条边。该文章使用经典的 GCN 模型 （Kipf &amp; Welling）对节点特征进一步编码。</p><h3 id="⑤-ASA-Adversary-Situation-Awareness-via-Heterogeneous-Graph-Convolutional-Networks-WWW-2020"><a href="#⑤-ASA-Adversary-Situation-Awareness-via-Heterogeneous-Graph-Convolutional-Networks-WWW-2020" class="headerlink" title="⑤ ASA: Adversary Situation Awareness via Heterogeneous Graph Convolutional Networks. WWW 2020."></a>⑤ ASA: Adversary Situation Awareness via Heterogeneous Graph Convolutional Networks. WWW 2020.</h3><p>该论文同上一篇文章来自同一团队，还是腾讯应用宝的评论水军检测问题。这篇较新的文章注意到了水军的<strong>对抗行为</strong>，即通过一些<strong>文本和行为的伪装</strong>，来降低自身的可疑程度。</p><p>例如，水军通过在评论中加入特殊符号来躲避基于文本特征的检测器（微博卖片是另一个例子）。水军也会通过虚拟机和 VPN 迅速切换设备信息和网络信息，从而躲避相关检测。</p><p>该论文通过各种关键实体信息（IP 地址，设备IMEI 号等）设计异质信息网络，然后通过分层聚合的方式用 GNN 来编码信息。</p><h3 id="⑥-Spam-Review-Detection-with-Graph-Convolutional-Networks-CIKM-2019"><a href="#⑥-Spam-Review-Detection-with-Graph-Convolutional-Networks-CIKM-2019" class="headerlink" title="⑥ Spam Review Detection with Graph Convolutional Networks. CIKM 2019."></a>⑥ Spam Review Detection with Graph Convolutional Networks. CIKM 2019.</h3><p>这篇论文被选为 CIKM 2019 工业最佳论文。其目标任务为闲鱼 APP 评论区的水军检测。</p><p>该文章是在异质信息网络上应用 GNN，与上述类似论文的区别是，该文章区分了三种类型的节点：<strong>用户，评论，商品</strong>；并<strong>直接对三种类型节点的特征同时聚合</strong>。之前的方法只是在同一种类型节点之间进行聚合。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/EvHCAT1VCBMHufvGspQTkiboMYDGQrpjZ0VkoLDFaOILybx0nic2Em0e8FGic1yFYLsv86nD6FicbwkQEF6uLrIaOg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=webp" alt="论文⑥设计的三部图"></p><p>和上述应用宝评论相似的是，闲鱼评论中也有很多水军的对抗行为。例如，水军通过特殊字符伪装 QQ 号来为自己的产品导流，或者从事欺诈活动。</p><p>该文章除了建模异质网络，还构建了一个基于评论文本相似度的同质图，然后通过应用经典的 GCN 模型来对评论单独编码。这种方式能够发现一些特征相似的评论，从而捕捉虚假评论的对抗行为。</p><p>该文章还提到了工业场景中的模型部署问题，提出了基于时间的邻居采样方法，即只选择相隔时间最近的几条评论之间加边，从而有效降低了计算的成本。</p><h3 id="⑦-Alleviating-the-Inconsistency-Problem-of-Applying-Graph-Neural-Network-to-Fraud-Detection-SIGIR-2020"><a href="#⑦-Alleviating-the-Inconsistency-Problem-of-Applying-Graph-Neural-Network-to-Fraud-Detection-SIGIR-2020" class="headerlink" title="⑦ Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection. SIGIR 2020."></a>⑦ Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection. SIGIR 2020.</h3><p>该文章借助Yelp 评论水军数据集，说明了现有 GNN 欺诈检测模型存在的问题，即<font color="red"><strong>忽略了图数据中的不一致性。</strong></font></p><p>例如可疑节点的邻居可能大部分是正常节点。该文章设计了一种基于邻居的过滤方式，从而在 GNN 邻居聚合时避免了噪音邻居的加入。</p><p>该论文通过<strong>预先定义的关系建立同质图</strong>，在聚合邻居信息时<strong>考虑到不同关系的重要程度</strong>。如下图所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/EvHCAT1VCBMHufvGspQTkiboMYDGQrpjZwzZlJ6H2woChzat1ibfS4UPEk69kR8n1rcl7PTNAYZT21e41ptCe3ibg/640" alt="论文⑦设计的邻居采样和聚合方法"></p><h3 id="⑧-Key-Player-Identification-in-Underground-Forums-over-Attributed-Heterogeneous-Information-Network-Embedding-Framework-CIKM-2019"><a href="#⑧-Key-Player-Identification-in-Underground-Forums-over-Attributed-Heterogeneous-Information-Network-Embedding-Framework-CIKM-2019" class="headerlink" title="⑧ Key Player Identification in Underground Forums over Attributed Heterogeneous Information Network Embedding Framework. CIKM 2019."></a>⑧ Key Player Identification in Underground Forums over Attributed Heterogeneous Information Network Embedding Framework. CIKM 2019.</h3><p>该文章利用异质信息网络对暗网论坛进行建模。该文章<strong>利用经典的异质信息网络建模方式</strong>，即<strong>采用元路径的方法建模同质节点的异质关系</strong>。不同的元路径类似于不同的关系。</p><p>同一个元路径下的节点构成一个同质图，<strong>该文章利用 GCN 对同质图进行编码</strong>。对于不同元路径下的编码信息，该文章利用注意力机制来学习不同元路径的重要程度。</p><h2 id="关键步骤"><a href="#关键步骤" class="headerlink" title="关键步骤"></a>关键步骤</h2><p>通过上面的介绍我们可以发现，将 GNN 应用到欺诈检测问题中，<font color="red"><strong>构建合适的图模型是最重要的步骤。</strong></font>不同邻居聚合方式之间的区别不大。</p><p>本文将设计 GNN 欺诈检测模型总结为下面四个步骤：</p><ol><li><p><strong>理解问题，数据，和现有条件。</strong></p><p>并不是所有的欺诈检测问题需要用图模型和GNN来解决。通过上面的介绍我们发现，<strong>如果数据有很多实体和关联关系，同时不存在明显的特征时</strong>，图模型和图神经网络比较适合。</p><p>对于大部分欺诈检测问题 ，从效果和成本角度来看，<strong>特征工程依然是最有效的方法。</strong>所以我们首先应该思考特征工程能不能解决现有问题，然后再考虑是否应用深度学习模型。</p><p>另外，图神经网络有<strong>端到端学习和半监督学习</strong>的特性，我们也需要考虑该问题是否适合用端到端学习建模，标签信息是否充足可靠，无监督的信息是否适合 GNN 使用等等。</p><p>由于图神经网络在大规模数据上部署需要很强的 infra 和算力的支持，如果现有生产环境不支持大规模深度学习部署，那么直接应用图神经网络显然不现实。 <strong>这就是为什么只有阿里腾讯等巨头公司才可以开展相关研究。</strong></p><p>当然，并不是一定要在生产环境中部署GNN，GNN 对于问题建模的方法也可以在特征工程中进行借鉴。</p></li><li><p><strong>设计有效的图结构和初始特征。</strong></p><p>这一步类似于特征工程中的特征选择，如果忽略了<font color="red"><strong>关键的关系和特征</strong></font>，在这样的图模型上进行下游任务是低效甚至无效的。</p><p>在设计图结构和节点特征时，我们需要思考其和问题及数据的关联。以评论水军检测为例，同一个 IP 地址下的账户有很强的关联关系，但是评论过同一个商品的用户不一定有很强的相似性。</p><p>同理，选取用户过去三十天的评论频率作为特征，和选择选取用户评论文本的词向量作为特征，这两种特征在不同应用场景下的有效性也不尽相同。</p><p><font color="red"><strong>所以我们在设计图结构和特征时一定要通过一些预处理和分析，来探究这种关系和特征是否适合GNN。</strong></font></p></li><li><p><strong>设计合适的采样方式。</strong></p><p>由于工业数据规模庞大且有很多冗余信息和噪音。在应用 GNN 之前进行数据采样和清洗是必须进行的步骤。</p><p>针对数据和问题的特性，我们可以从<strong>时间维度</strong>，<strong>特征维度</strong>，<strong>图结构维度</strong>对图数据进行采样。要保证最终保留的图数据是不稀疏且保留了重要的关系和特征信息。</p></li><li><p><strong>确定合适的GNN结构。</strong></p><p>由于大部分工业数据都是异质的，所以在设计 GNN 时要考虑如何处理异质信息。对于异质网络，在聚合邻居信息时，<font color="red"><strong>可以通过分层聚合，也可以通过全部转化为同质信息聚合。</strong></font></p><p>需要注意的是，<strong>在邻居密集的图数据上直接应用注意力机制，会导致运算量非常大。</strong>所以邻居以及视角重要程度的编码需要<strong>考虑到其实现的可行性</strong>。</p></li></ol><h2 id="现有挑战和未来展望"><a href="#现有挑战和未来展望" class="headerlink" title="现有挑战和未来展望"></a>现有挑战和未来展望</h2><p>通过上面的分析，目前 GNN 欺诈检测问题主要有两个挑战，即<strong>如何设计有效的图结构来建模数据和问题</strong>，以及<strong>如何部署 GNN 到大规模数据集上</strong>。</p><p>上述两个挑战，学术界更多关注第一个挑战，即<strong>思考欺诈检测问题和其他问题的区别，以及在应用 GNN 时需要做哪些适应性的调整。</strong></p><p>工业界关注第二个挑战多一点。随着 GNN 自动机器学习以及开源计算包的逐渐发展，相信未来规模化部署 GNN 也会变得简单。关于这方面最新的开源动态，可以关注下面两个 repo：</p><p>DGFraud: A Deep Graph-based Toolbox for Fraud Detection</p><p><a href="https://github.com/safe-graph/DGFraud">https://github.com/safe-graph/DGFraud</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/EvHCAT1VCBMHufvGspQTkiboMYDGQrpjZ6qWiaCZbyicGL0n2icUAIffFZpqSxgmcAQOV696aRDQHa2IoQABjlHDeg/640" alt="img"></p><p>DGL: Deep Graph Library</p><p><a href="https://github.com/dmlc/dgl">https://github.com/dmlc/dgl</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/EvHCAT1VCBMHufvGspQTkiboMYDGQrpjZK4J7eMvZMZ9yzrm3ibyxs6cEqHxXjH3jpKTMPL0cH1vEWojibroMRkqQ/640" alt="img"></p><h2 id="相关的GitHub仓库"><a href="#相关的GitHub仓库" class="headerlink" title="相关的GitHub仓库"></a>相关的GitHub仓库</h2><ol><li><a href="https://github.com/safe-graph/graph-fraud-detection-papers">https://github.com/safe-graph/graph-fraud-detection-papers</a></li><li><a href="https://github.com/safe-graph/DGFraud">https://github.com/safe-graph/DGFraud</a></li><li><a href="https://github.com/thunlp/GNNPapers">https://github.com/thunlp/GNNPapers</a></li><li><a href="https://github.com/benedekrozemberczki/awesome-fraud-detection-papers">https://github.com/benedekrozemberczki/awesome-fraud-detection-papers</a></li></ol><h2 id="相关的领域"><a href="#相关的领域" class="headerlink" title="相关的领域"></a>相关的领域</h2><ol><li>推荐系统</li></ol><h2 id="相关开源榜单"><a href="#相关开源榜单" class="headerlink" title="相关开源榜单"></a>相关开源榜单</h2><ol><li>Open Graph Benchmark（OGB）</li></ol><h2 id="相关的会议"><a href="#相关的会议" class="headerlink" title="相关的会议"></a>相关的会议</h2><ol><li><a href="https://www.kdd.org/">KDD</a></li><li><a href="http://icdm2019.bigke.org/">ICDM</a></li><li><a href="https://sigir.org/">SIGIR</a></li><li><a href="https://www.siam.org/conferences/cm/conference/sdm20">SDM</a></li><li><a href="https://www2019.thewebconf.org/">WWW</a></li><li><a href="http://www.cikmconference.org/">CIKM</a></li><li><a href="https://www.aaai.org/">AAAI</a></li><li>NeurlPS</li><li>ICLR</li></ol><h2 id="专有名词"><a href="#专有名词" class="headerlink" title="专有名词"></a>专有名词</h2><ol><li><code>Financial Fraud Detection</code> 金融欺诈检测</li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 综述 </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【ESE680 GNN】Lecture 4: Graph Neural Networks</title>
      <link href="/2020/12/04/ese680-gnn-lecture-4-graph-neural-networks/"/>
      <url>/2020/12/04/ese680-gnn-lecture-4-graph-neural-networks/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这节课主要是从<strong>频域</strong>的角度出发介绍GNN，带大家入门GNN。</p><ul><li><p>首先讲解graph filters；</p></li><li><p>然后通过【graph filter+pointwise nonlinearity】的构造graph perceptron，多个graph perceptron的叠加就是</p><p>the most simplest GNN了。</p></li><li><p>最后，在最基本的GNN的基础上，我们引入filter banks和multiple-input-multiple-output graph filters，得到MIMO GNNs。而MIMO GNNs就是我们常见的GNN了。</p></li></ul></blockquote><h2 id="Graph-Convolutional-Filter"><a href="#Graph-Convolutional-Filter" class="headerlink" title="Graph Convolutional Filter"></a>Graph Convolutional Filter</h2><h3 id="Graph-Convolutional-Filter的输入"><a href="#Graph-Convolutional-Filter的输入" class="headerlink" title="Graph Convolutional Filter的输入"></a>Graph Convolutional Filter的输入</h3><p>从频域的角度出发，Graph Convolutional Filter的输入和输出是Graph Signals。</p><h3 id="Graph-Convolutional-Filter的优化目标"><a href="#Graph-Convolutional-Filter的优化目标" class="headerlink" title="Graph Convolutional Filter的优化目标"></a>Graph Convolutional Filter的优化目标</h3><p>Empirical Risk Minimization通常包含3个组成成分：<br><img src="https://img-blog.csdnimg.cn/20201204185558684.png"><br>而机器学习的目标就是在function class里面找到损失最小的那个function：<br><img src="https://img-blog.csdnimg.cn/20201204185908488.png"><br>那么，对于GNN来说，它的function class是什么呢？就是graph convolutional filters！</p><blockquote><p>注意啦，在课程中graph convolutional filters和graph filters指的都是同一种东西。</p></blockquote><p>给定GSO（Graph Shift Operator）$S$，其感受野为$K$的graph convolutional filters可以统一表示成下方的形式：<br><img src="https://img-blog.csdnimg.cn/2020120419093549.png"><br>在这个function class里面，$S$和$h$都应该是超参数，但是由于往往$S$都是给定的，所以超参数只剩下h了。所以，<strong>GNN的优化目标</strong>就是：<br><img src="https://img-blog.csdnimg.cn/20201204191801999.png"></p><h3 id="Graph-Convolutional-Filter的输出"><a href="#Graph-Convolutional-Filter的输出" class="headerlink" title="Graph Convolutional Filter的输出"></a>Graph Convolutional Filter的输出</h3><p>从频域的角度出发，Graph Neural Networks的输入和输出是Graph Signals。</p><p>但是当我们训练集上的$y$不是graph signal的时候，可以在GNN输出上加一层Readout Layer（其Transformation Matrix为$A$），将GNN输出的graph signal变成和$y$相同的维度，具体的流程图如下：<br><img src="https://img-blog.csdnimg.cn/20201204192220915.png"><br>相应的，优化目标变成了：<br><img src="https://img-blog.csdnimg.cn/20201204192734587.png"><br>那么这个$A$如何定义呢？可能我们的第一想法是将其设置为超参数，但是老师说这是不可取的，$A$最好根据实际情况，自己进行定义，例如可以定义为：<br><img src="https://img-blog.csdnimg.cn/20201204192458213.png"></p><blockquote><p>其实在使用PyG的Cora数据集时，里面有使用<code>train_mask</code>和<code>test_mask</code>，它们就可以看做是read out specific nodes的操作~</p></blockquote><h2 id="Graph-Neural-Networks（GNNs）"><a href="#Graph-Neural-Networks（GNNs）" class="headerlink" title="Graph Neural Networks（GNNs）"></a>Graph Neural Networks（GNNs）</h2><h3 id="Graph-Perceptron"><a href="#Graph-Perceptron" class="headerlink" title="Graph Perceptron"></a>Graph Perceptron</h3><p><strong>Simplest GNN是由很多graph perceptron构成的</strong>，一个graph perceptron只是在Graph Convolutional Filter之后添加了一些东西，其结构如下：<br><img src="https://img-blog.csdnimg.cn/2020120419330124.png"></p><ul><li>第一个component是一个graph filter；</li><li>第二个component是一个Pointwise Nonlinearties👇<h4 id="Pointwise-Nonlinearties"><a href="#Pointwise-Nonlinearties" class="headerlink" title="Pointwise Nonlinearties"></a>Pointwise Nonlinearties</h4>首先是What？</li></ul><p>这种Pointwise非线性函数通过下面的操作图就能理解它是什么运作的了：<br><img src="https://img-blog.csdnimg.cn/20201204193615985.png"><br>我们常见的Pointwise Nonlinearities有：<br><img src="https://img-blog.csdnimg.cn/20201204193717666.png"><br>那么Why呢？<br>老师给出的答案：</p><ol><li><strong>Pointwise nonlinearities decrease variability. =&gt; They function as demodulators.</strong></li><li>Graph lters have limited expressive power because they can only learn linear maps。所以需要引入nonlinearity。<br>我没有理解。。。<h4 id="Graph-Perceptron的优化目标"><a href="#Graph-Perceptron的优化目标" class="headerlink" title="Graph Perceptron的优化目标"></a>Graph Perceptron的优化目标</h4>正是引入了Pointwise Nonlinearities的概念，所以Graph Perceptron的优化目标和Graph Convolutional Filter有点点不同：<br><img src="https://img-blog.csdnimg.cn/20201204194625583.png"><br>其中：<br><img src="https://img-blog.csdnimg.cn/20201204194603180.png"></li></ol><h3 id="GNN的结构"><a href="#GNN的结构" class="headerlink" title="GNN的结构"></a>GNN的结构</h3><p><strong>Simplest GNN是由很多graph perceptron构成的</strong>，对于一个拥有3层graph perceptron的GNN来说，其结构为：<br><img src="https://img-blog.csdnimg.cn/20201204195157684.png"></p><h3 id="GNN的优化目标"><a href="#GNN的优化目标" class="headerlink" title="GNN的优化目标"></a>GNN的优化目标</h3><p>通过上面的结构我们可以总结出GNN的function class为：</p><p><img src="https://img-blog.csdnimg.cn/20201204195334651.png"><br>那么它的优化目标，就是<strong>找到一组最合适的$\mathcal{H}$使得loss最小</strong>：<br><img src="https://img-blog.csdnimg.cn/20201204195809941.png"></p><blockquote><p>其实想想参数的数量是$K \times L$个。</p></blockquote><h3 id="GNN-vs-Graph-Filters"><a href="#GNN-vs-Graph-Filters" class="headerlink" title="GNN vs. Graph Filters"></a>GNN vs. Graph Filters</h3><table><thead><tr><th>比较方面</th><th>结果</th></tr></thead><tbody><tr><td>结构</td><td>GNN是在Graph Filter的基础上增加了<strong>pointwise nonlinearities</strong>和<strong>layer compositions</strong>；</td></tr><tr><td>效果</td><td>GNN表现更加优秀</td></tr><tr><td>迁移性</td><td>两者都具有迁移性，我们可以将$x$和$S$都看作模型的input，那么就可以在多个图之间共享$\mathcal{H}$了。</td></tr></tbody></table><h3 id="GNN-v-s-CNN"><a href="#GNN-v-s-CNN" class="headerlink" title="GNN v.s. CNN"></a>GNN v.s. CNN</h3><p>GNNs are proper generalizations of CNNs</p><h3 id="GNN-vs-FCNN"><a href="#GNN-vs-FCNN" class="headerlink" title="GNN vs. FCNN"></a>GNN vs. FCNN</h3><p>首先，回顾一下，我们常见的FCNN（Fully Connected Neural Network）的执行流程：<br><img src="https://img-blog.csdnimg.cn/20201204201231293.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ljX2N5MTk5OQ==,size_16,color_FFFFFF,t_70"><br>由于GNN可以看作是FCNN的子类，GNN只是对FCNN里面的$\mathcal{H}$增加了限制，因此FCNN的效果一定是不会弱于GNN的：<br><img src="https://img-blog.csdnimg.cn/20201204201639514.png"><br><strong>那为什么我们还要使用GNN呢？</strong></p><p>这就回到我们机器学习课上面讲的<strong>过拟合</strong>和<strong>模型泛化性</strong>了，GNN在这两个方面表现得更好。</p><blockquote><p>Because it exploits internal symmetries of graph signals codified in the graph shift operator，涉及到了Permutation Equivariance of Graph Neural Network的知识，这里就不多讲了。</p></blockquote><h2 id="Graph-Filter-Banks"><a href="#Graph-Filter-Banks" class="headerlink" title="Graph Filter Banks"></a>Graph Filter Banks</h2><p><strong>A Graph Filter Bank = A Set of Filters</strong></p><p>我们用$h^f$表示第$f$个filter，将$x$分别输入多个filter之后可以得到：<br><img src="https://img-blog.csdnimg.cn/20201204202743332.png"><br>上图的结果就是将我们的feature matrix从$N \times 1$变成了$N \times F$大小，扩充了feature matrix的维度。</p><h3 id="Output-Energy-of-a-Graph-Filter-in-the-GFT-Domain"><a href="#Output-Energy-of-a-Graph-Filter-in-the-GFT-Domain" class="headerlink" title="Output Energy of a Graph Filter in the GFT Domain"></a>Output Energy of a Graph Filter in the GFT Domain</h3><p>接下来介绍一个关于Graph Filter的output energy的定理：<br><img src="https://img-blog.csdnimg.cn/20201204203320800.png"><br>定理的证明如下：<br><img src="https://img-blog.csdnimg.cn/20201204203639678.png"></p><h2 id="Multiple-Feature-GNNs"><a href="#Multiple-Feature-GNNs" class="headerlink" title="Multiple Feature GNNs"></a>Multiple Feature GNNs</h2><blockquote><p>至此，我们所讲的graph signal都是$N \times 1$大小的，也就是每个结点只有一个特征，但是实际生活中的图上，结点的特征通常是多维度的，那么怎么处理多维特征的情况呢？<br><strong>前面我们介绍了filter bank，引入它的作用就在于实现处理多维特征。</strong></p></blockquote><p>对于输入的特征矩阵$\mathbf{X} \in \mathbb{R}^{N \times F}$，<font color="red">我们的处理思想就是对每一维的feature都进行filter bank操作。</font></p><h3 id="Multiple-Input-Multiple-Output-MIMO-Graph-Filters"><a href="#Multiple-Input-Multiple-Output-MIMO-Graph-Filters" class="headerlink" title="Multiple-Input-Multiple-Output (MIMO) Graph Filters"></a>Multiple-Input-Multiple-Output (MIMO) Graph Filters</h3><p>假设现在有一个Feature Matrix $\mathbf{X} = [\mathbf{x}^1,…,\mathbf{x}^f,…,\mathbf{x}^F]$，对于其中的每一个$\mathbf{x}^f$都使用一个大小为$G$的filter bank。</p><p>下面是对某一个$\mathbf{x}^f$使用一个大小为$G$的filter bank的示例：<br><img src="https://img-blog.csdnimg.cn/2020120420515215.png"><br>下面是所有的$\mathbf{x}^f$使用一个大小为$G$的filter bank的结果：<img src="https://img-blog.csdnimg.cn/20201204205214193.png"><br>可以想象，如果不做任何的处理，最终将会产生$F \times G$个特征维度，随着网络越来越深，这样的增长会限制我们的运行速度。<font color="red"><strong>因此，在这里，我们采用了上图中相加的方式。</strong></font>其公式表示为：<br><img src="https://img-blog.csdnimg.cn/20201204205413646.png"><br>最后的矩阵计算公式也非常的漂亮，只不过要多思考一下为什么是这样的矩阵计算公式：<br><img src="https://img-blog.csdnimg.cn/20201204205845685.png"></p><blockquote><p>Tips：上面的矩阵乘法是从<strong>列向量线性组合</strong>的角度看待的。</p></blockquote><h3 id="MIMO-GNN"><a href="#MIMO-GNN" class="headerlink" title="MIMO GNN"></a>MIMO GNN</h3><p>跟graph perceptron类比，我们将MIMO Graph Filters后面再加一个pointwise nonlinearity，就组成了MIMO perceptron。</p><p>再进行类比，叠加MIMO perceptron就构成了MIMO GNN（也就是我们最常用的GNN）了。<br><img src="https://img-blog.csdnimg.cn/20201204205920793.png"><br>对于拥有三个MIMO perceptron的MIMO GNN，它的执行流程图如下所示：<br><img src="https://img-blog.csdnimg.cn/20201204210004247.png"><br>再次总结一下，GNN每一层的操作如下：<br><img src="https://img-blog.csdnimg.cn/20201204232041380.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>torch.nn 与 torch.nn.functional 的区别</title>
      <link href="/2020/12/02/torch-nn-yu-torch-nn-functional-de-qu-bie/"/>
      <url>/2020/12/02/torch-nn-yu-torch-nn-functional-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<p>在初学Pytorch 创建模型的时候,总会出现不知道要把layer放在 <strong><code>__init()__</code></strong> 中还是 <strong><code>forwad()</code></strong> 中,也不知道到底该使用<code>nn.Conv2d</code>还是<code>F.conv2d</code>。为此带来了不必要的烦恼。</p><p>我为了搞清用法查看了官方doc并在pytorch论坛上做了询问,此为讨论的<a href="https://discuss.pytorch.org/t/beginner-should-relu-sigmoid-be-called-in-the-init-method/18689/13"><strong>链接</strong></a>，整理结果如下：</p><h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><p><code>torch.nn</code>这个模块下面存的主要是<strong>Module类</strong>。以<code>torch.nn.Conv2d</code>为例, 也就是说<code>torch.nn.Conv2d</code>这种”函数”其实是个 <strong>Module类</strong>，在实例化类后会初始化2d卷积所需要的参数。这些参数会在你做<code>forward()</code>和<code>backward()</code>之后根据loss进行更新，所以通常存放在定义模型的 <strong><code>__init__()</code></strong> 中，如:</p><pre><code class="python">class MyModel(nn.Module):    def __init__(self):        super(MyModel,self).__init__()        self.conv1 = nn.Conv2d(3,6,3,1,1)        self.act = nn.ReLU()    def forward(self,x):        x=self.act(self.conv1(x))        return x</code></pre><p><strong>那在定义模型时,可不可以把nn.Conv2d写在forward处?</strong></p><p>不可以</p><p>如果写成类似这样会有什么影响呢?</p><pre><code class="python">class MyModel(nn.Module):    def __init__(self):        super(MyModel, self).__init__()        self.act = nn.ReLU()            def forward(self, x):        # 把卷积函数写在forward中        x= nn.Conv2d(3, 6, 3, 1, 1)(x)        x = self.act(x)        return x</code></pre><p>把<code>nn.Conv2d</code>写在<code>forward()</code>中就相当于模型每次跑<code>forward()</code>的时候，都重新实例化了<code>nn.Conv2d</code>和<code>nn.Conv2d</code>的参数，导致模型学不到参数。</p><h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><p><code>torch.nn.functional.x</code> 为函数,与<code>torch.nn</code>不同, <code>torch.nn.x</code>中包含了初始化需要的参数等 <code>attributes</code> 而<code>torch.nn.functional.x</code>则需要把相应的weights 作为输入参数传递,才能完成运算, 所以用<code>torch.nn.functional</code>创建模型时需要创建并初始化相应参数。</p><p>例如：</p><pre><code class="python">import torch.nn.functional as Fclass MyModel(nn.Module):    def __init__(self):        super(MyModel, self).__init__()        self.act = nn.ReLU()        self.weighs = nn.Parameter(torch.rand(x,x,x,x))        self.bias = nn.Parameter(torch.rand(x))            def forward(self, x):        # 把卷积函数写在forward中,把w和b传入函数        x= F.conv2d(x,self.weighs,self.bias)        x = self.act(x)        return x</code></pre><h2 id="PyTorch官方推荐用法"><a href="#PyTorch官方推荐用法" class="headerlink" title="PyTorch官方推荐用法"></a>PyTorch官方推荐用法</h2><ul><li>具有学习参数的（例如，<code>conv2d</code>,<code> linear</code>, <code>batch_norm</code>）采用<code>nn.Xxx</code>方式；</li><li>没有学习参数的（例如，<code>maxpool</code>,<code>loss func</code>, <code>activation func</code>）等根据个人选择使用<code>nn.functional.xxx</code>或者<code>nn.Xxx</code>方式。</li></ul><p>但关于<code>dropout</code>，个人强烈推荐使用<code>nn.Xxx</code>方式，因为一般情况下只有训练阶段才进行<code>dropout</code>，在eval阶段都不会进行<code>dropout</code>。使用<code>nn.Xxx</code>方式定义<code>dropout</code>，在调用<code>model.eval()</code>之后，model中所有的dropout layer都关闭，但以<code>nn.function.dropout</code>方式定义<code>dropout</code>，在调用<code>model.eval()</code>之后并不能关闭<code>dropout</code>。</p><pre><code class="python">class Model1(nn.Module):        def __init__(self):        super(Model1, self).__init__()        self.dropout = nn.Dropout(0.5)            def forward(self, x):        return self.dropout(x)        class Model2(nn.Module):    def __init__(self):        super(Model2, self).__init__()    def forward(self, x):        return F.dropout(x)    m1 = Model1()m2 = Model2()inputs = torch.rand(10)print(m1(inputs))print(m2(inputs))print(20 * '-' + "eval model:" + 20 * '-' + '\r\n')m1.eval()m2.eval()print(m1(inputs))print(m2(inputs))</code></pre><p>输出：</p><p><img src="http://pic4.zhimg.com/80/v2-b2b202a21c13bdacba02a9d69d602b77_720w.jpg?source=1940ef5c" alt="img"></p><p>从上面输出可以看出<code>m2</code>调用了<code>eval</code>之后，dropout照样还在正常工作。当然如果你有强烈愿望坚持使用<code>nn.functional.dropout</code>，也可以采用下面方式来补救。</p><pre><code class="python">class Model3(nn.Module):    def __init__(self):        super(Model3, self).__init__()    def forward(self, x):        return F.dropout(x, training=self.training)</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>查看两者的doc即可看出区别:</p><blockquote><p> <strong><code>torch.nn.functional.conv2d</code></strong>(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor</p></blockquote><blockquote><p><strong>CLASS <code>torch.nn.Conv2d</code></strong>(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=‘zeros’)</p></blockquote><p>即一个侧重数据结构,一个侧重算法运算。 <strong>其实两个都是完成了同样的功能,只是实现方式有些不同而已</strong>：</p><table><thead><tr><th>torch.nn.X</th><th>torch.nn.functional.X</th></tr></thead><tbody><tr><td>是类</td><td>是函数</td></tr><tr><td>结构中包含所需要初始化的参数</td><td>需要在函数外定义并初始化相应参数,并作为参数传入</td></tr><tr><td>一般情况下放在<code>__init__</code> 中实例化,并在<code>forward()</code>中完成操作</td><td>一般在<code>__init__</code> 中初始化相应参数,在<code>forward()</code>中传入</td></tr><tr><td>运行效率也是近乎相同</td><td>运行效率也是近乎相同</td></tr></tbody></table><p>所以模型要么写成这样</p><pre><code class="python">class MyModel(nn.Module):    def __init__(self):        super(MyModel, self).__init__()        # which has its own hidden parameters         self.conv_like = nn.convlike()             def forward(self, x):        x = self.conv_like(x)</code></pre><p>要么写成这样：</p><pre><code class="python">class MyModel(nn.Module):    def __init__(self):        super(MyModel, self).__init__()        # which will be used in nn.functional.funs         self.func_params = params             def forward(self, x):        x = nn.functional.funs(x,self.func_params)</code></pre><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol><li><a href="https://www.zhihu.com/question/66782101/answer/579393790">PyTorch 中，nn 与 nn.functional 有什么区别？</a></li><li><a href="https://blog.csdn.net/wangweiwells/article/details/100531264">【pytorch】torch.nn 与 torch.nn.functional 的区别</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NumPy:对Axis的理解</title>
      <link href="/2020/12/02/numpy-dui-axis-de-li-jie/"/>
      <url>/2020/12/02/numpy-dui-axis-de-li-jie/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文地址：<a href="https://zhuanlan.zhihu.com/p/31275071">Numpy:对Axis的理解</a></p></blockquote><ul><li><strong>Axis就是数组层级</strong></li><li><strong>设axis=i，则Numpy沿着第i个下标变化的方向进行操作</strong></li><li><strong>Axis的应用</strong></li></ul><h2 id="Axis就是数组层级"><a href="#Axis就是数组层级" class="headerlink" title="Axis就是数组层级"></a>Axis就是数组层级</h2><p>要想理解axis，首先我们先要弄清楚“Numpy中数组的维数”和”线性代数中矩阵的维数”这两个概念以及它们之间的关系。在数学或者物理的概念中，dimensions被认为是在空间中表示一个点所需要的最少坐标个数，但是在Numpy中，dimensions指代的是数组的维数。比如下面这个例子：</p><pre><code class="python">&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.array([[1,2,3],[2,3,4],[3,4,9]])&gt;&gt;&gt; aarray([[1, 2, 3],       [2, 3, 4],       [3, 4, 9]])</code></pre><p>这个array的维数只有2，即axis轴有两个，分别是axis=0和axis=1。如下图所示，该二维数组的第0维(axis=0)有三个元素(左图)，即axis=0轴的长度length为3；第1维(axis=1)也有三个元素(右图)，即axis=1轴的长度length为3。正是因为axis=0、axis=1的长度都为3，矩阵横着竖着都有3个数，所以<strong>该矩阵在线性代数是3维的(rank秩为3)。</strong></p><p>因此，axis就是数组层级。</p><p>当axis=0，该轴上的元素有3个(数组的size为3)</p><p><code>a[0]</code>、<code>a[1]</code>、<code>a[2]</code></p><p>当axis=1，该轴上的元素有3个(数组的size为3)</p><p><code>a[0][0]</code>、<code>a[0][1]</code>、<code>a[0][2]</code></p><p>（或者<code>a[1][0]</code>、<code>a[1][1]</code>、<code>a[1][2]</code>）</p><p>（或者<code>a[2][0]</code>、<code>a[2][1]</code>、<code>a[2][2]</code>）</p><p>再比如下面shape为(3,2,4)的array：</p><pre><code class="python">&gt;&gt;&gt; b = np.array([[[1,2,3,4],[1,3,4,5]],[[2,4,7,5],[8,4,3,5]],[[2,5,7,3],[1,5,3,7]]])&gt;&gt;&gt; barray([[[1, 2, 3, 4],        [1, 3, 4, 5]],       [[2, 4, 7, 5],        [8, 4, 3, 5]],       [[2, 5, 7, 3],        [1, 5, 3, 7]]])&gt;&gt;&gt; b.shape(3, 2, 4)</code></pre><p>这个shape（用tuple表示）可以理解为在每个轴（axis）上的size，也即占有的长度（length)。为了更进一步理解，我们可以暂时把多个axes想象成多层layers。axis=0表示第一层(下图黑色框框)，该层数组的size为3，对应轴上的元素length = 3；axis=1表示第二层(下图红色框框)，该层数组的size为2，对应轴上的元素length = 2；axis=2表示第三层(下图蓝色框框)，对应轴上的元素length = 4。</p><p><img src="https://pic3.zhimg.com/80/v2-5c8aad40f74a3adee72f8f7fb5be89a6_720w.jpg" alt="img"></p><h2 id="设axis-i，则Numpy沿着第i个下标变化的方向进行操作"><a href="#设axis-i，则Numpy沿着第i个下标变化的方向进行操作" class="headerlink" title="设axis=i，则Numpy沿着第i个下标变化的方向进行操作"></a>设axis=i，则Numpy沿着第i个下标变化的方向进行操作</h2><p><strong>1. 二维数组示例：</strong></p><p>比如<code>np.sum(a, axis=1)</code>，结合下面的数组， <code>a[0][0]</code>=1、<code>a[0][1]</code>=2、<code>a[0][2]</code>=3 ，下标会发生变化的方向是数组的第一维。</p><p><img src="https://pic3.zhimg.com/80/v2-7a0716230a6f3d4840a6098001b1d2a2_720w.jpg" alt="img"></p><p>我们往下标会变化的方向，把元素相加后即可得到最终结果：</p><pre><code class="python">[  [6],  [9],  [16]]</code></pre><p><strong>2. 三维数组示例：</strong></p><p>再举个例子，比如下边这个<code>np.shape(a)=(3,2,4)</code>的3维数组，该数组第0维的长度为3(黑色框框)，再深入一层，第1维的长度为2(红色框框)，再深入一层，第2维的长度为4(蓝色框框)。</p><p><img src="https://pic3.zhimg.com/80/v2-2036beabc1c7e7d5dc8437a475766052_720w.jpg" alt="img"></p><p>如果我们要计算<code>np.sum(a, axis=1)</code>，在第一个黑色框框中，</p><p><img src="https://pic3.zhimg.com/80/v2-f77a4317e1fe6f139718acb7c80a3e86_720w.jpg" alt="img"></p><p>下标的变化方向如下所示：</p><p><img src="https://pic2.zhimg.com/80/v2-4b303d4c375ed35f2e3a336287b0980d_720w.jpg" alt="img"></p><blockquote><p><font color="red">需要注意的是，不变的部分就是新的数组的下标，例如上图将形成新数组的<code>[0][0]</code>,<code>[0][1]</code>、<code>[0][2]</code>元素~</font></p></blockquote><p>按照同样的逻辑处理第二个和第三个黑色的框框，可以得出最终结果：</p><p><img src="https://pic4.zhimg.com/80/v2-be09442b6ad87f96932412c5b8ab6fe3_720w.jpg" alt="img"></p><p>所以，依然是我们前边总结的那一句话，<strong>设axis=i，则Numpy沿着第i个下标变化的方向进行操作。</strong></p><p><strong>3. 四维数组示例：</strong></p><p>比如下面这个巨复杂的4维数组，</p><pre><code class="python">&gt;&gt;&gt; data = np.random.randint(0, 5, [4,3,2,3])&gt;&gt;&gt; dataarray([[[[4, 1, 0],         [4, 3, 0]],        [[1, 2, 4],         [2, 2, 3]],        [[4, 3, 3],         [4, 2, 3]]],       [[[4, 0, 1],         [1, 1, 1]],        [[0, 1, 0],         [0, 4, 1]],        [[1, 3, 0],         [0, 3, 0]]],       [[[3, 3, 4],         [0, 1, 0]],        [[1, 2, 3],         [4, 0, 4]],        [[1, 4, 1],         [1, 3, 2]]],       [[[0, 1, 1],         [2, 4, 3]],        [[4, 1, 4],         [1, 4, 1]],        [[0, 1, 0],         [2, 4, 3]]]])</code></pre><p>当axis=0时，numpy沿着第0维的方向进行求和，也就是第一个元素值=a0000+a1000+a2000+a3000=11,第二个元素=a0001+a1001+a2001+a3001=5，同理可得最后的结果如下：</p><pre><code class="python">&gt;&gt;&gt; data.sum(axis=0)array([[[11,  5,  6],        [ 7,  9,  4]],       [[ 6,  6, 11],        [ 7, 10,  9]],       [[ 6, 11,  4],        [ 7, 12,  8]]])</code></pre><p>当axis=3时，numpy沿着第3维的方向进行求和，也就是第一个元素值=a0000+a0001+a0002=5,第二个元素=a0010+a0011+a0012=7，同理可得最后的结果如下：</p><pre><code class="python">&gt;&gt;&gt; data.sum(axis=3)array([[[ 5,  7],        [ 7,  7],        [10,  9]],       [[ 5,  3],        [ 1,  5],        [ 4,  3]],       [[10,  1],        [ 6,  8],        [ 6,  6]],       [[ 2,  9],        [ 9,  6],        [ 1,  9]]])</code></pre><h2 id="Axis的应用"><a href="#Axis的应用" class="headerlink" title="Axis的应用"></a>Axis的应用</h2><p>例如现在我们收集了四个同学对苹果、榴莲、西瓜这三种水果的喜爱程度进行打分的数据（总分为10），每个同学都有三个特征：</p><pre><code class="python">&gt;&gt;&gt; item = np.array([[1,4,8],[2,3,5],[2,5,1],[1,10,7]])&gt;&gt;&gt; itemarray([[1, 4, 8],       [2, 3, 5],       [2, 5, 1],       [1, 10, 7]])</code></pre><p>每一行包含了同一个人的三个特征，如果我们想看看哪个同学最喜欢吃水果，那就可以用：</p><pre><code class="python">&gt;&gt;&gt; item.sum(axis=1)array([13, 10,  8, 18])</code></pre><p>可以大概看出来同学4最喜欢吃水果。</p><p>如果我们想看看哪种水果最受欢迎，那就可以用：</p><pre><code class="python">&gt;&gt;&gt; item.sum(axis = 0)array([ 6, 22, 21])</code></pre><p>可以看出基本是<strong>榴莲</strong>最受欢迎。</p>]]></content>
      
      
      <categories>
          
          <category> NumPy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 学习笔记 </tag>
            
            <tag> NumPy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor.data与Tensor.detach()的区别</title>
      <link href="/2020/12/02/tensor.data-yu-tensor.detach-de-qu-bie/"/>
      <url>/2020/12/02/tensor.data-yu-tensor.detach-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考资料：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/83329768">pytorch中的detach()和data</a></li><li><a href="https://github.com/pytorch/pytorch/issues/6990">Differences between .data and .detach #6990</a></li></ol></blockquote><pre><code class="python">import torch</code></pre><h2 id="Tensor-detach"><a href="#Tensor-detach" class="headerlink" title="Tensor.detach()"></a>Tensor.detach()</h2><pre><code class="python">a = torch.tensor([1,2,3.], requires_grad = True)</code></pre><pre><code class="python">out = a.sigmoid()</code></pre><pre><code class="python">c = out.detach()</code></pre><pre><code class="python">c.zero_()</code></pre><pre><code class="tex">tensor([0., 0., 0.])</code></pre><pre><code class="python">out   # modified by c.zero_() !!</code></pre><pre><code class="tex">tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)</code></pre><pre><code class="python">out.sum().backward()  # Requires the original value of out, but that was overwritten by c.zero_()</code></pre><pre><code>---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last)&lt;ipython-input-25-ada644350cc4&gt; in &lt;module&gt;----&gt; 1 out.sum().backward()  # Requires the original value of out, but that was overwritten by c.zero_()F:\Anaconda3\envs\pyg\lib\site-packages\torch\tensor.py in backward(self, gradient, retain_graph, create_graph)    219                 retain_graph=retain_graph,    220                 create_graph=create_graph)--&gt; 221         torch.autograd.backward(self, gradient, retain_graph, create_graph)    222     223     def register_hook(self, hook):F:\Anaconda3\envs\pyg\lib\site-packages\torch\autograd\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)    130     Variable._execution_engine.run_backward(    131         tensors, grad_tensors_, retain_graph, create_graph,--&gt; 132         allow_unreachable=True)  # allow_unreachable flag    133     134 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</code></pre><p>以及，</p><h2 id="Tensor-data"><a href="#Tensor-data" class="headerlink" title="Tensor.data"></a>Tensor.data</h2><pre><code class="python">a = torch.tensor([1,2,3.], requires_grad = True)</code></pre><pre><code class="python">out = a.sigmoid()</code></pre><pre><code class="python">c = out.data</code></pre><pre><code class="python">c.zero_()</code></pre><pre><code>tensor([0., 0., 0.])</code></pre><pre><code class="python">out  # out  was modified by c.zero_()</code></pre><pre><code>tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)</code></pre><pre><code class="python">out.sum().backward()</code></pre><pre><code class="python">a.grad  # The result is very, very wrong because `out` changed!</code></pre><pre><code>tensor([0., 0., 0.])</code></pre><p>这里说的很清楚，但事实上还有一个微小差别，请看下面的实验。</p><p>首先，笔者的配置是pytorch 1.1.0和python 3。</p><p>那detach和data两个区别到底是什么呢？首先都是无梯度的纯tensor，如下，</p><pre><code class="python">t = torch.tensor([0., 1.], requires_grad=True)t2 = t.detach()t3 = t.dataprint(t2.requires_grad, t3.requires_grad)  # ouptut: False, False</code></pre><pre><code>False False</code></pre><p>事实上，这两个新的tensor t2和t3和原始tensort都共享一块数据内存。</p><p><strong>其次，detach之后，在in-place的操作，并不会一定报错，而且，有些情况下，梯度反传计算是完全正确的！</strong>这是为什么呢？其实是基于一个很简单的道理，在计算梯度的时候，分两种计算方式，第一种，</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gl9byxc9d3j206b02xt8m.jpg" alt="sigmoid"></p><p>以及第二种，</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gl9bza5tmrj205j03wq2s.jpg" alt="二次函数"></p><p>你们一定看出来区别了，就是bp的时候自变量不一样，第一种是y，第二种是x。做个实验看看，</p><pre><code class="python">x = torch.tensor(0., requires_grad=True)y = x.sigmoid()y.detach().zero_()print(y)y.backward()</code></pre><pre><code>tensor(0., grad_fn=&lt;SigmoidBackward&gt;)---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last)&lt;ipython-input-34-bac852285750&gt; in &lt;module&gt;      3 y.detach().zero_()      4 print(y)----&gt; 5 y.backward()F:\Anaconda3\envs\pyg\lib\site-packages\torch\tensor.py in backward(self, gradient, retain_graph, create_graph)    219                 retain_graph=retain_graph,    220                 create_graph=create_graph)--&gt; 221         torch.autograd.backward(self, gradient, retain_graph, create_graph)    222     223     def register_hook(self, hook):F:\Anaconda3\envs\pyg\lib\site-packages\torch\autograd\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)    130     Variable._execution_engine.run_backward(    131         tensors, grad_tensors_, retain_graph, create_graph,--&gt; 132         allow_unreachable=True)  # allow_unreachable flag    133     134 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of SigmoidBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</code></pre><p>原因：这里修改了y的data，而bp的计算依赖这个data，因此报错，那换成另外一个操作呢，</p><pre><code class="python">x = torch.tensor(1., requires_grad=True)y = x ** 2y.detach().zero_()print(y)y.backward()print(x.grad)</code></pre><pre><code>tensor(0., grad_fn=&lt;PowBackward0&gt;)tensor(2.)</code></pre><p>这里成功输出如上。</p><p>总之，用detach还是很保险的，有些情况下是能够报错的，但并不全都是。事实上，直接修改图中的节点很少用到（有的话请在评论区给出），一般都是用来计算一些其他的辅助变量，用以debug，这是比较多。鉴于笔者能力有限，难免有疏漏之处，欢迎大家多交流、讨论、指正。</p><h2 id="再来一个例子加深印象"><a href="#再来一个例子加深印象" class="headerlink" title="再来一个例子加深印象"></a>再来一个例子加深印象</h2><pre><code class="python">x = torch.tensor([1,1,1],dtype=torch.float,requires_grad=True)</code></pre><pre><code class="python">y = torch.tensor([2,4,6],dtype=torch.float,requires_grad=True).view(-1,1)</code></pre><pre><code class="python">y</code></pre><pre><code>tensor([[2.],        [4.],        [6.]], grad_fn=&lt;ViewBackward&gt;)</code></pre><pre><code class="python">z = torch.matmul(x,y)</code></pre><pre><code class="python">tmp = y.detach()</code></pre><pre><code class="python">tmp.mul_(2)</code></pre><pre><code>tensor([[ 4.],        [ 8.],        [12.]])</code></pre><pre><code class="python">y.requires_grad</code></pre><pre><code>True</code></pre><pre><code class="python">z.backward()</code></pre><pre><code>---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last)&lt;ipython-input-43-40c0c9b0bbab&gt; in &lt;module&gt;----&gt; 1 z.backward()F:\Anaconda3\envs\pyg\lib\site-packages\torch\tensor.py in backward(self, gradient, retain_graph, create_graph)    219                 retain_graph=retain_graph,    220                 create_graph=create_graph)--&gt; 221         torch.autograd.backward(self, gradient, retain_graph, create_graph)    222     223     def register_hook(self, hook):F:\Anaconda3\envs\pyg\lib\site-packages\torch\autograd\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)    130     Variable._execution_engine.run_backward(    131         tensors, grad_tensors_, retain_graph, create_graph,--&gt; 132         allow_unreachable=True)  # allow_unreachable flag    133     134 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 1]], which is output 0 of ViewBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</code></pre><pre><code class="python">x.grad</code></pre><hr><pre><code class="python">import torch</code></pre><pre><code class="python">x = torch.tensor([1,1,1],dtype=torch.float,requires_grad=True)</code></pre><pre><code class="python">y = torch.tensor([2,4,6],dtype=torch.float,requires_grad=True)</code></pre><pre><code class="python">y</code></pre><pre><code>tensor([2., 4., 6.], requires_grad=True)</code></pre><pre><code class="python">z = torch.add(x,y)</code></pre><pre><code class="python">tmp = y.detach()</code></pre><pre><code class="python">tmp.mul_(2)</code></pre><pre><code>tensor([ 4.,  8., 12.])</code></pre><pre><code class="python">y</code></pre><pre><code>tensor([ 4.,  8., 12.], requires_grad=True)</code></pre><pre><code class="python">z.sum().backward()</code></pre><pre><code class="python">x.grad</code></pre><pre><code>tensor([1., 1., 1.])</code></pre>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning with PyTorch: A 60 Minute Blitz</title>
      <link href="/2020/11/30/deep-learning-with-pytorch-a-60-minute-blitz/"/>
      <url>/2020/11/30/deep-learning-with-pytorch-a-60-minute-blitz/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这不是PyTorch入门教程，而是入门教程的学后思考~</p></blockquote><h2 id="Why-PyTorch？"><a href="#Why-PyTorch？" class="headerlink" title="Why PyTorch？"></a>Why PyTorch？</h2><p>使用PyTorch有两个主要原因：</p><ol><li><strong>作为NumPy的替换物。</strong>支持GPU计算；</li><li><strong>深度学习库（掉包侠的最爱）。</strong>支持动态图，秒杀静态图TensorFlow~</li></ol><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>说了PyTorch可以作为NumPy的替代物，那么Tensor就是用来替代NumPy中的ndarry的~Tensor可以很方便地支持GPU计算，炼丹速度+++。</p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>现在假设有一个Tensor类型的变量<code>x</code>，常见的需要注意的属性如下：</p><ul><li><code>x.requires_grad</code>：决定了在<code>backpropagate</code>的时候，是否需要为<code>x</code>计算梯度。通常我们网络的参数需要设置为<code>True</code>，不是网络的参数一般设置为<code>False</code>；</li><li><code>x.grad</code>：<code>x</code>的梯度大小。通常在每次训练的时候都要将上一轮训练得到的<code>x.grad</code>清零，否则会得到叠加之后的<code>x.grad</code>；</li></ul><p>依我之见，在用PyTorch构造深度学习模型的时候，我们用到的Tensor类型的变量主要有两种：</p><ul><li><strong>我们自己定义的</strong>。比如输入的特征矩阵<code>x</code>；</li><li><strong>我们的网络定义的</strong>。比如全连接网络的参数<code>w</code>和<code>b</code>；</li></ul><p>这两种Tensor变量的区别在于：</p><ul><li>我们定义的Tensor，它的<code>.requires_grad</code>一般都是<code>False</code>，是不会通过<code>backward()</code>计算梯度的；</li><li>而网络里面定义的，它的<code>.requires_grad</code>一般都是<code>True</code>，就会通过<code>backward()</code>计算梯度的。</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>现在假设有一个Tensor类型的变量<code>x</code>，它的函数有：</p><ul><li><p><code>x.item()</code>：If you have a one element tensor, use <code>.item()</code> to get the value as a Python number；</p></li><li><p><code>x.numpy()</code>：返回一个<code>ndarry</code>，其维度和值与Tensor一模一样。<font color="red"><strong>同时注意它们是共享内存的，一个变量值改变，另一个变量值也会跟着改变。（类似于指针）</strong></font>；</p></li><li><p><code>x.to(deviceName)</code>：Tensors can be moved onto any device using the <code>.to()</code> method；</p></li><li><p><code>x.backward()</code>：以<code>x</code>为起点，求涉及到<code>x</code>计算的所有<code>x.requires_grad</code>为<code>True</code>的Tensor的梯度；一般<code>x.backward()</code>的要求是<code>x</code>是一个标量Tensor，否则需要按照下面的操作进行：</p><blockquote><p>If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to <code>backward()</code>, however if it has more elements, you need to specify a <code>gradient</code> argument that is a tensor of matching shape.</p></blockquote></li></ul><h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><p>PyTorch官方关于Function的描述：</p><blockquote><p>Tensor and <code>Function</code> are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a <code>.grad_fn</code> attribute that references a <code>Function</code> that has created the Tensor (except for Tensors created by the user - their <code>grad_fn is None</code>).</p></blockquote><p>上面的意思是说，每个Tensor都是通过一个<code>Function</code>产生的，Tensor会记录产生自己的<code>Function</code>是什么。</p><h2 id="神经网络搭建步骤"><a href="#神经网络搭建步骤" class="headerlink" title="神经网络搭建步骤"></a>神经网络搭建步骤</h2><ol><li>Define the neural network that has some learnable parameters (or weights)</li><li>Iterate over a dataset of inputs</li><li>Process input through the network</li><li>Compute the loss (how far is the output from being correct)</li><li>Propagate gradients back into the network’s parameters</li><li>Update the weights of the network, typically using a simple update rule: <code>weight = weight - learning_rate * gradient</code></li></ol><h2 id="代码技巧"><a href="#代码技巧" class="headerlink" title="代码技巧"></a>代码技巧</h2><ol><li><p>在测试阶段使用<code>with torch.no_grad()</code>可以对整个网络都停止自动求导，可以大大加快速度，也可以使用大的<code>batch_size</code>来测试。当然，也可以不使用with torch.no_grad；<a href="https://zhuanlan.zhihu.com/p/193102483">model.train() model.eval() with torch.no_grad()</a></p></li><li><p>在训练时，使用<code>optim.zero_grad()</code>，清空上一步的<code>grad</code>；</p></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd operations like <code>backward()</code>. Also <em>holds the gradient</em> w.r.t. the tensor.</li><li><code>nn.Module</code> - Neural network module. <em>Convenient way of encapsulating parameters</em>, with helpers for moving them to GPU, exporting, loading, etc.</li><li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically registered as a parameter when assigned as an attribute to a</em> <code>Module</code>.</li><li><code>autograd.Function</code> - Implements <em>forward and backward definitions of an autograd operation</em>. Every Tensor operation creates at least a single <code>Function</code> node that connects to functions that created a Tensor and <em>encodes its history</em>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows下同一个显卡配置多个CUDA工具包以及它们之间的切换</title>
      <link href="/2020/11/30/windows-xia-tong-yi-ge-xian-qia-pei-zhi-duo-ge-cuda-gong-ju-bao-yi-ji-ta-men-zhi-jian-de-qie-huan/"/>
      <url>/2020/11/30/windows-xia-tong-yi-ge-xian-qia-pei-zhi-duo-ge-cuda-gong-ju-bao-yi-ji-ta-men-zhi-jian-de-qie-huan/</url>
      
        <content type="html"><![CDATA[<p>故事是这样的，我最近在学GNN，之前使用的框架是百度团队的PGL，使用的CUDA是10.0版本的。</p><p>但是其实PYG才是主流的用于图卷积神经网络的框架，所以打算在PyTorch环境下安装PYG，但是PYG需要的PyTorch版本要大于1.4，我之前是1.2，所以要安装新的PyTorch。</p><p>问题又来了，PyTorch 1.7只支持CUDA 10.1，10.2和11.0。至此，我要么卸载CUDA 10.0安装10.1，要么尝试在Windows下配置多个CUDA工具包以及在它们之间的切换。我找到了一篇<a href="https://blog.csdn.net/qq_27825451/article/details/89135592">基于Tensorflow框架的安装博客</a>，希望能够解决我的问题。</p><blockquote><p><strong>1、多版本的CUDA以及cudnn安装</strong></p><p><strong>2、不同版本的tensorflow在CUDA之间的切换</strong></p><p><strong>3、验证自己的CUDA是否安装成功</strong></p></blockquote><p>前面的一片文章里面讲到了tensorflow、NVIDIA显卡驱动、CUDA工具包、cudnn之间的一些关系，详情请参考原文：</p><p><a href="https://blog.csdn.net/qq_27825451/article/details/89082978">https://blog.csdn.net/qq_27825451/article/details/89082978</a></p><p>tensorflow最大的问题就是版本问题，各个版本之间差异比较明显，我们有时候需要不同的tensorflow版本，而不同的版本对于CUDA toolKit的版本要求和cudnn的要求又不一样，我们肯定不能每次使用一个版本都重新安装，前面的那篇文章明确了几个基本观点：</p><p>（1）NVIDIA显卡驱动和CUDA ToolKit不是一一对应的，我们一般保持最新的驱动程序，安装其他不同版本的CUDA即可；</p><p>（2）CUDA和cudnn也不是严格的一一对应关系，但是这个官网上有着明确的对应连接，即很么版本的cuda配置什么样的cudnn；</p><p>所以如果需要在一台电脑上安装多个版本的CUDA和cudnn是完全可行的，由于Linux上面的配置教程很多，这里就不讲了，本文以windows为例来说明，</p><h2 id="1-多版本的CUDA以及cudnn安装"><a href="#1-多版本的CUDA以及cudnn安装" class="headerlink" title="1. 多版本的CUDA以及cudnn安装"></a>1. 多版本的CUDA以及cudnn安装</h2><p>由于CUDA会默认捆绑NVIDIA驱动程序，所以在安装的时候不要默认安装，<strong>一定要自定义安装</strong>，并且在自定义时<strong>只选择安装CUDA</strong>即可，其他的那些就不要安装了，我的电脑上安装的版本如下：</p><p><img src="https://img-blog.csdnimg.cn/2019040910092244.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3ODI1NDUx,size_16,color_FFFFFF,t_70" alt="img"></p><p>我们一般安装CUDA的时候就使用默认路径，安装到C盘即可，这样方便管理。</p><p>然后在NVIDIA官网上面下载CUDA对应的cudnn版本，解压之后将cudnn对应的三个文件拷贝到CUDA对应的文件夹之下，这个时候我们的环境变量应该如下所示：</p><p><img src="https://img-blog.csdnimg.cn/2019040910123739.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3ODI1NDUx,size_16,color_FFFFFF,t_70" alt="img"></p><p>现在多个版本的CUDA就安装完成了。</p><h2 id="2-不同版本的tensorflow在CUDA之间的切换"><a href="#2-不同版本的tensorflow在CUDA之间的切换" class="headerlink" title="2. 不同版本的tensorflow在CUDA之间的切换"></a>2. 不同版本的tensorflow在CUDA之间的切换</h2><p>网上有很多在Linux下面的CUDA的切换，其实都是通过环境变量的设置与配置来实现的，但是window这一点坐的很方便，</p><p><font color="red"><strong>不需要切换，不需要切换，不需要切换，只要环境变量PATH中有相应的CUDA路径即可，无需手动切换了。</strong></font></p><p>比如我的电脑上同事安装了</p><p>tensorflow1.9，它对应于CUDA9.0</p><p>tensorflow1.13，它对应于CUDA10.0</p><p>tensorflow2.0.0 alpha0，它对应于CUDA10.0</p><p>我可以使用任何一个版本，只要在环境变量中有对应的CUDA路径即可，</p><p>本人也是通过实验得出来的，首先我删除了CUDA10.0的环境变量,重启之后，发现tensorflow1.13和tensorflow2.0.0都不能使用了，但是tensorflow1.9还可以用；然后我又删除了CUDA9.0的环境变量，重启，这个时候tensorflow1.9也不能使用了；</p><p>接下来我又添加CUDA10.0的环境变量，重启，此时tensorflow1.13和tensorflow2.0.0又可以1使用了，然后我又通过添加CUDA9.0环境变量，重启，这时tensorflow1.9又可以使用了。</p><p><font color="red"><strong>总结：windows多个不同版本的CUDA使用时不需要切换，只要环境变量PATH中有相应的CUDA路径即可，无需手动切换了。tensorflow在运行的时候会自动在环境变量中寻找合适的CUDA版本，直到找到为止，如果没有，则会报错。</strong></font></p><h2 id="3-验证自己的CUDA是否安装成功"><a href="#3-验证自己的CUDA是否安装成功" class="headerlink" title="3. 验证自己的CUDA是否安装成功"></a>3. 验证自己的CUDA是否安装成功</h2><p>每一个版本的CUDA配置完成后，我们可以验证是否配置成功，主要使用CUDA内置的deviceQuery.exe 和 bandwithTest.exe这两个程序来验证。<br>首先win+R启动cmd，cd到安装目录下的 ，比如<em><strong>*我的安装目录是（以CUDA 10.1为例）：*</strong></em></p><p>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\demo_suite</p><p>执行**<code>bandwidthTest.exe</code>**和<code>***\*deviceQuery.exe\****这两个应用程序，得到下面的结果：</code></p><pre><code class="shell">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\demo_suite&gt;deviceQuerydeviceQuery Starting...  CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: "GeForce GTX 1070"  CUDA Driver Version / Runtime Version          10.1 / 10.1  CUDA Capability Major/Minor version number:    6.1  Total amount of global memory:                 8192 MBytes (8589934592 bytes)  (15) Multiprocessors, (128) CUDA Cores/MP:     1920 CUDA Cores  GPU Max Clock rate:                            1785 MHz (1.78 GHz)  Memory Clock rate:                             4004 Mhz  Memory Bus Width:                              256-bit  L2 Cache Size:                                 2097152 bytes  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers  Total amount of constant memory:               zu bytes  Total amount of shared memory per block:       zu bytes  Total number of registers available per block: 65536  Warp size:                                     32  Maximum number of threads per multiprocessor:  2048  Maximum number of threads per block:           1024  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)  Maximum memory pitch:                          zu bytes  Texture alignment:                             zu bytes  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)  Run time limit on kernels:                     Yes  Integrated GPU sharing Host Memory:            No  Support host page-locked memory mapping:       Yes  Alignment requirement for Surfaces:            Yes  Device has ECC support:                        Disabled  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)  Device supports Unified Addressing (UVA):      Yes  Device supports Compute Preemption:            No  Supports Cooperative Kernel Launch:            No  Supports MultiDevice Co-op Kernel Launch:      No  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0  Compute Mode:     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1, Device0 = GeForce GTX 1070Result = PASS</code></pre><p>和</p><pre><code class="shell">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\demo_suite&gt;bandwidthTest[CUDA Bandwidth Test] - Starting...Running on...  Device 0: GeForce GTX 1070 Quick Mode  Host to Device Bandwidth, 1 Device(s) PINNED Memory Transfers   Transfer Size (Bytes)        Bandwidth(MB/s)   33554432                     12180.7  Device to Host Bandwidth, 1 Device(s) PINNED Memory Transfers   Transfer Size (Bytes)        Bandwidth(MB/s)   33554432                     12782.8  Device to Device Bandwidth, 1 Device(s) PINNED Memory Transfers   Transfer Size (Bytes)        Bandwidth(MB/s)   33554432                     191225.0 Result = PASS</code></pre><p>当两个 <strong>Result=PASS</strong> 的时候，说明我们的安装配置是没有问题的。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>test</title>
      <link href="/2020/11/29/test/"/>
      <url>/2020/11/29/test/</url>
      
        <content type="html"><![CDATA[<pre><code class="python">squares = []for x in range(10):    squares.append(x**2)squares</code></pre><pre><code class="c">#include &lt;iostream&gt;using namespace std;int main{    return 0;}</code></pre><pre><code class="tex">blocks {  idx: 0  parent_idx: -1  vars {    name: "mean_1.tmp_0"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: 1        }      }    }    persistable: false  }  vars {    name: "square_error_cost_1.tmp_1"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: -1          dims: 1        }        lod_level: 0      }    }    persistable: false  }  vars {    name: "square_error_cost_1.tmp_0"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: -1          dims: 1        }        lod_level: 0      }    }    persistable: false    ...</code></pre><pre><code class="python">vars {    name: "x"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: -1          dims: 1        }        lod_level: 0      }    }    persistable: false</code></pre><pre><code class="protobuf">message BlockDesc {  required int32 idx = 1;  required int32 parent_idx = 2;  repeated VarDesc vars = 3;  repeated OpDesc ops = 4;}</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>5分钟PaddlePaddle知识点——program和它的朋友们</title>
      <link href="/2020/11/28/5-fen-zhong-paddlepaddle-zhi-shi-dian-program-he-ta-de-peng-you-men/"/>
      <url>/2020/11/28/5-fen-zhong-paddlepaddle-zhi-shi-dian-program-he-ta-de-peng-you-men/</url>
      
        <content type="html"><![CDATA[<blockquote><p>转载自AI Studio<a href="https://aistudio.baidu.com/aistudio/personalcenter/thirdview/39">某网友</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>paddle 静态图的开发结构可以看成“八股文”。一般进来，按照简单例子就直接组网训练。示例代码中会看到如下语句：</p><pre><code class="python">fluid.default_main_program()with fluid.program_guard(main_prog, startup_prog)     with fluid.unique_name.guard()</code></pre><p>如果完全照着来，能跑通。但是想要实现自己的东西来点变化，不理解这块儿可能容易碰到问题。不同版本 paddle 的具体实现会有区别，但大致相似，此处以 paddle 1.6 为例子讲解这部分知识点，总的一句话：</p><p><font color="red"><strong>program 只存储网络结构（比如参数的描述，算子的描述），但不存储实际参数的值，真正的参数保存在 scope 中，并且按照参数名字去重。</strong></font></p><h2 id="scope"><a href="#scope" class="headerlink" title="scope"></a>scope</h2><blockquote><p>scope：作用域。为了解释清楚 program，先介绍一个新概念：scope。</p></blockquote><p><strong>scope 在 paddle 里可以看作变量空间，存储fluid创建的变量</strong>。</p><p>在 Paddle/blob/develop/paddle/fluid/framework/scope.h 中可以看到存储的数据结构的定义：</p><pre><code class="c++">mutable std::unordered_map&lt;std::string, std::unique_ptr&lt;Variable&gt;, KeyHasher&gt; vars_;</code></pre><p><code>unordered_map</code> 数据结构类似于python中的dict，键是变量的名字，值是变量的指针。</p><p>类似 <code>fluid.default_main_program()</code>，**默认有一个全局的 <code>fluid.global_scope()</code>**。如果我们没有主动创建 <code>scope</code> 并且通过 <code>fluid.scope_guard()</code> 替换当前 <code>scope</code>，那么所有参数都在全局 <code>scope</code> 中。</p><p>参数创建的时机不是在组网时，而是在 <code>executor.run()</code> 执行时。可以从 <code>executor.run()</code> 的注释得到印证：</p><pre><code class="tex">And you could specify the scope to store the `Variables`  during the executor running if the scope is not set, the executor will use the global scope, i.e. `fluid.global_scope()`</code></pre><h2 id="program"><a href="#program" class="headerlink" title="program"></a>program</h2><p>program 的作用是<strong>存储网络结构，但不存储实际参数的值。</strong></p><p><strong>网络结构</strong>通过<strong>输入数据在各个操作(operator，即常说的OP)之间的流动</strong>来定义。我们以卷积为例，看看参数和op是如何组合到一起，并且<strong>加入到 prgram 中</strong>。在 <code>fluid.layers.nn.py</code> 文件里，可以看到 <code>conv2d</code> 的函数定义。其中有类似这样的代码：</p><pre><code class="tex">helper = LayerHelper(l_type, **locals())...filter_param = helper.create_parameter(...)helper.append_op(...)</code></pre><p>这表示，通过 <code>helper</code> 实例创建参数，然后将 <code>op</code> 添加到当前 <code>program</code> 中。</p><h3 id="create-parameter"><a href="#create-parameter" class="headerlink" title="create_parameter()"></a><code>create_parameter()</code></h3><p>在 helper 的 <code>create_parameter</code> 函数里，静态图模式会运行如下代码：</p><pre><code class="python">self.startup_program.global_block().create_parameter(    dtype=dtype,    shape=shape,    **attr._to_kwargs(with_initializer=True))return self.main_program.global_block().create_parameter(    dtype=dtype, shape=shape, **attr._to_kwargs())</code></pre><p>这表示参数会在初始化参数的 <code>startup_program</code> 和训练主体的 <code>main_program</code> 中创建。这也侧面支持我们需要通过 <code>startup_program</code> 来初始化网络参数。</p><h3 id="append-op"><a href="#append-op" class="headerlink" title="append_op"></a><code>append_op</code></h3><p>在 append_op 函数里，执行部分只有一行：</p><pre><code class="python">def append_op(self, *args, **kwargs):    return self.main_program.current_block().append_op(*args, **kwargs)</code></pre><p>这表示把 <code>op</code> 加入到训练主体的 <code>main_program</code> 中。</p><p>继续跟踪，可以找到 Block 的 <code>append_op()</code> 函数在 <code>fluid.framework.py</code> 文件中的实现，部分如下：</p><pre><code class="python">op_desc = self.desc.append_op()op = Operator(    block=self,    desc=op_desc,    type=kwargs.get("type", None),    inputs=kwargs.get("inputs", None),    outputs=kwargs.get("outputs", None),    attrs=kwargs.get("attrs", None))self.ops.append(op)</code></pre><p>这表示创建了一个操作 op，然后加入自己的一个集合中。再查看 fluid.framework.py 文件中 Operator 类的定义：</p><pre><code class="python">class Operator(object):    def __init__(self, block, desc, type=None, inputs=None,             outputs=None, attrs=None):</code></pre><p>可以看到，<code>op</code> 会记录输入和输出。换个角度，把 <code>op</code> 想像成水管的各种转接头，定义流入的水和流出的水，同时对流经的水做处理。</p><p>只有 <code>program</code> 和 <code>scope</code> 配合，才能表达完整模型（模型=网络结构+参数）。</p><h2 id="unique-name"><a href="#unique-name" class="headerlink" title="unique_name"></a>unique_name</h2><p>前面讲了 scope 中的变量名不能重复，但是Program中声明变量的时候可能没有指定变量名，那怎么生成名字的呢？还是以 conv2d 为例子，打开一个训练好的模型，可以看到参数名形如：conv2d_0.b_0 和     conv2d_0.w_0。因为变量是在 <code>helper.create_parameter()</code> 中创建，所以跟踪进去看看里面和名字相关的部分：</p><pre><code class="python">suffix = 'b' if is_bias else 'w'if attr.name is None:    attr.name = unique_name.generate(".".join([self.name, suffix]))</code></pre><p>首先可以看到先判断变量的尾缀是 w 还是 b。然后涉及到两个部分，一个是 helper 自己的 name；一个是 unique_name.generate() 方法。<br>先关注 helper 的 name。在创建 helper 实例之前有这么一小段：</p><pre><code class="python"># conv2d 的 l_type 默认是 conv2d，在特殊条件下是 depthwise_conv2d l_type = 'conv2d'if (num_channels == groups and num_filters % num_channels == 0 and        not use_cudnn):    l_type = 'depthwise_conv2d'helper = LayerHelper(l_type, **locals())</code></pre><p>l_type 是操作类型。在 fluid.layer_helper.py 中可以看到 LayerHelper 类的定义：</p><pre><code class="python">def __init__(self, layer_type, **kwargs):    self.kwargs = kwargs    # 在没有设置 name 的情况下，name 为 None    name = self.kwargs.get('name', None)    if name is None:        self.kwargs['name'] = unique_name.generate(layer_type)    super(LayerHelper, self).__init__(        self.kwargs['name'], layer_type=layer_type)</code></pre><p>当没有指定 name 的情况下，也会通过 unique_name.generate() 产生名字。重点关注一下这个函数的实现。在 fluid.unique_name.py 中可以看到 generate 函数的定义，其中有一段注释：</p><pre><code class="python">def generate(key):    """    Examples:             import paddle.fluid as fluid            name1 = fluid.unique_name.generate('fc')            name2 = fluid.unique_name.generate('fc')            print(name1, name2) # fc_0, fc_1    """</code></pre><p>简单来说，通过对每一个名字产生当前的计数，从而生成全局唯一的名字。</p><p>把 conv2d_0.w_0 的过程串起来：首先生成 helper，用 conv2d 的类型产生了 conv2d_0 的 helper.name。然后生成具体参数的时候，根据类型，以 conv2d_0.w 和 conv2d_0.d 为参数送入 unique_name.generate()，产生 conv2d_0.b_0 和 conv2d_0.w_0。</p><p>以上就是这个部分的知识点总结。如果有什么问题、建议或者觉得内容有误，欢迎留言。</p><p>这个部分还可以参考：</p><p><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/addon_development/design_idea/fluid_design_idea.html#id2">https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/addon_development/design_idea/fluid_design_idea.html#id2</a></p><hr><p>如果喜欢，可以在AI Studio <a href="https://aistudio.baidu.com/aistudio/personalcenter/thirdview/39">关注我</a></p><p>还可以关注我的微信公众号：程序员宇波没有智。分享平凡程序员生活的点滴</p><div align="center">  <img src="https://ai-studio-static-online.cdn.bcebos.com/cfebc175a587454683ffe41998b82d138f3b79f8eb7c4be2a3735aa36bbf47be"></div>## 自身使用体会<h3 id="关于train-program和test-program变量共享问题"><a href="#关于train-program和test-program变量共享问题" class="headerlink" title="关于train_program和test_program变量共享问题"></a>关于train_program和test_program变量共享问题</h3><p>在这份PGL图神经网络课程中，有这样一段代码：</p><pre><code class="python">train_program = fluid.default_main_program()                startup_program = fluid.default_startup_program()           with fluid.program_guard(train_program, startup_program):     with fluid.unique_name.guard():         gw, loss, acc, pred = build_model(dataset,                               config=config,                            phase="train",                            main_prog=train_program)test_program = fluid.Program()with fluid.program_guard(test_program, startup_program):      with fluid.unique_name.guard():                                _gw, v_loss, v_acc, v_pred = build_model(dataset,            config=config,            phase="test",            main_prog=test_program)</code></pre><p>通过两个<code>with fluid.program_guard()</code>语句，我们定义了两段<strong>局部的</strong>Program——<code>train_program</code>和<code>test_program</code>，它们是<code>Program</code>类型的，里面包含了网络框架描述信息。同时，<code>startup_program</code>同时参与了两个<code>with fluid.program_guard()</code>语句，所以它包含两者的所有信息。这里有一个非常有趣的现象就是，由于<code>Scope</code>是同名的变量是共享的内容，所以，当<code>train_program</code>训练完后<code>test_program</code>在运行时，发现自己的变量早已经被<code>train_program</code>创建了，于是它就会直接使用已有的参数值，这也是<code>PaddlePaddle</code>里面能够同时训练和测试的原因，属于一个极小的细节，如下图：</p><pre><code class="python">for epoch in range(epoch):    # Full Batch 训练    # 设定图上面那些节点要获取    # node_index: 训练节点的nid        # node_label: 训练节点对应的标签    feed_dict["node_index"] = np.array(train_index, dtype="int64")    feed_dict["node_label"] = np.array(train_label, dtype="int64")        train_loss, train_acc = exe.run(train_program,                                feed=feed_dict,                                fetch_list=[loss, acc],                                return_numpy=True)    # Full Batch 验证    # 设定图上面那些节点要获取    # node_index: 训练节点的nid        # node_label: 训练节点对应的标签    feed_dict["node_index"] = np.array(val_index, dtype="int64")    feed_dict["node_label"] = np.array(val_label, dtype="int64")    val_loss, val_acc = exe.run(test_program,                            feed=feed_dict,                            fetch_list=[v_loss, v_acc],                            return_numpy=True)    print("Epoch", epoch, "Train Acc", train_acc[0], "Valid Acc", val_acc[0])</code></pre>]]></content>
      
      
      <categories>
          
          <category> PaddlePaddle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PaddlePaddle设计思想</title>
      <link href="/2020/11/28/paddlepaddle-she-ji-si-xiang/"/>
      <url>/2020/11/28/paddlepaddle-she-ji-si-xiang/</url>
      
        <content type="html"><![CDATA[<blockquote><p>转载自<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/addon_development/design_idea/fluid_design_idea.html#id1">PaddlePaddle官方文档</a></p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇文档主要介绍飞桨（PaddlePaddle，以下简称Paddle）底层的设计思想，帮助用户更好的理解框架运作过程。</p><p>阅读本文档，您将了解：</p><ul><li>Paddle 内部的执行流程</li><li>Program 如何描述模型</li><li>Executor 如何执行运算</li></ul><h2 id="1-Paddle内部执行流程"><a href="#1-Paddle内部执行流程" class="headerlink" title="1. Paddle内部执行流程"></a>1. Paddle内部执行流程</h2><p>Paddle使用一种<strong>编译器式</strong>的执行流程，分为<strong>编译时</strong>和<strong>运行时</strong>两个部分，具体包括：<strong>编译器定义 Program</strong> ，<strong>创建Executor 运行 Program</strong>。</p><p>本地训练任务执行流程图如下所示：</p><p>（<code>transpiler</code>：转义器）</p><p><img src="https://githubraw.cdn.bcebos.com/PaddlePaddle/FluidDoc/develop/doc/fluid/advanced_usage/design_idea/image/fluid_process.png" alt="img"></p><ol><li>编译时，用户编写一段python程序，通过调用 Paddle 提供的算子，向一段 Program 中添加变量（Tensor）以及对变量的操作（Operators 或者 Layers）。<font color="red"><strong>用户只需要描述核心的前向计算</strong></font>，不需要关心反向计算、分布式下以及异构设备下如何计算。</li><li>原始的 Program 在框架内部转换为中间描述语言： <code>ProgramDesc</code>。</li><li><code>Transpiler</code> 接受一段 <code>ProgramDesc</code> ，输出一段变化后的 <code>New ProgramDesc</code> ，作为后端的 <code>C++ Executor</code> 最终需要执行的 Program 。（ <code>Transpiler</code> 并非必需步骤。）</li><li>执行 <code>ProgramDesc</code> 中定义的 Operator（可以类比为程序语言中的指令），在执行过程中会为 Operator <strong>创建所需的输入输出</strong>并进行管理。</li></ol><blockquote><p>从上面的过程可以总结到，<font color="red">编译的结果只是生成一些“描述性”的文件，并不会在内存上分配空间并创建变量，这些动态操作都是要在Executor执行之后才能进行的！</font></p></blockquote><h2 id="2-Program设计思想"><a href="#2-Program设计思想" class="headerlink" title="2. Program设计思想"></a>2. Program设计思想</h2><p>用户完成网络定义后，一段 Paddle 程序中通常存在 2 个 Program：</p><ol><li><code>fluid.default_startup_program</code>：定义了模型参数初始化、优化器参数初始化、reader初始化等各种操作。</li></ol><pre><code class="tex">default_startup_program 可以由框架自动生成，使用时无需显式地创建如果调用修改了参数的默认初始化方式，框架会自动的将相关的修改加入default_startup_program</code></pre><ol start="2"><li><code>fluid.default_main_program</code> ：定义了神经网络模型，前向反向计算，以及模型参数更新、优化器参数更新等各种操作。</li></ol><pre><code class="tex">使用Paddle的核心就是构建起 default_main_program</code></pre><h3 id="Programs-and-Blocks"><a href="#Programs-and-Blocks" class="headerlink" title="Programs and Blocks"></a>Programs and Blocks</h3><blockquote><p>Program = 嵌套的blocks</p></blockquote><p>Paddle 的 Program 的基本结构是一些嵌套 blocks，形式上类似一段 C++ 或 Java 程序。</p><p>blocks中包含：</p><ul><li>本地<strong>变量</strong>的<strong>定义</strong>（只是定义而已！）</li><li>一系列的<strong>operator</strong></li></ul><p>block的概念与通用程序一致，例如在下列这段C++代码中包含三个block：</p><pre><code class="c++">#include &lt;iostream&gt;int main() {    int x = 5; // block 0    int y = 4; // block 0    int out;   // block 0        if (x &lt; y) { // block 0        out = 1; // block 1    } else {        out = 0; // block 2    }        std::cout &lt;&lt; out &lt;&lt; std::endl;    return 0;}</code></pre><p>类似的，在下列 Paddle 的 Program 包含3段block：</p><pre><code class="python">import paddle.fluid as fluidx = fluid.data(name='x', shape=[1], dtype='int64') # block 0y = fluid.data(name='y', shape=[1], dtype='int64') # block 0def true_block():    return fluid.layers.fill_constant(dtype='int64', value=1, shape=[1]) # block 1    def false_block():    return fluid.layers.fill_constant(dtype='int64', value=0, shape=[1]) # block 2condition = fluid.layers.less_than(x, y) # block 0out = fluid.layers.cond(condition, true_block, false_block) # block 0</code></pre><h3 id="ProgramDesc-and-BlockDesc"><a href="#ProgramDesc-and-BlockDesc" class="headerlink" title="ProgramDesc and BlockDesc"></a>ProgramDesc and BlockDesc</h3><p>用户书写的python代码中的<code>block</code>与<code>program</code>信息在Paddle中以<a href="https://en.wikipedia.org/wiki/Protocol_Buffers">protobuf</a> 格式保存，所有的<code>protobuf</code>信息被定义在<code>framework.proto</code>中，在Paddle中被称为<code>BlockDesc</code>和<code>ProgramDesc</code>。<code>ProgramDesc</code>和<code>BlockDesc</code>的概念类似于一个<a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">抽象语法树</a>。</p><p><code>BlockDesc</code>中包含本地变量的定义 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/api_guides/low_level/program.html#variable"><code>vars</code></a>，和一系列的operator<code>ops</code>：</p><pre><code class="protobuf">message BlockDesc {  required int32 idx = 1;  required int32 parent_idx = 2;  repeated VarDesc vars = 3;  repeated OpDesc ops = 4;}</code></pre><p><code>parent_idx</code>表示父块，因此block中的操作符可以引用本地定义的变量，也可以引用祖先块中定义的变量。</p><p>Program 中的每层 block 都被压平并存储在数组中。blocks ID是这个数组中块的索引。</p><pre><code class="protobuf">message ProgramDesc {  repeated BlockDesc blocks = 1;}</code></pre><h3 id="使用Blocks的Operator"><a href="#使用Blocks的Operator" class="headerlink" title="使用Blocks的Operator"></a>使用Blocks的Operator</h3><p>前面<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/addon_development/design_idea/fluid_design_idea.html#ProgramsAndBlocks">Programs and Blocks</a>的例子中，IfElseOp这个Operator包含了两个block——true分支和false分支。</p><p>下述<code>OpDesc</code>的定义过程描述了一个operator可以包含哪些属性：</p><pre><code class="python">message OpDesc {  AttrDesc attrs = 1;  ...}</code></pre><p>属性可以是block的类型，实际上就是上面描述的block ID:</p><pre><code class="python">message AttrDesc {  required string name = 1;  enum AttrType {    INT = 1,    STRING = 2,    ...    BLOCK = ...  }  required AttrType type = 2;  optional int32 block = 10; // when type == BLOCK  ...}</code></pre><h2 id="3-Executor设计思想"><a href="#3-Executor设计思想" class="headerlink" title="3. Executor设计思想"></a>3. Executor设计思想</h2><p>Executor 在运行时将接受一个<code>ProgramDesc</code>、一个<code>block_id</code>和一个<code>Scope</code>：</p><ul><li><code>ProgramDesc</code>是<code>block</code>的列表，每一项包含<code>block</code>中所有参数和<code>operator</code>的<code>protobuf</code>定义；</li><li><code>block_id</code>的作用是<strong>指定入口块</strong>；</li><li><code>Scope</code>是<strong>所有变量实例的容器</strong>。</li></ul><p>其中 <code>Scope</code> 包含了 <code>name</code> 与 <code>Variable</code> 的映射，所有变量都被定义在 <code>Scope</code> 里。<font color="red"><strong>大部分API会默认使用 <code>global_scope</code></strong></font> ，例如 <code>Executor.run</code> ，您也可以指定网络运行在某个特定的 <code>Scope</code> 中，一个网络可以在不同的 <code>Scope</code>内运行，并在该 <code>Scope</code> 内更新不同的 <code>Variable</code>。</p><p>完整的编译执行的具体过程如下图所示：</p><p><img src="https://githubraw.cdn.bcebos.com/PaddlePaddle/FluidDoc/develop/doc/fluid/advanced_usage/design_idea/image/executor_design.png" alt="img"></p><ol><li>Executor 为每一个block创建一个Scope，Block是可嵌套的，因此Scope也是可嵌套的。</li><li>创建所有Scope中的变量。</li><li>创建并执行所有operator。</li></ol><p>Executor的C++实现代码如下：</p><pre><code class="c++">class Executor{    public:        void Run(const ProgramDesc&amp; pdesc,                Scope* scope,                int block_id) {            auto&amp; block = pdesc.Block(block_id);            //创建所有变量            for (auto&amp; var : block.AllVars())                scope-&gt;Var(Var-&gt;Name());            }            //创建OP并执行            for (auto&amp; op_desc : block.AllOps()){                auto op = CreateOp(*op_desc);                op-&gt;Run(*local_scope, place_);            }        };</code></pre><p><strong>创建Executor</strong></p><p>Paddle中使用<code>fluid.Executor(place)</code>创建Executor，place属性由用户定义，代表程序将在哪里执行。</p><p>下例代码表示创建一个Executor，其运行场所在CPU内：</p><pre><code class="python">cpu=fluid.CPUPlace()exe = fluid.Executor(cpu)</code></pre><p><strong>运行Executor</strong></p><p>Paddle使用<code>Executor.run()</code>来运行程序。定义中通过<code>feed</code>映射获取数据，通过<code>fetch_list</code>获取结果：</p><pre><code class="python">...x = numpy.random.random(size=(10, 1)).astype('float32')outs = exe.run(    feed={'X': x},    fetch_list=[loss.name])</code></pre><h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><p>本节通过<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/beginners_guide/basic_concept/programming_guide/programming_guide.html">编程指南</a>中简单的线性回归例子，为您介绍上述内容如何在代码中实现。</p><p><strong>定义Program</strong></p><p>您可以随意定义自己的数据和网络结构，定义的结果都将作为一段 Program 被 Paddle 接收，Program 的基本结构是一些 blocks，本节的 Program 仅包含一个 block 0：</p><pre><code class="python">#加载函数库import paddle.fluid as fluid #block 0import numpy#定义数据train_data=numpy.array([[1.0],[2.0],[3.0],[4.0]]).astype('float32')y_true = numpy.array([[2.0],[4.0],[6.0],[8.0]]).astype('float32')#定义网络x = fluid.data(name="x",shape=[None, 1],dtype='float32')y = fluid.data(name="y",shape=[None, 1],dtype='float32')y_predict = fluid.layers.fc(input=x,size=1,act=None)#定义损失函数cost = fluid.layers.square_error_cost(input=y_predict,label=y)avg_cost = fluid.layers.mean(cost)#定义优化方法sgd_optimizer = fluid.optimizer.SGD(learning_rate=0.01)sgd_optimizer.minimize(avg_cost)</code></pre><p>完成上述定义，也就是完成了 <code>fluid.default_main_program</code> 的构建过程，<code>fluid.default_main_program</code> 中承载着神经网络模型，前向反向计算，以及优化算法对网络中可学习参数的更新。</p><p>此时可以输出这段 Program 观察定义好的网络形态：</p><pre><code class="python">print(fluid.default_main_program().to_string(True))</code></pre><p>完整<code>ProgramDesc</code>可以在本地查看，本次仅节选前三个变量的结果如下：</p><pre><code class="python">blocks {  idx: 0  parent_idx: -1  vars {    name: "mean_1.tmp_0"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: 1        }      }    }    persistable: false  }  vars {    name: "square_error_cost_1.tmp_1"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: -1          dims: 1        }        lod_level: 0      }    }    persistable: false  }  vars {    name: "square_error_cost_1.tmp_0"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: -1          dims: 1        }        lod_level: 0      }    }    persistable: false    ...</code></pre><p>从输出结果中可以看到，整个定义过程在框架内部转化为了一段<code>ProgramDesc</code>，以block idx为索引。本次线性回归模型中仅有1个block，ProgramDesc中也仅有block 0一段BlockDesc。</p><p>BlockDesc中包含定义的 vars 和一系列的 ops，以输入x为例，python代码中定义 x 是一个数据类型为”float32”的1维数据：</p><pre><code class="python">x = fluid.data(name="x",shape=[None, 1],dtype='float32')</code></pre><p>在BlockDesc中，变量x被描述为：</p><pre><code class="python">vars {    name: "x"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: -1          dims: 1        }        lod_level: 0      }    }    persistable: false</code></pre><p>在Paddle中所有的数据类型都为<code>LoD-Tensor</code>，对于不存在序列信息的数据（如此处的变量X），其<code>lod_level=0</code>。</p><p>dims表示数据的维度，这里表示 x 的维度为[-1,1]，其中-1是batch的维度，无法确定具体数值时，Paddle 自动用 -1 占位。</p><p>参数<code>persistable</code>表示该变量在整个训练过程中是否为持久化变量。</p><p><strong>创建Executor</strong></p><p>Paddle使用Executor来执行网络训练，Executor运行细节请参考<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/addon_development/design_idea/fluid_design_idea.html#Executor%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3">Executor设计思想</a>的介绍。作为使用者，实际并不需要了解内部机制。</p><p>创建Executor只需调用 <code>fluid.Executor(place)</code> 即可，在此之前请您依据训练场所定义place变量：</p><pre><code class="python"> #在CPU内执行训练 cpu = fluid.CPUPlace() #创建Executor exe = fluid.Executor(cpu)</code></pre><p><strong>运行Executor</strong></p><p>Paddle使用Executor.run来运行一段Program。</p><p>正式进行网络训练前，需先执行参数初始化。其中 <code>defalut_startup_program</code> 中定义了模型参数初始化、优化器参数初始化、reader初始化等各种操作。</p><pre><code class="python"> #参数初始化 exe.run(fluid.default_startup_program())</code></pre><p>由于传入数据与传出数据存在多列，因此 Paddle 通过 feed 映射定义数据的传输数据，通过 fetch_list 取出期望结果：</p><pre><code class="python">#开始训练 outs = exe.run(     feed={'x':train_data,'y':y_true},     fetch_list=[y_predict.name,avg_cost.name])</code></pre><p>上述代码段中定义了train_data传入x变量，y_true传入y变量，输出y的预测值和最后一轮cost值。</p><p>输出结果为：</p><pre><code class="python">[array([[1.5248038],       [3.0496075],       [4.5744114],       [6.099215 ]], dtype=float32), array([1.6935859], dtype=float32)]</code></pre><p>至此您已经了解了Paddle内部的执行流程的核心概念，更多框架使用细节可以参考<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/user_guides/index_cn.html">典型案例</a>。</p><h2 id="自身使用体会"><a href="#自身使用体会" class="headerlink" title="自身使用体会"></a>自身使用体会</h2><h3 id="关于编译和运行"><a href="#关于编译和运行" class="headerlink" title="关于编译和运行"></a>关于编译和运行</h3><p>其实整个<code>PaddlePaddle</code>的<strong>编译</strong>和<strong>运行</strong>的两个过程，可以分别对应【画一张深度学习网络框架】和【给这个网络框架喂数据】。</p><ul><li><p><strong>编译：画一张深度学习网络框架</strong></p><p>这个深度学习网络就是我们常在论文中看的那种网络模型之类的，如下图：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gl5bthly70j21140rsgpy.jpg" alt="示例"></p><blockquote><p>上面的红色字母就是我们声明的变量，并没有赋值哦~只是声明好了的，给个名字。</p></blockquote><p>这个框架由两个部分组成：变量，算子。既然是框架，那么说明它只是一个架子，里面没有真实的东西，比如定义了变量名，但是没有给变量名赋值；定义了算子，但是并没有运算。要等到<strong>运行</strong>的时候才会给变量名</p><p>举个变量的例子：</p><pre><code class="python">with fluid.program_guard(train_program, startup_program):     with fluid.unique_name.guard():         gw, loss, acc, pred = build_model(dataset,                               config=config,                            phase="train",                            main_prog=train_program)</code></pre><p>执行了上面的代码后，我们打印<code>loss</code>，<code>acc</code>，<code>pred</code>的数据类型，我们直观上可能会以为它们是Tensor，但是根据之前说的它只是框架，因此会得到下面的结果：</p><pre><code class="python">type(loss): &lt;class 'paddle.fluid.framework.Variable'&gt;type(acc): &lt;class 'paddle.fluid.framework.Variable'&gt;type(pred): &lt;class 'paddle.fluid.framework.Variable'&gt;</code></pre><p>没错，真的就是仅仅定义了框架，再看看<code>loss</code>到底是什么：</p><pre><code class="python">print(loss)output:    name: "mean_0.tmp_0"    type {      type: LOD_TENSOR      lod_tensor {        tensor {          data_type: FP32          dims: 1        }      }    }    persistable: false</code></pre></li><li><p><strong>运行：给这个网络框架喂数据</strong></p><p>有了前面的框架思维，那么这个“运行”就很好理解了，我们将数据喂进框架，它就从静态的变成运行的了，能够得到我们想要的运行结果。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> PaddlePaddle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】8-Graph Neural Networks</title>
      <link href="/2020/11/26/si-tan-fu-cs224w-tu-ji-qi-xue-xi-8-graph-neural-networks/"/>
      <url>/2020/11/26/si-tan-fu-cs224w-tu-ji-qi-xue-xi-8-graph-neural-networks/</url>
      
        <content type="html"><![CDATA[<p><img src="https://pic1.zhimg.com/80/v2-338272ff7d5ed60dd67f5cb531d0afd0_720w.jpg"></p><h2 id="复习Node-Embeddings"><a href="#复习Node-Embeddings" class="headerlink" title="复习Node Embeddings"></a>复习Node Embeddings</h2><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p><font color="red">将图中的节点映射到d-维度嵌入空间，使得图中的<strong>相似的节点</strong>在d-维度嵌入空间中紧密地<strong>相邻</strong>。</font>即要使得$similarity(u,v) \approx z_u^{T}z_v$。</p><blockquote><p>这里的“相似的节点”并不是指在原空间相邻，相似（similarity）的定义有很多种，除了相邻，如果两个节点在图中承担了相同的structure role，它们也可以称作是相似的。</p></blockquote><p><img src="https://pic2.zhimg.com/80/v2-de441b95f0d25cfab8b4931fcca81365_720w.jpg"></p><p><img src="https://pic3.zhimg.com/80/v2-56fcfef9a32472ffbb01fa663bd356be_720w.jpg"></p><p>完成Node Embedding需要定义两个东西：</p><ol><li></li></ol><p><img src="%E3%80%90%E6%96%AF%E5%9D%A6%E7%A6%8Fcs224w-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%918-Graph-Neural-Networks/v2-814f105bd80f0c22b103bcbb7ecae18d_720w.jpg"></p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>从Node Embedding的目的的定义中，我们可以抽取出实现Node Embedding的两个重要组件：</p><ol><li><strong><code>Encoder</code>函数</strong></li><li><strong>input network上节点<code>similarity</code>的度量方式</strong></li></ol><p><img src="https://pic1.zhimg.com/80/v2-95c2ed92986bbeb1edd60c04f589d1c0_720w.jpg"></p><blockquote><p>这里的<code>dot product</code>$z_v^Tz_u$其实最后计算的是<strong>余弦距离</strong>，余弦距离是一种常见的相似度度量方式。</p></blockquote><h3 id="实现方式一：Shallow-Node-Embedding"><a href="#实现方式一：Shallow-Node-Embedding" class="headerlink" title="实现方式一：Shallow Node Embedding"></a>实现方式一：Shallow Node Embedding</h3><p>我们之前所提到的<code>DeepWalk</code>也好，<code>node2vec</code>也好，都属于浅层的encoders。它们的作用类似是建立了一张Hash Table，如下图：</p><p><img src="https://pic2.zhimg.com/80/v2-1c4527985f9cc47399bb1a0aa9abd2d5_720w.jpg" alt="img"></p><p>这些内容第七课实际上都已经介绍过了，所以就不赘述了。</p><p><img src="https://pic3.zhimg.com/80/v2-01b4bbb1874cbf8455652e7676be0eae_720w.jpg"></p><h4 id="局限性——即为什么要引入Graph-Neural-Network？👏"><a href="#局限性——即为什么要引入Graph-Neural-Network？👏" class="headerlink" title="局限性——即为什么要引入Graph Neural Network？👏"></a>局限性——即为什么要引入Graph Neural Network？👏</h4><p><img src="https://pic4.zhimg.com/80/v2-ab45604f5ad2f562177c9445ab9cf5bb_720w.jpg"></p><p>那么浅层的encoders有哪些缺陷呢？</p><ol><li><p><strong>太多参数</strong>：v个节点就有v个embedding，并且nodes之间不共享参数，每一个node都有其独立的embedding，那么如果embedding的维度是100，有100万个节点，我们就要训练1亿个参数。</p></li><li><p><strong>固有的局限性</strong>： 无法为训练期间未见的节点生成embedding，这就好比word2vec对于未见过的词无法进行embedding一样；woe对于没见过的类别无法编码一样；除此之外，当我们在一个网络上训练得到了各个节点的embedding，然后比如时间长了，网络里又增加了很多新的节点和边，那么我们又得重新训练一次，</p></li><li><p><strong>压根没用上node的feature啊</strong>：我们在cs224w第七课中，DeepWalk和node2vec压根没有用到每一个节点的属性特征，<strong>它们只是使用了图结构信息</strong>，比如某个user的节点，会有age、gender之类的特征，我们在embedding的时候也应该将这些考虑尽量，因为如果两个节点的属性完全相同很可能也代表了这两个节点非常接近，所以他们在embedding的空间里按理说也应该接近。</p></li></ol><p>除了要解决上面的这些问题之外，还有一个问题：</p><blockquote><p>Spectral Clustering可以看作是一种Embedding，它是人工数学推导计算出来的用于聚类的Embedding。（其数学推导是为了近似最大化conductance！）</p><p>那如果我们想，如果我要进行一个链路预测问题的话，难道我又要另外自己去公式推导找Embedding？如果我要进行一个分类问题的话，我又要找另外一个Embedding？难道我们就不能训练一个模型，这个模型能够根据任务的特征，自动得到Embedding？</p></blockquote><p>那么这里就是GNN大展身手的时候了！</p><h3 id="实现方式二：Deep-Node-Embedding"><a href="#实现方式二：Deep-Node-Embedding" class="headerlink" title="实现方式二：Deep Node Embedding"></a>实现方式二：Deep Node Embedding</h3><h4 id="Shallow-Node-Embedding-vs-Deep-Node-Embedding"><a href="#Shallow-Node-Embedding-vs-Deep-Node-Embedding" class="headerlink" title="Shallow Node Embedding vs. Deep Node Embedding"></a>Shallow Node Embedding vs. Deep Node Embedding</h4><p><img src="https://pic3.zhimg.com/80/v2-f07200b5fcf80779f48e2cb6ac79774a_720w.jpg"></p><p>所有的deep encoders都可以和上一节课我们介绍的<code>node similarity functions</code>结合起来，即：</p><ul><li><strong>Adjacency-based（即如果两个节点连通，那么它们相似）</strong></li><li><strong>Multi-hop similarity definitions</strong></li><li><strong>Random walk approaches</strong></li></ul><p><img src="https://pic2.zhimg.com/80/v2-806fe1a8de0fc3db8290573fb826b45d_720w.jpg"></p><p>上一节课我们这里的$ENC(V)$如下：</p><p><img src="https://pic1.zhimg.com/80/v2-3bb7c01b3052d94475f17a42f96a1690_720w.jpg"></p><blockquote><p>这个地方其实我一直在概念上比较困惑，因为总是不自觉地联想到DNN和CNN，在普通的深度学习的领域里，DNN是普通的全连接层，CNN则是带有卷积层和池化层的特殊结构，二者应该是两种不同的结构所以总是误以为有一种GNN的图网络结构和一种GCN的图网络结构。。。实际上这里的GNN指代的是nn的概念，即CNN属于nn的一种，GCN属于GNN的一种。另一方面，<strong>GCN有广义和狭义之分</strong>：</p><p><img src="https://pic1.zhimg.com/80/v2-d3f9db91a8e5c246cd4243bf03a9532c_720w.jpg"></p></blockquote><p>这节课谈到的GCN属于狭义的GCN，也就是上图最右边的那个。这也是2017年的论文《semi-supervised classification with GCN》里所谈到的结构，是目前我们经常谈论到的那种GCN。</p><h4 id="已有机器学习的局限性——研究GNN的原因"><a href="#已有机器学习的局限性——研究GNN的原因" class="headerlink" title="已有机器学习的局限性——研究GNN的原因"></a>已有机器学习的局限性——研究GNN的原因</h4><p><img src="https://pic3.zhimg.com/80/v2-e20062c927ba80aadd3c715b546b0bca_720w.jpg"></p><p><img src="https://pic3.zhimg.com/80/v2-27ded1135bc6632435e55ac7db325662_720w.jpg"></p><p>这些实际上都是综述里的内容，这里引出了为什么我们使用常规的NN结构无法处理图数据：</p><ol><li><p>图中的节点没有固定的顺序或参考系，而我们使用CNN或RNN所面对的数据都是fixed size，固定大小的，每一张图片的大小都处理成完全一样的，每一个序列都会通过补零然后mask从而达到同样size的效果；</p></li><li><p>网络常常是动态的并且具有多模态特征；</p></li></ol><h4 id="从计算机视觉到图神经网络"><a href="#从计算机视觉到图神经网络" class="headerlink" title="从计算机视觉到图神经网络"></a>从计算机视觉到图神经网络</h4><p><img src="https://pic3.zhimg.com/80/v2-51f7c2b0ad15f7852c71ad533e31c492_720w.jpg"></p><p>我们希望能够将CNN中<font color="red"><strong>卷积</strong></font>的思想推广到网格结构之外，同时充分利用节点原有的属性（例如文本或图像）。</p><blockquote><p>你甚至可以认为结点不同的feature作为convolution操作里面不同的channel。</p></blockquote><p><strong>（这里建议看下gnn的综述，里面详细介绍了图像和文本都属于图数据结构的一种）</strong></p><p>将图像转变为图结构进行思考：</p><p><img src="https://pic4.zhimg.com/80/v2-363f4b11b8581f8c6cf8fdcd304aaa0f_720w.jpg"></p><p>上图左边为一个简单的3X3的卷积核，其计算的本质就是对$W_i*h_i$进行求和。比如原图中的9个像素点$h_i$代表了9个输入，每个输入对应一个$W_i$，即连接权，然后进行加权求和再进入activation function。</p><p>我们前面提到过图像属于图数据的一种，因此这里也可以从图数据的角度来描述，如右图，因为$CNN$传播的时候所有节点都参与了加权求和，对应到图上就是中心节点再加一个自循环进来，这样实际上右图也一样可以表示为加权求和的形式。这样我们就把$CNN$的输入转化为用图来描述。</p><blockquote><p>但是，需要辨别的是：在CNN中，聚合的单位是特征；而在GNN中，聚合的单位是节点。特征和节点的粒度大小是不一样的！所以我认为只能说GNN是利用了CNN这种聚合的思想，而不是完全照搬CNN的模型~🤔</p></blockquote><p>这样我们就可以用常规$CNN$处理这种规整的图数据了，然而：</p><p><img src="https://pic3.zhimg.com/80/v2-2d0cdcb7b250a787f59b12729c29c5ce_720w.jpg"></p><p>现实中的网络复杂的多，比如上面这样形式的图，就没办法用CNN来处理的。。。</p><p>这种结构别说CNN，目前我们常见的模型都无法接受这种形式的数据作为输入，</p><p><font color="red"><strong>所以我们该怎么做才能像在图像上使用卷积层来slide（卷积层按照步长滑动）那样在graph的数据上slide呢?</strong></font></p><h4 id="一种简单的方法——没看懂"><a href="#一种简单的方法——没看懂" class="headerlink" title="一种简单的方法——没看懂"></a>一种简单的方法——没看懂</h4><p><img src="https://pic2.zhimg.com/80/v2-29de3154e5da90640ecbfd56c080a655_720w.jpg"></p><p>一个简单的思路，我们把图转化为其领接矩阵的形式，然后这个邻接矩阵和node的属性组成的特征矩阵合并（concat）在一起，如上图，这样就转化为一个规整的二维矩阵了，这样我们就可以用一个普通的nn结构或lr、gbdt之类的算法来处理了；</p><p>然而这种做法的坏处也很明显：</p><ol><li><p>如果node的数量非常多，那么最终的合并矩阵的维度会非常大，高维稀疏的情况下非常容易过拟合；</p></li><li><p>这种情况下训练出来的模型对于不同size的网络用不了。比如我们原来是100个node，扩展成100维加上原来计入50维的node feature一共150维，然后我们训练一个nn，这个nn只能接受150维的输入，然后新的网络有500个node，concat之后是550维，这样我们就面临了输入数据的异构问题了；</p></li><li><p>显然破坏了图的结构信息，比如A和E是二度关联的，但是转换成领结矩阵之后我们无法直接观察到这种关联；</p></li><li><p>这里提到了第四点，就是比如我们保持原始的图结构不变，然后变换了node的位置，但是领域信息是完全不变的（建议自己把node换一下然后看看每个节点的领域关系是否改变就知道了）但是其领接矩阵确发生了变化，显然我们希望这种变化对于模型来说没有影响。</p></li></ol><h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><p>GNN的兴起，是以GCN模型的提出为标志的，所以我们学习GCN能够让我们了解一些GNN的入门知识。</p><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p><img src="https://pic2.zhimg.com/80/v2-09145283e9e3b99a1a6a547bd66467fd_720w.jpg"></p><p>我们给定一个图结构：</p><ul><li>$V$是节点的集合；</li><li>$A$是图的邻接矩阵；</li><li>$X$是节点的属性矩阵组成的特征矩阵（维度有点意思~）</li></ul><p>不同网络有不同的<code>node features</code>，比如：</p><ul><li><p><strong>社交网络</strong>：用户信息与用户画像等；</p></li><li><p><strong>生物网络</strong>：基因表达谱、基因功能信息</p></li><li><p><strong>数字特征</strong>：指示向量（节点的独热编码）</p></li></ul><h3 id="GCN的idea⭐⭐⭐⭐⭐"><a href="#GCN的idea⭐⭐⭐⭐⭐" class="headerlink" title="GCN的idea⭐⭐⭐⭐⭐"></a>GCN的idea⭐⭐⭐⭐⭐</h3><p>GNN的思想：<font color="red"><strong>Generate Embedding by Aggregating Neighbors‘ Embedding Through Computation Graph</strong></font></p><p>顾名思义，我们需要解决两个问题：</p><ol><li><p>how to capture computation graph？</p></li><li><p>how to aggregate information？</p></li></ol><blockquote><p>很多基于GCN的优化也是从这两个方面出发的。</p></blockquote><h4 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a>Computation Graph</h4><p><img src="https://pic3.zhimg.com/80/v2-e170eb706dff414655c96d56536ca1aa_720w.jpg"></p><p>上图中的橙色部分就是经过对节点$i$的邻域采样后的得到的Computation Graph。</p><p>这里我们希望学习如何通过图的结构来<strong>传播信息（Message Passing）</strong>从而更好地计算节点的embedding，<font color="red"><strong>说白了就是不仅像node2vec那些算法那样考虑节点和节点之间的关联关系还要考虑节点的属性（特征）。</strong></font></p><p>上图的<code>k=1</code>表示一度关联，<code>k=2</code>表示二度关联 。比如node $i$，我们后面介绍的方法不仅能够捕获到node $i$ 所在图的结构信息，还能够捕获node $i$ 的邻节点的属性信息从而增强embedding的表达能力。</p><p><img src="https://pic4.zhimg.com/80/v2-e479954bdaf518814bb9a000f73a3e87_720w.jpg"></p><p><font color="red">通常来说这个<strong>聚合邻居的操作</strong>（也就是这个彩色的层数，如上图中有两层）是不需要叠很多层的，因为著名的6度理论告诉我们，走了6层邻居差不多就可以遍历到图中的每一个节点了。但是黑色盒子里面的DNN是可以叠很多层的！</font></p><p>这里就看的非常明白了，例如我们要计算$A$的embedding，我们希望能够collect来自$B$、$C$、$D$的信息，而$B$、$C$、$D$又分别collect来自其各自的领域的信息，这里可能有点懵逼，那么所有Nodes最原始的属性（即之前提到的user profiles，images之类的属性）到底是在哪里参与了这个网络结构，答案就是</p><p><img src="https://pic3.zhimg.com/80/v2-496d99e9b6f5b2a54d70a3f3cc892476_720w.png" alt="img"></p><p><strong>here，这里是layer-0，第0层，这里的输入都是节点的属性</strong>。这样我们就巧妙的把node A的邻节点的属性考虑进来了，同时node A所在的网络的结构也一并考虑进来了，因为不同的结构会使得最终embedding的结果不同，通过这种方式捕获的结构的信息。</p><blockquote><p>（这里学生问了一个我也想问的问题，如果说整个网络非常复杂，A的邻居节点B存在邻节点C，而邻节点C存在一个更远的邻节点D，邻节点D存在一个更更远的邻节点E，那么我们是不是也要统一考虑进来，这里教授的答案是“no need”。。。。额。。。）</p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-e5669b607992ae1439f2556130035c0b_720w.jpg"></p><ul><li><p>到这里还有一个问题，这些黑色的<code>box</code>到底是什么东西？</p><p>关于<code>box</code>，我们目前只知道有一点性质，它需要满足，就是这个<code>box</code>中的函数对于输入节点的顺序是完全不敏感的，比如第一个灰色<code>box</code>的输入是$A$和$C$节点，我们替换$A$和$C$的顺序之后，经过这个<code>box</code> function之后的结果应该是不变的。<strong>这个function应该是order invariant function（目的是为了解决上面讲的对顺序敏感的问题）</strong>，具体后面会描述清楚的。</p></li></ul><p><img src="https://pic1.zhimg.com/80/v2-a0127dd305b11831f4b00f3405b3f164_720w.jpg"></p><p>那么通过刚才的思路，我们为每一个节点都进行了上面所说的展开的方法进行展开得到了上图。可以发现，每个结点拥有不同的Computation Graph。</p><p>怎样初始化，以及怎样计算：</p><p><img src="https://pic1.zhimg.com/80/v2-3b5eeaedd1efa5c79e6720500a763ddc_720w.jpg" alt="img"></p><p>这里整个Deep Graph Encoder的结构就出来了，我们的输入是target node的二阶邻居节点，输出是target node的embedding。</p><p>需要注意，以节点$C$为例，layer-0节点$C$是$C$的原有的features，比如$C$是一个用户，那么就可能是用户的age、gender、nationality等，但是除了layer-0之外，其它的layer里，$C$都是该层对应的embedding。这是一个很奇怪很有意思的地方。</p><p>其实这里涉及到了两种思维方式——<strong>Top to Down</strong> 和 <strong>Down to Top</strong>（有点类似于递归的思想）：</p><ul><li><strong>Top to Down</strong>：是我们得到计算图的过程，类似于递归函数不断递归深入的过程。给定节点$u$，如果我们要聚合它的二阶以内邻居的信息，我们首先得到它的一阶邻居，然后通过一阶邻居的一阶邻居得到它的二阶邻居。过程类似于从左到右看上面的网络图。</li><li><strong>Down to Top</strong>：是我们聚合邻居信息的过程，类似于递归函数不断返回的过程。同样是上面那个例子，首先是二阶邻居将自己的信息聚合到对应的一阶邻居，然后一阶邻居再聚合到节点$u$。过程类似于从右到左看上面的网络图。这是我们的网络计算的顺序。</li></ul><p>可以思考，以上两个过程的确能够确保节点$u$能够聚合到二阶邻居以内的所有信息。</p><p>上图是一个二层的结构（这里的层数是<code>box</code>​代表了层，并不是指Deep Neural Network里面的层数），这意味着我们取的是node $A$的2 hops away的邻节点，这里又提到了为什么我们不继续”go deep“的问题，如果我们考虑太远的节点，会把太多的结构信息考虑进来，举一个极端的例子，比如我们go 20 steps deeper，可能会把整个网络结构的信息考虑进来。</p><blockquote><ul><li>（这里是我自己的思考，我们以word2vec为例，假设一个句子有10个单词，而我们的windows是10，这就意味着在这个句子中，每个单词的embedding表示最终都是由这个句子中所有的单词一同参与训练的，这会导致最终这个句子中所有的单词训练出来的embedding接近一致，这显然不是我们想要的结果。）</li><li>但是这并不意味着你不能做很深的Deep Neural Network！我们完全可以让$<code>box</code>$里面的DNN非常深~</li></ul></blockquote><p>另外，假设我们图中的F直接又接入了一个G，这个G仅仅和F连接而不和其它所有的已知节点有连接，那么我们就要在F的屁股后面接入一个<code>box</code>，这个<code>box</code>的输入是F的邻节点E、C、G。</p><p>btw，问了一下，之所以我们不go太过deeper，一方面引入太多的领域信息：</p><p><img src="https://pic2.zhimg.com/80/v2-c6fc85c57ef9bdc6258329f245f562d5_720w.jpg" alt="img"></p><p>90%的nodes会在8hops之内访问到，因此这意味着hops取得太大，会导致最终每个节点的embedding的计算过程中，输入的features几乎差不多，从而最终导致over-smooth的问题，即每个节点最终计算出来的embedding接近一致。</p><p>这类似于我们使用word2vec对一篇文章中的每个词进行向量化，windows取得太大使得整篇文章的词语都参与了每一个词的embedding训练——换句还说，每个词都在每个词的附近。。。最终结算的结果当然是所有的embedding都差不多了。。。</p><p><img src="https://pic1.zhimg.com/80/v2-f95db64e7eb618a8279543e08acdd4d8_720w.jpg" alt="img"></p><h4 id="Aggregation-Function"><a href="#Aggregation-Function" class="headerlink" title="Aggregation Function"></a>Aggregation Function</h4><p>现在一个重要的地方在于我们如何汇总各层的信息？做基本的思路如下：</p><p><img src="https://pic2.zhimg.com/80/v2-3d216fbdf25a72c8977ba58aee455909_720w.jpg" alt="img"></p><p>最简单的思路，我们把所有来自于邻域的信息进行平均然后输入一个$NN$结构即可；注意，所有的<code>box</code>都是这样的，箭头的地方先进行average，然后送到一个$NN$的结构里。也就是<code>box</code>前面的箭头是average，<code>box</code>里面是一个$NN$。</p><p>把整个过程用公式来表示就是：</p><p><img src="https://pic1.zhimg.com/80/v2-0379221b6a3be49372a232c88dc13a48_720w.jpg" alt="img"></p><p>这个过程用公式来表示如上图，这就是狭义的GCN的整个前向传播的过程了。这里，如果$W_k$全是0，则意味着我们根本不考虑邻节点的信息，这个时候这个GCN的结构就退化成了普通的MLP（DNN）。如果$W_k$非常非常大，$B_k$相对非常非常小，则相当于我们只考虑了邻节点信息而忽略了自身的信息。具体应该怎么去分$W_k$和$B_k$的大小，是由<font color="red"><strong>整个模型训练的过程中自己决定的</strong></font>。</p><h4 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h4><p><img src="https://pic4.zhimg.com/80/v2-ce4bcc92f33a8d65c8520dfeefd42043_720w.jpg" alt="img"></p><p>这里我们定义完了前向传播的过程，还缺一个损失函数。</p><p><img src="https://pic1.zhimg.com/80/v2-b7a2fa86319d8e28369c0c942822d9b4_720w.jpg"></p><ul><li>这里，我们把上面的公式表达成<strong>矩阵</strong>的形式，如上图，其中$H(l+1)$表示所有node的在$l+1$层的embedding表示，$H(l)$同理，$A$表示graph的领接矩阵，$D$表示graph的度矩阵，关于graph常用的矩阵的含义可见：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/weixin_43330946/article/details/102802768">GNN之常用矩阵汇总</a></li><li>这里的$W_k$和$B_k$可以用于衡量邻居和自己的作用的大小。</li></ul><h5 id="无监督方式"><a href="#无监督方式" class="headerlink" title="无监督方式"></a>无监督方式</h5><p><img src="https://pic2.zhimg.com/80/v2-5d14a6d2184257209e29649bcff39cb5_720w.jpg"></p><p>损失函数可以使用<code>random walks</code>定义的损失函数也可以用<code>graph factorization</code>或者<code>node proximity</code>的损失函数。比如第七课用过的：</p><p><img src="https://pic2.zhimg.com/80/v2-8031dd9bdddc204a162e6725af9e3e81_720w.jpg" alt="img"></p><p>这样损失函数也确定下来惹~，我们就在自动微分的框架下快乐的等待整个网络训练到收敛为止。</p><h5 id="有监督方式"><a href="#有监督方式" class="headerlink" title="有监督方式"></a>有监督方式</h5><p>除此之外，我们可以直接在最终的输出上接一个分类层，然后使用分类的损失函数比如<strong>交叉熵</strong>作为最终的损失函数。</p><p><img src="https://pic3.zhimg.com/80/v2-1b426366c3d3c3d2be95bfbecf592622_720w.jpg" alt="img"></p><p><img src="https://pic4.zhimg.com/80/v2-6af4e8a2bfcf2ffbbaa503aaad88db43_720w.jpg" alt="img"></p><p>这里公式写的非常明了了。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>下面进行了一个完整的总结：</p><p><img src="https://pic4.zhimg.com/80/v2-58331aa6bf9830a75b8769710281807b_720w.jpg" alt="img"></p><ol><li><p>我们定义一个neighborhood aggregation function，狭义的GCN使用的就是简单平均的策略；</p></li><li><p>我们定义一个funtion用于指导GCN进行反向传播更新权重参数W和B；</p></li></ol><p><img src="https://pic3.zhimg.com/80/v2-e4e178c1890d901beb297a3c31997cde_720w.jpg" alt="img"></p><ol start="3"><li><strong>一个节点对应一个模型输入的样本</strong>，我们在这些样本上进行训练；</li></ol><p><img src="https://pic4.zhimg.com/80/v2-f42e8fb1942e2decf7dfd7245c3dbee3_720w.jpg" alt="img"></p><p>假设我们的训练集是node $A$、node $B$和node $C$，那么我们直接可以把训练完的model用于预测未见过的node $B$、$E$、$F$，这里就比较重要了，首先我们训练完毕的模型是什么样子的？</p><p>实际上我们训练完毕的model就是**两个<code>box</code>**：</p><p><img src="https://pic2.zhimg.com/80/v2-d2dd20edbe84abce89ec038895850e95_720w.jpg" alt="img"></p><p>一个是浅灰色的<code>box</code>，一个是深灰色的<code>box</code>，这两个<code>box</code>都是$NN$的结构，具体是什么结构可以自己用普通的神经网络的方法来定义。另一个重要的地方在于，可以看到深灰色的<code>box</code>有3个，实际上只有一个，这3个浅灰色<code>box</code>进行了权值共享，是不是和CNN里的卷积核非常相似？实际上CNN里的卷积核的传播形式展开之后也可以表示成上面的形式的。</p><p>所以，对于new node，我们可以很轻易的使用已经训练完的两个<code>box</code>来进行预测得到最终的结果。</p><p>见下：</p><h5 id="inductive-capability"><a href="#inductive-capability" class="headerlink" title="inductive capability"></a>inductive capability</h5><p>归纳能力</p><p><img src="https://pic1.zhimg.com/80/v2-f9adcb6fe13068709d43f24e361aac64_720w.jpg" alt="img"></p><p>我们最终share的权重如上图，这里就是体现$GNN$权值共享的地方。可以看到，相对于node2vec和deepwalk这种算法，deep graph encoders可以对unseen的node进行embedding，而node2vec和deepwalk无法实现这样的功能，并且deep graph encoders还考虑了node的features，使得最终的embedding结果更加expressive。</p><p>不仅如此，即使是新的graph结果，我们也可以直接apply原来的model了！简直太牛逼了！！</p><p><img src="https://pic1.zhimg.com/80/v2-47740995df57d3572f1cb06446ded854_720w.jpg" alt="img"></p><blockquote><p>上面的例子很赞👍</p></blockquote><p><img src="https://pic3.zhimg.com/80/v2-85f70177567e2688f83d7ba120392f0e_720w.jpg"></p><p><strong>还有一个非常有用的地方就是，你可以只训练一个子图，就可以获取这个全局的信息。</strong>但是这个采样子图一定要保证良好的随机性！！！</p><p><img src="https://pic3.zhimg.com/80/v2-5f51ef654d2ad235c597bd903bfaa18e_720w.jpg" alt="img"></p><p>需要区别聚合信息的depth和深度学习的depth。你完全可以让你的黑盒子非常的deep，因为一个黑盒子就是一个deep neural network，这个可以联想的~</p><hr><p>这样我们就从graph embedding过渡到了GCN，这也是我认为最好理解的方式了，后面补充一下graphsage的内容，关于gat，我得去复习下attention的内容，后面佛系更新了。</p><p><img src="https://pic2.zhimg.com/80/v2-425604b98f62b7a519d886198daace35_720w.jpg" alt="img"></p><p>这里实际上我们就实现了在graph上slide，这两个<code>box</code>就是我们的“卷积核”，我们取2 hops away的节点进行展开，就类比于固定了“卷积核”的size，比如我们从D开始slide，“卷积核”的size（这里的卷积核取双引号因为是类比的关系，不是真正指cnn里的卷积核的概念），取其2hops away的节点展开成一个computation graph，得到了上面第四个图，然后我们slide到节点A，取其2hops away的邻节点，展开成一个computation graph图，得到上面第一个图。。。。。依次类推，可以看到，实际上类似于我们把每一个node当作cnn中卷积核的中心的像素点~。</p><hr><h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><h3 id="GraphSAGE的提出原因"><a href="#GraphSAGE的提出原因" class="headerlink" title="GraphSAGE的提出原因"></a>GraphSAGE的提出原因</h3><p>从GCN过渡到GraphSAGE的过程是非常自然，二者的不同之处仅仅是<code>box</code>处做了一些修改：</p><p><img src="https://pic2.zhimg.com/80/v2-252e5eb0dc002dd7956b77647c6202f5_720w.jpg"></p><p>这里提到了weighted average，实际上指的就是当图为带权图的时候，我们进行average之前，每个节对应的要乘上edge的权重再平均，这实际上也是很好理解的，比如我认识很多大佬，但是大佬们都不认识我，所以他们收入高跟我可能没什么关系，我们之间的edge的权重可能是0.001。。。</p><p><img src="https://pic4.zhimg.com/80/v2-9b591856199e4690cd3b3dca0f6af10f_720w.jpg"></p><p>比较一下：</p><p><img src="https://pic2.zhimg.com/80/v2-3f346fa4266b61f5ce70ace8790c8a85_720w.jpg" alt="img"></p><p>主要有两个不同：</p><ol><li>邻域信息和自身信息的结合从代数运算变成了concatenate（这样能够让节点自身的信息与邻域的信息区分开来）；</li><li>不使用简单的mean函数作为聚合函数，而是使用Generalized的聚合函数。这里，我们从广义的角度上来定义聚合函数，即我们在GCN中使用的是简单的邻居的聚合，而在<code>GraphSAGE</code>中，这个聚合函数的形式更加多样：</li></ol><h3 id="不同聚合函数的性质"><a href="#不同聚合函数的性质" class="headerlink" title="不同聚合函数的性质"></a>不同聚合函数的性质</h3><p><img src="https://pic4.zhimg.com/80/v2-b3dfc3976ef6bc0aa4de4a7a10fa53af_720w.jpg"></p><p>我们可以使用原来的mean的策略；也可以使用pooling的策略；甚至可以加一个LSTM进来，只不过因为LSTM是序列相关的，而我们的节点是无序的，所以处理方法就是对节点进行几次shuffle，每次shuffle的结构都当作一个有order的序列然后传入LSTM进行训练；</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m61svcfj22bc1qindx.jpg"><h3 id="实现聚合操作的矩阵运算"><a href="#实现聚合操作的矩阵运算" class="headerlink" title="实现聚合操作的矩阵运算"></a>实现聚合操作的矩阵运算</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m61y6q2j22bc1qitk4.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m64vwz2j22bc1qiaq0.jpg"><h2 id="Graph-Attention-Networks（GAT）"><a href="#Graph-Attention-Networks（GAT）" class="headerlink" title="Graph Attention Networks（GAT）"></a>Graph Attention Networks（GAT）</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m62xxe5j22bc1qiwt1.jpg"><h3 id="GAT提出的原因"><a href="#GAT提出的原因" class="headerlink" title="GAT提出的原因"></a>GAT提出的原因</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m62z6mbj22bc1qiwrz.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m63jtawj22bc1qianh.jpg"><h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>byproduct：副产品</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m65625dj22bc1qin9n.jpg"><p>注意力计算公式：<br>$$<br>e_{uv}=a(W_kh_u^{k-1},W_kh_v^{k-1})<br>$$</p><p>其中：</p><ul><li><p>$W_k$：transformation matrix</p></li><li><p>$a$：a function</p></li></ul><blockquote><p>老师乘机解释了一波为啥叫softmax叫softmax函数，赞。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m64m4yvj22bc1qi7h3.jpg"><blockquote><p>agnostic：不可知论的。注意力函数可以有多种选择，而且它也可以拥有函数。</p></blockquote><h3 id="Properties-of-Attention-Mechanism"><a href="#Properties-of-Attention-Mechanism" class="headerlink" title="Properties of Attention Mechanism"></a>Properties of Attention Mechanism</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m65huuqj22bc1qi4cl.jpg"><p>我们能够同样用到没有见过的节点上面。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6m6plkj22bc1qi1kz.jpg"><h2 id="Practical-tips-and-demos"><a href="#Practical-tips-and-demos" class="headerlink" title="Practical tips and demos"></a>Practical tips and demos</h2><h3 id="Application：Pinterest"><a href="#Application：Pinterest" class="headerlink" title="Application：Pinterest"></a>Application：Pinterest</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6yoyawj22bc1qiqv8.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6pfuu9j22bc1qi1kz.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m65cpqcj22bc1qijz7.jpg"><p>在Pinterest上面，有两类节点：</p><ul><li>一类节点叫做<strong>Pin</strong>（类似于收藏的物品，Pin的特征包括文字，图片，链接）；</li><li>一类节点叫做<strong>Board</strong>，类似于收藏夹。</li></ul><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m68ddfhj22bc1qib29.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m67wc64j22bc1qiqu8.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6b8popj22bc1qib29.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6owoy6j22bc1qi7wj.jpg"><h4 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m684065j22bc1qi7f0.jpg"><h5 id="动态图卷积"><a href="#动态图卷积" class="headerlink" title="动态图卷积"></a>动态图卷积</h5><p>不进行全图的节点训练</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m67weqbj22bc1qiav9.jpg"><h5 id="基于random-walk进行邻居选择"><a href="#基于random-walk进行邻居选择" class="headerlink" title="基于random walk进行邻居选择"></a>基于random walk进行邻居选择</h5><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m68eu48j22bc1qi16e.jpg"><h5 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h5><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6blhmxj22bc1qi4d4.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6wbw6ej22bc1qi4qs.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6qdv1mj22bc1qiqv6.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6t71e6j22bc1qi1kz.jpg"><h3 id="Jure大佬给大家的实践经验⭐⭐⭐⭐⭐"><a href="#Jure大佬给大家的实践经验⭐⭐⭐⭐⭐" class="headerlink" title="Jure大佬给大家的实践经验⭐⭐⭐⭐⭐"></a>Jure大佬给大家的实践经验⭐⭐⭐⭐⭐</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m69uxo3j22bc1qin6w.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6aa4f7j22bc1qitjn.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6i5wf4j22bc1qie81.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl2m6ieyl2j22bc1qix6p.jpg"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/112087612">知乎——cs224w 8.1-Graph Neural Network</a></li><li><a href="https://link.zhihu.com/?target=https://blog.csdn.net/weixin_43330946/article/details/102802768">CSDN——GNN之常用矩阵汇总</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】7-Graph-Representation-Learning</title>
      <link href="/2020/11/25/si-tan-fu-cs224w-tu-ji-qi-xue-xi-7-graph-representation-learning/"/>
      <url>/2020/11/25/si-tan-fu-cs224w-tu-ji-qi-xue-xi-7-graph-representation-learning/</url>
      
        <content type="html"><![CDATA[<p>无监督学习方法——Node Embedding</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmh7g5vj22bc1qi1kx.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmhl1exj22bc1qiqcd.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmg63e3j22bc1qiqam.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmh3erpj22bc1qigw2.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmhermbj22bc1qi7ee.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmj5vvvj22bc1qie7d.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmhzypzj22bc1qinnc.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmjkbxbj22bc1qidxi.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmk9tdfj22bc1qiwsa.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmiss44j22bc1qinaa.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmjy6axj22bc1qin1v.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmjvu5dj22bc1qi4by.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmlb1c4j22bc1qi7hh.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmkdnv1j22bc1qi7eu.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmlo1k7j22bc1qiwq4.jpg"><table><thead><tr><th></th><th>原始网络</th><th>嵌入空间</th></tr></thead><tbody><tr><td>度量标准</td><td>$similarity(u,v)$</td><td>$z_{v}^{T}z_{u}$</td></tr></tbody></table><h3 id="Embedding-lookup"><a href="#Embedding-lookup" class="headerlink" title="Embedding-lookup"></a>Embedding-lookup</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmlpsq7j22bc1qin6j.jpg"><blockquote><p>其实就是一个哈希表，key为节点，value为节点的embedding。</p><p>今天的课程都是这种look-up性质的Embedding，它是静态的，在之后的downstream任务中是不会改变的。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmlj810j22bc1qin5y.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmlobwvj22bc1qi46a.jpg"><h3 id="Step-1：定义Similarity"><a href="#Step-1：定义Similarity" class="headerlink" title="Step 1：定义Similarity"></a>Step 1：定义Similarity</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmldngbj22bc1qiwo5.jpg"><h2 id="Random-Walk-Approaches-to-Node-Embedding"><a href="#Random-Walk-Approaches-to-Node-Embedding" class="headerlink" title="Random Walk Approaches to Node Embedding"></a>Random Walk Approaches to Node Embedding</h2><blockquote><p>本小节讲解的是下面两篇论文：</p><ul><li>Perozzi et al. 2014. <a href="https://arxiv.org/pdf/1403.6652.pdf">DeepWalk: Online Learning of Social Representations</a>. KDD.</li><li>Grover et al. 2016. <a href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf">node2vec: Scalable Feature Learning for Networks.</a> KDD.</li></ul></blockquote><h3 id="Random-Walk"><a href="#Random-Walk" class="headerlink" title="Random Walk"></a>Random Walk</h3><ul><li>random walk是一个名词，它是图上的一个<strong>节点序列</strong>；</li><li>random walk的长度是一个自定义的参数，需要根据不同的应用场景进行选择；</li></ul><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmnwuvlj22bc1qin6e.jpg"><p>你可能会想，我明明在学习Node Embedding，为什么要给我讲Random Walk呢？难道Random Walk可以帮助我们来设计学习Node Embedding的算法？</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmnerapj22bc1qijzf.jpg"><p>修改random walk的类型，那么我们就能定义不同的相似性，比如neighborhood，structural role等等。</p><p>基于random-walk的embedding的思想是：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmnf06fj22bc1qih2a.jpg"><blockquote><ul><li><p>为什么固定为dot product为$cos(\theta)$呢？</p><p>因为概率的大小是介于[0,1]之间的。</p></li></ul></blockquote><h3 id="选择random-walks作为similarity的原因"><a href="#选择random-walks作为similarity的原因" class="headerlink" title="选择random walks作为similarity的原因"></a>选择random walks作为similarity的原因</h3><ol><li><p><strong>表达性</strong>：采用不同的random walk机制，能够获取到局部和全局的信息。</p></li><li><p><strong>高效性</strong>：Node Embedding算法要优化是$z_{v}^{T}z_{u}$，如果考虑所有的$(src，dst)$对，那么计算量将会是巨大的。在random walk中，我们只考虑出现在同一random walk中的$(v,u)$对，这无疑减少了计算量。<strong>（Scalable）</strong></p><blockquote><p>这一点在图变得越来越大的时候，尤其的重要！</p></blockquote></li></ol><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmn97e6j22bc1qi7ed.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmnpyiij22bc1qi49g.jpg"><p>对于基于Random Walk的Node Embedding算法来说，random walk的采集策略是非常重要的，可以说绝大部分的算法都是在对这个策略进行创新。不同的策略将会产生不同的$N_{R}(u)$。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmo8cwhj22bc1qidr2.jpg"><blockquote><ul><li>这个式子有点极大似然估计的味道；</li><li>==<strong>为什么要使用Log-likelihood作为目标函数？</strong>==<ul><li>对于概率$P(N_R(u)|z_u)$，我们知道它是一个介于[0,1]之间的数字，是非常小的。老师说小的数字将会在训练过程中引发numerical instability；</li><li>而且，使用它并不会改变我们的直观感受。</li></ul></li></ul></blockquote><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmobn6gj22bc1qi167.jpg"><p>$P(N_R(u)|z_u)$依然很抽象，我们没有定义它的计算方式，接下来我们定义一下：</p><p>同时，我们书写出我们的loss function：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmp9gdtj22bc1qik3s.jpg"><p>需要注意的是这里计算了$V$中的所有顶点$v$！！！</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmpc3n3j22bc1qidsa.jpg"><p>我们的Loss Function的时间复杂度在$O(|V^2|)$，这是非常大的。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmpvuikj22bc1qi48j.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmpmg6zj22bc1qi133.jpg"><p>需要解决这些问题，我们就需要来拆解$O(|V^2|)$，首先对于第一个$V$，我们无法减小，因为我们一定需要对于每一个节点都要求和的。所以我们要从第二个$V$减小，或者说是approximate it。</p><p>解决的办法就是Negative Sampling——</p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><h5 id="noise-contrastive-estimation"><a href="#noise-contrastive-estimation" class="headerlink" title="noise contrastive estimation"></a>noise contrastive estimation</h5><p>噪声对比评估</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmqdrzuj22bc1qi7i2.jpg"><ul><li>quadratic</li><li>linear</li></ul><p>关于负采样中需要采集的样本数$k$，它需要正比于节点的度。（没听懂）</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmpikm8j22bc1qial6.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmqmom9j22bc1qianj.jpg"><h2 id="node2vec"><a href="#node2vec" class="headerlink" title="node2vec"></a>node2vec</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmquhruj22bc1qigy1.jpg"><p><code>DeepWalk</code>的similarity的定义方式太狭隘了，它只是考虑了连接。但是如果我们想要抽取structure role这样的相似性，那么它就显然不适用了，因此需要修改。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmr5mdbj22bc1qiqfb.jpg"><p>node2vec实现了local和global的一个平衡。</p><p>DeepWalk做的工作一直是local，现在我们考虑一下global的。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmsortsj22bc1qi18d.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmsutj2j22bc1qi19q.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmt1f32j22bc1qigyy.jpg"><p>interpolating：插值</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmsjr0ej22bc1qiqck.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmswkj3j22bc1qi4a1.jpg"><p>为什么称为是2nd-order，因为我们保留了prev_node。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmswi8vj22bc1qitkf.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmthi3oj22bc1qiqg4.jpg"><p>p,q是超参数，需要进行学习。将BFS的概率设置为1，是为了减少参数的数量。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmu7fqlj22bc1qiqc2.jpg"><h2 id="How-to-Use-Embeddings？"><a href="#How-to-Use-Embeddings？" class="headerlink" title="How to Use Embeddings？"></a>How to Use Embeddings？</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmucuotj22bc1qik30.jpg"><p>既然要结合$z_i$和$z_j$就需要结合函数，有以下几种：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmu4qobj22bc1qi474.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmv1x2tj22bc1qiajw.jpg"><p>根据不同的任务使用不同的embedding方法</p><h2 id="Translating-Embeddings-for-Modeling-Multi-relational-Data"><a href="#Translating-Embeddings-for-Modeling-Multi-relational-Data" class="headerlink" title="Translating Embeddings for Modeling Multi-relational Data"></a>Translating Embeddings for Modeling Multi-relational Data</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmvp7b6j22bc1qi4ja.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmxtheij22bc1qinhs.jpg"><p>在知识图谱上我们经常做一个KG Completion的任务</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmx3zi1j22bc1qiqls.jpg"><p>还要知道link的type是什么？</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmvda4bj22bc1qidn1.jpg"><p>两个蓝色节点应该越来越近，两个蓝色节点都有the genre edge指向fantasy，那么这两个蓝色节点应该越来越近才是。</p><p>translate：平移</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmygokij22bc1qi4el.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmy8sl7j22bc1qi4qp.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmx4vn7j22bc1qinb0.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmxosz0j22bc1qi14c.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmy2jt7j22bc1qiqaz.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmye685j21mb17qwn9.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mn0evf9j22bc1qi4o0.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mn135q0j22bc1qiayd.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mn1iyzaj22bc1qiqeg.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mn1ktd3j22bc1qitr6.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mn1bjlej22bc1qitoj.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mmzu1spj22bc1qi4bc.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl1mn15l6uj22bc1qin6d.jpg">]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】7,8,9-Graph Representation Learning</title>
      <link href="/2020/11/25/si-tan-fu-cs224w-tu-ji-qi-xue-xi-7-8-9-graph-representation-learning/"/>
      <url>/2020/11/25/si-tan-fu-cs224w-tu-ji-qi-xue-xi-7-8-9-graph-representation-learning/</url>
      
        <content type="html"><![CDATA[<p>在前面几周的学习中，我们学习了motifs，graphlets等等来表示一个图，本质上，它们就是图的一种特征表示（representation），接下来我们介绍自动的方法：</p><p>第七八两节介绍的都是关于<strong>图表示学习</strong>的，所谓万物皆可embedding嘛~</p><p>第九节介绍了一些代码相关，这部分还是很建议认真看看代码找几个数据集实验一下的~</p><p>（其实介绍表示学习算法的博客已经太多太多了，但是强迫症为了笔记完整一些，还是写一写==）【<a href="https://link.zhihu.com/?target=http://web.stanford.edu/class/cs224w/">ppt传送</a>】【<a href="https://link.zhihu.com/?target=https://www.bilibili.com/video/BV1Vg4y1z7Nf?p=11">视频传送</a>】</p><p>介绍分成了三个部分：</p><ul><li><strong>图表示学习思想</strong></li><li><strong>基于随机游走的表示学习（包括了DeepWalk, Node2vec, LINE)</strong></li><li><strong>基于深度学习的表示学习（包括了SDNE, GCN, GraphSAGE, GAT)</strong></li></ul><h2 id="图表示学习思想"><a href="#图表示学习思想" class="headerlink" title="图表示学习思想"></a><strong>图表示学习思想</strong></h2><p>Embedding一直是学术界研究的热点，简单说就是利用一个低维稠密向量来对某一Object进行表示，然而在图领域，网络通常任意大小，有复杂的拓扑结构，且经常动态变化，它不像图像一样有规则的网状结构，也不像文本一样具有序列结构，而且每个节点本身也含有大量特征，所以传统的embedding方法在图数据种并不实用。</p><blockquote><p>lattice：格子</p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-874ed5db68aefc8c5d8fc1457f4a8f5f_720w.jpg" alt="img"></p><p>图网络问题的核心思想如下图所示，可以理解为首先把每一个节点映射为低维向量表示，并且需要保证可以重新利用某种解码方式，尽可能的还原高维空间种的图信息，比如说邻居节点之间的依赖关系，保证原有空间中节点的相似度等等。这样就可以利用到学习到的embedding用于下游任务的学习，比如说异常检测、节点分类、关系预测、社区划分、连接预测等等。</p><p><img src="https://pic1.zhimg.com/80/v2-8b5e4668792d64ec0041356ecefc8904_720w.jpg" alt="img"></p><p>图表示学习大致可以分成两类：基于随机游走 &amp; 基于深度学习</p><h2 id="基于随机游走的表示学习"><a href="#基于随机游走的表示学习" class="headerlink" title="基于随机游走的表示学习"></a>基于随机游走的表示学习</h2><p>算法概览</p><ul><li><a href="https://link.zhihu.com/?target=http://www.perozzi.net/publications/14_kdd_deepwalk.pdf">DeepWalk：Online Learning of Social Representations</a></li><li><a href="https://link.zhihu.com/?target=https://www.kdd.org/kdd2016/papers/files/rfp0218-groverA.pdf">node2vec：Scalable Feature Learning for Networks</a></li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1503.03578.pdf">Line：Large-scale Information Network Embedding</a></li><li>代码：<a href="https://link.zhihu.com/?target=https://github.com/shenweichen/GraphEmbedding">https://github.com/shenweichen/GraphEmbedding</a>（大佬写的代码，还是很建议认真研究研究代码的~）</li></ul><p><strong>1. DeepWalk</strong></p><p>DeepWalk在某种程度上是序列学习embedding在图网络中的一种迁移，它借鉴了nlp中序列的思想，可以把图中的每个节点类比为一个词语，利用随机游走出来的序列类比为一个句子，根据随机游走形成的句子序列来构建词向量模型Word2vec, 并且利用Skip-gram模型来学习每个节点的向量表示。下图为DeepWalk模型结构，分成了三个部分：随机游走采样序列；Skip-gram模型训练；输出层利用Softmax优化，最终得到每个节点的向量表示。</p><p><img src="https://pic2.zhimg.com/80/v2-d3a860ae063681ad5fed801e98585531_720w.jpg" alt="img"></p><ul><li>Step1：随机游走采样序列</li></ul><p>首先定义采样次数y，每次采样的长度t，在图 <img src="https://www.zhihu.com/equation?tex=G(V,E)" alt="[公式]"> ，首先随机采样节点 <img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]"> ,然后在节点 <img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]"> 的邻居中随机采样节点 <img src="https://www.zhihu.com/equation?tex=v_j" alt="[公式]"> ,直至采样t次，生成一个采样序列，如上图所示，从节点 <img src="https://www.zhihu.com/equation?tex=v_2" alt="[公式]"> 开始，采样长度为5，生成的第一个采样序列为： <img src="https://www.zhihu.com/equation?tex=v_2+%5Crightarrow+v_5+%5Crightarrow+v_%7B11%7D+%5Crightarrow+v_%7B12%7D+%5Crightarrow+v_%7B15%7D" alt="[公式]"> ，如此遍历，直至采样y次，得到了最终的采样结果。为什么要采样呢？这样利用采样后的点训练，无需关注网络中所有节点，只需要关注采样得到的序列的共现组合即可。</p><ul><li>Step2：Skip-gram</li></ul><p>Skip-gram 为三层的神经网络，分为输入层，映射层，输出层。核心思想是在一个序列中，利用中间节点，去预测周围的节点，对应上图的采样序列 <img src="https://www.zhihu.com/equation?tex=v_2+%5Crightarrow+v_5+%5Crightarrow+v_%7B11%7D+%5Crightarrow+v_%7B12%7D+%5Crightarrow+v_%7B15%7D" alt="[公式]"> ，输入为节点 <img src="https://www.zhihu.com/equation?tex=v_%7B11%7D" alt="[公式]"> ，预测周围四个节点 <img src="https://www.zhihu.com/equation?tex=v_2+,v_5+,v_%7B12%7D,v_%7B15%7D" alt="[公式]"> 。Skip-gram 模型中输入为单词的 one-hot encoding，首先通过输入层到映射层的权重矩阵 <img src="https://www.zhihu.com/equation?tex=W_1" alt="[公式]"> ，得到中间层，然后通过映射层到输出层的权重矩阵 <img src="https://www.zhihu.com/equation?tex=W_2" alt="[公式]"> ，得到输出，输出层可用 softmax 计算，根据输出向量与真实的 one-hot 向量进行比较，根据损失函数利用反向传播算法对权重矩阵进行更新，最终的节点向量为模型的中间产物，即映射层，即当模型训练结束后，利用节点 one-hot 向量与权重矩阵 <img src="https://www.zhihu.com/equation?tex=W_1" alt="[公式]"> 相乘即可得到节点向量。</p><p>在参数求解中，上述模型等价于求解在给定节点 <img src="https://www.zhihu.com/equation?tex=v%5E%7B(t)%7D" alt="[公式]"> 的条件下，节点 <img src="https://www.zhihu.com/equation?tex=v%5E%7B(t+j)%7D" alt="[公式]"> 分布的联合最大概率，即：</p><p><img src="https://www.zhihu.com/equation?tex=max%5Cprod_%7Bt=1%7D%5ET%5Cprod_%7B-m+%5Cleq+j+%3Cm,+j%5Cneq+0%7Dp(v%5E%7B(t+j)%7D%7Cv%5E%7B(t)%7D)" alt="[公式]"> ，</p><p>等价于求解：</p><p><img src="https://www.zhihu.com/equation?tex=min-%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bt=1%7D%5ET%5Csum_%7B-m%5Cleq+j+%3C+m,+j%5Cneq+0%7Dlog+p(v%5E%7B(t+j)%7D%7Cv%5E%7B(t)%7D)" alt="[公式]"></p><p>利用softmax函数有：</p><p><img src="https://www.zhihu.com/equation?tex=p(v_o%7Cv_c)=%5Cfrac%7Bexp(u_o%5ETw_c)%7D%7B%5Csum_%7Bi%5Cin+V%7Dexp(u_i%5ETw_c)%7D" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=v_c" alt="[公式]"> 为中心节点， <img src="https://www.zhihu.com/equation?tex=v_o" alt="[公式]"> 为背景节点， <img src="https://www.zhihu.com/equation?tex=w_c" alt="[公式]"> 为中心节点向量， <img src="https://www.zhihu.com/equation?tex=u_o" alt="[公式]"> 为背景节点向量，取对数后求导可得：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+logp(v_o%7Cv_c)%7D%7B%5Cpartial+w_c%7D=u_o-%5Csum_%7Bj%5Cin+V%7D%5Cfrac%7Bexp(u_j%5ETw_c)%7D%7B%5Csum_%7Bi%5Cin+V%7Dexp(u_i%5ETw_c)%7Du_j=u_o-%5Csum_%7Bj%5Cin+V%7Dp(v_j%7Cv_c)u_j" alt="[公式]"></p><p>则权重更新为：</p><p><img src="https://www.zhihu.com/equation?tex=w_c=w_c-%5Cfrac%7B%5Cpartial+log+p(v_o%7Cv_c)%7D%7B%5Cpartial+w_c%7D" alt="[公式]"></p><p>通过上述步骤，即可对Skip-gram模型中各个参数更新，进而得到每个节点的向量。</p><ul><li>Step3：层次Softmax</li></ul><p>上述对Skip-gram模型的参数求解的过程中，由于softmax算法在每一次参数计算与更新的过程中都引入了全部的节点，复杂度较高，为了提高算法效率，降低复杂度，就有了层次Softmax算法。</p><p>层次Softmax(Hierarchical Softmax)利用二叉树的思想，使复杂度从 <img src="https://www.zhihu.com/equation?tex=O(%7CV%7C)" alt="[公式]"> 降到 <img src="https://www.zhihu.com/equation?tex=O(%7Clog_2V%7C)" alt="[公式]"> 。下图为层次Softmax的结构示意图，在输出层中引入二叉树，其中树中每一个节点代表图中一个节点，每个节点的左右子树分别为1-0，相当于二分类，则每个节点都可以通过二叉树的路径被0-1唯一编码。</p><p><img src="https://pic1.zhimg.com/80/v2-aa0a111f86425eb88a95e368fef62130_720w.jpg" alt="img"></p><p>当给定背景节点来计算目标节点的条件概率<img src="https://www.zhihu.com/equation?tex=p(v_o%7Cv_c)" alt="[公式]">为：</p><p><img src="https://www.zhihu.com/equation?tex=p(v_o%7Cv_c)=%5Cprod_%7Bj=2%7D%5E%7Bl%5Ew%7Dp(d_j%5Ew%7Cx_w,+%5Ctheta_%7Bj-1%7D%5Ew)=%5Cprod_%7Bj=2%7D%5E%7Bl%5Ew%7D%5B%5Csigma(x_w%5ET%5Ctheta_%7Bj-1%7D%5Ew)%5D%5E%7B1-d_j%5Ew%7D%5B1-%5Csigma(x_w%5ET%5Ctheta_%7Bj-1%7D%5Ew)%5D%5E%7Bd_j%5Ew%7D" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=l%5Ew" alt="[公式]"> 为从根节点到目标节点中路径经过的节点个数； <img src="https://www.zhihu.com/equation?tex=d_j%5Ew%5Cin%5C%7B0,1%5C%7D" alt="[公式]"> 为该节点对应的0-1编码，左子树为1，对应负类，右子树为0，对应正类； <img src="https://www.zhihu.com/equation?tex=%5Ctheta_j%5Ew" alt="[公式]"> 为路径中每个非叶子结点对应的向量；式中每一项为logistics回归， <img src="https://www.zhihu.com/equation?tex=%5Csigma(x_w%5ET%5Ctheta)=%5Cfrac%7B1%7D%7B1+exp(-x_w%5ET%5Ctheta)%7D" alt="[公式]"> 可看作该节点被判定为正类的概率，则 <img src="https://www.zhihu.com/equation?tex=1-%5Csigma(x_w%5ET%5Ctheta)" alt="[公式]"> 为该点被判定为负类的概率；可以通过把上式代入最大似然函数求解参数。</p><p>通过上述三个步骤，就可以得到每个节点的向量表示，Deep Walk完整流程如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-d9e2d2f0e11e731d2b171a1f260e8a48_720w.jpg" alt="img"></p><p><strong>2. Node2vec</strong></p><p>Node2vec算法结合了深度优先搜索与广度优先搜索，利用有偏的随机游走，更好的挖掘邻居节点的特异性。DeepWalk是使邻居节点尽可能相似，但是如果两个点没有邻居，而且没有共同邻居时，也可能存在相似性，比如两个完全生活在不同时区的人，虽然互相不认识，但是也可能有相似的兴趣爱好，相似的人生轨迹，因此不仅要学习网络中位于同一社区的邻居信息，也要学习每个节点的特异性社会角色。</p><p>Node2vec与DeepWalk极为相似，关键在于随机游走的策略不同，Node2vec中结合了深度优先搜索与广度优先搜索。通过广度优先搜索，可以保持节点的结构相似性，即学习每个节点结构上扮演的角色，比如说，对于一架桥，可以直接通过它直接相连的节点识别出来；而通过深度优先搜索，可以捕捉节点的同质性，即节点之间的依赖关系，反应微观邻居的信息，随着搜索的加深，也可以捕捉更复杂的节点依赖关系。因此可以结合深度优先搜索与广度优先搜索，利用有偏的随机游走策略进行采样。</p><p>给定初始节点<img src="https://www.zhihu.com/equation?tex=c_0=u" alt="[公式]"> ，模拟长度为l的随机游走，其中 <img src="https://www.zhihu.com/equation?tex=c_i" alt="[公式]"> 为游走中的第i个节点，则节点 <img src="https://www.zhihu.com/equation?tex=c_i" alt="[公式]"> 可由以下分布得到：</p><p><img src="https://www.zhihu.com/equation?tex=p(c_i=x%7Cc_%7Bi-1%7D=v)=%5Cbegin%7Bcases%7D+%5Cfrac%7B%5Cpi_%7Bvx%7D%7D%7BZ%7D,+&amp;if(v,x)%5Cin+E%5C%5C+0,+&amp;otherwise+%5Cend%7Bcases%7D" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cpi_%7Bvx%7D" alt="[公式]"> 是节点v和节点x的转移概率，Z是归一化常数。</p><p>为了捕捉游走的回溯性与搜索方向，定义带有参数p和参数q的随机游走，如下图所示：</p><p><img src="https://pic2.zhimg.com/80/v2-3bf4db7f2ebf9eaeb560898cb049ac55_720w.jpg" alt="img"></p><p>假设一个随机游走刚好经过边 <img src="https://www.zhihu.com/equation?tex=(t,v)" alt="[公式]"> , 当前在节点v，接下来需要决定下一步，定义转移概率 <img src="https://www.zhihu.com/equation?tex=%5Cpi_%7Bvx%7D=%5Calpha_%7Bpq%7D(t,x)w_%7Bvx%7D" alt="[公式]"> 其中</p><p><img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bpq%7D(t,x)=%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7Bp%7D,+&amp;if%5C+d_%7Btx%7D=0;%5C%5C+1,+&amp;+if+%5C+d_%7Btx%7D=1;%5C%5C+%5Cfrac%7B1%7D%7Bq%7D,+&amp;+if+%5C+d_%7Btx%7D=2+%5Cend%7Bcases%7D" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=d_%7Btx%7D" alt="[公式]"> 是节点t和x的最短距离。在上图中，当前节点v在下一次游走时，以概率 <img src="https://www.zhihu.com/equation?tex=w_%7Bvx_1%7D" alt="[公式]"> 到达节点 <img src="https://www.zhihu.com/equation?tex=x_1" alt="[公式]"> , 以概率 <img src="https://www.zhihu.com/equation?tex=w_%7Bvx_2%7D/q" alt="[公式]"> 到达节点 <img src="https://www.zhihu.com/equation?tex=x_2" alt="[公式]"> , 以概率 <img src="https://www.zhihu.com/equation?tex=w_%7Bvt%7D/p" alt="[公式]"> 返回节点t。可以理解为参数p和q控制搜索的回溯性与搜索方向。参数p控制节点回访的概率，当 <img src="https://www.zhihu.com/equation?tex=p%3E1" alt="[公式]"> 时，访问之前的节点的概率就会变低，反之，当 <img src="https://www.zhihu.com/equation?tex=p%3C1" alt="[公式]"> 时，更可能回溯之前访问过的节点；参数q控制搜索方向，当 <img src="https://www.zhihu.com/equation?tex=q%3E1" alt="[公式]"> 时，倾向于游走距离节点t较近的节点，即偏向于广度优先搜索；当 <img src="https://www.zhihu.com/equation?tex=q%3C1" alt="[公式]"> 时，倾向于访问距离t远的节点，即倾向于深度优先搜索。</p><p>通过上述有偏的随机游走即可得到采样序列，接下来可通过与DeepWalk类似的方法利用Skip-gram进行训练，从而得到每个节点的向量表示。Node2vec的完整流程如下：</p><p><img src="https://pic3.zhimg.com/80/v2-12a90cb3170962dcb43b18b71836de26_720w.jpg" alt="img"></p><p><strong>3. LINE</strong></p><p>LINE其实不能算作随机游走方法，但是也经常拿来和DeepWalk，Node2vec比较。LINE的核心思想是基于近邻相似，利用一阶和二阶的相似信息和编码后向量的相似性进行比较建立损失函数，从而得到每个节点的向量表示。模型中首先定义了一阶与二阶相似性。一阶相似性是两个邻居节点的局部相似性，在一定程度上可以衡量图的局部关系。一阶相似在真实网络中很常见，比如好朋友之间有相近的兴趣爱好，引用同一篇论文的两篇文章话题相似等。然而在实际生活中，仍然有大部分不存在一阶相似性的用户，但其行为是相似的，比如两个完全不认识的人，可能因为有共同好友，而有相似的兴趣爱好，所以一阶相似度不足以表示网络结构，故引入二阶相似性，用来衡量两个节点邻居的相似性，并且认为有较多共同邻居的节点是相似的。在下图中，节点6，7有边直接相连，有较高的一阶相似度，而节点5，6虽然不直接相连，但两个节点有共同的邻居，故两节点有较高的二阶相似度。LINE分别对一阶、二阶相似度进行学习，最终把二者向量进行拼接，得到最终的节点表示。</p><p><img src="https://pic3.zhimg.com/80/v2-ef3af412a26fc9a1d467dd348d7a41e2_720w.jpg" alt="img"></p><p>（1）一阶相似性</p><p>一阶相似度用来表示图中局部有连边的节点相似度，对于一个无向边(i,j), 定义节点 <img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=v_j" alt="[公式]"> 的经验概率分布为： <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bp%7D_1(v_i,v_j)=%5Cfrac%7Bw_%7Bij%7D%7D%7BW%7D" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=w_%7Bij%7D" alt="[公式]"> 为边权， <img src="https://www.zhihu.com/equation?tex=W=%5Csum_%7B(i,j)%5Cin+E%7Dw_%7Bij%7D" alt="[公式]"></p><p>当转化为低维向量时，定义其对应的联合概率分布为： <img src="https://www.zhihu.com/equation?tex=p_1(v_i,v_j)=%5Cfrac%7B1%7D%7B1+exp(-%5Coverrightarrow%7Bu%7D_i%5ET%5Coverrightarrow%7Bu%7D_j)%7D" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=%5Coverrightarrow%7Bu%7D_i%5Cin+R%5Ed" alt="[公式]"> 为节点 <img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]"> 的低维向量表示, 因此，为了保证在转化后仍然保持高维空间中的相似关系，即需要使如下损失函数最小化：</p><p><img src="https://www.zhihu.com/equation?tex=O_1=d(%5Chat%7Bp%7D_1(%5Ccdot,%5Ccdot),p_1(%5Ccdot,+%5Ccdot))" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=d(%5Ccdot,+%5Ccdot)" alt="[公式]"> 为两种分布的距离，可以利用KL散度来进行优化，即损失函数定义为：</p><p><img src="https://www.zhihu.com/equation?tex=O_1=-%5Csum_%7B(i,j)%5Cin+E%7Dw_%7Bij%7Dlogp_1(v_i,v_j)" alt="[公式]"></p><p>通过最小化损失函数，不断迭代优化更新参数，即可得到节点的低维向量表示，但是一阶相似度只适用于无向图。</p><p>（2）二阶相似性</p><p>二阶相似度在有向图与无向图中都适用。二阶相似性认为如果两个节点有许多共同的邻居，则他们相似。在这样的假设下，每一个节点都可以看作是其他节点的“上下文”，那么有相同上下文的节点是相似的。与一阶相似度类似，对于有向图，首先定义节点 <img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=v_j" alt="[公式]"> 的条件概率分布为： <img src="https://www.zhihu.com/equation?tex=%5Chat%7Bp%7D_2(v_j%7Cv_i)=%5Cfrac%7Bw_%7Bij%7D%7D%7Bd_i%7D" alt="[公式]"> ，其中， <img src="https://www.zhihu.com/equation?tex=w_%7Bij%7D" alt="[公式]"> 为边权， <img src="https://www.zhihu.com/equation?tex=d_i" alt="[公式]"> 为节点i的出度，即 <img src="https://www.zhihu.com/equation?tex=d_i=%5Csum_%7Bk%5Cin+N(i)%7Dw_%7Bik%7D" alt="[公式]"> ，N(i)是节点i的邻居节点。</p><p>而当转化为低维向量时，由于每个节点都有两个角色，节点本身，以及其他节点的“上下文”，因此每个节点对应两个向量，自身的向量 <img src="https://www.zhihu.com/equation?tex=%5Coverrightarrow%7Bu%7D_i" alt="[公式]"> , 以及作为其他节点的“上下文”时的向量 <img src="https://www.zhihu.com/equation?tex=%5Coverrightarrow%7Bu%7D_l%27" alt="[公式]"> 。对于一个有向边(i,j)，定义条件概率分布：</p><p><img src="https://www.zhihu.com/equation?tex=p_2(v_j%7Cv_i)=%5Cfrac%7Bexp(%5Coverrightarrow%7Bu_j%7D%5ET%5Coverrightarrow%7Bu_i%7D)%7D%7B%5Csum_%7Bk=1%7D%5E%7B%7CV%7C%7Dexp(%5Coverrightarrow%7Bu_k%7D%5ET%5Coverrightarrow%7Bu_i%7D)%7D" alt="[公式]"></p><p>其中|V|是每个节点的上下文个数，因此，为了保证在转化后仍然保持高维空间中的相似关系，即需要使如下损失函数最小化： <img src="https://www.zhihu.com/equation?tex=O_2=%5Csum_%7Bi%5Cin+V%7D%5Clambda_id(%5Chat%7Bp%7D_2(%5Ccdot%7Cv_i),p_2(%5Ccdot%7Cv_i))" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=d(%5Ccdot,+%5Ccdot)" alt="[公式]"> 为两种分布的距离，利用参数 <img src="https://www.zhihu.com/equation?tex=%5Clambda_i" alt="[公式]"> 来调节不同节点的重要性，该参数可以利用节点的度，中心性，PageRank，或者模型训练得到。然后，可以利用KL散度来进行优化，即损失函数定义为： <img src="https://www.zhihu.com/equation?tex=O_2=-%5Csum_%7B(i,j)%5Cin+E%7Dw_%7Bij%7Dlogp_2(v_j%7Cv_i)" alt="[公式]"></p><p>通过最小化损失函数，不断迭代优化更新参数，即可得到节点的低维向量表示。</p><p>(3)模型优化</p><p>一方面，可以用负采样算法，计算开销由原来的 <img src="https://www.zhihu.com/equation?tex=O(%7CV%7C)" alt="[公式]"> 降到 <img src="https://www.zhihu.com/equation?tex=O(%7CK%7C)" alt="[公式]"> ，提高了运算效率。另外一方面，在利用梯度下降法对上述参数求解时，由于有边权重的存在，当边权重的方差较大时，对模型的扰动影响也较大，因此可以对边根据其权重大小进行采样，首先计算边权重总和，然后随机选择一个值，看其落在哪个区间中，此处利用Alias 采样方法，只需要花费常数O(1)时间，结合负采样方法，每一步只需花费 <img src="https://www.zhihu.com/equation?tex=O(dK)" alt="[公式]"> 时间，全部的复杂度为 <img src="https://www.zhihu.com/equation?tex=O(dK%7CE%7C)" alt="[公式]"> , 其中E为边的个数。</p><p>通过上述三个步骤，则可以得到每个节点的向量表示，LINE完整流程如图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-7b6aff677a33ac32cd2e4f250271c544_720w.jpg" alt="img"></p><h2 id="基于深度学习的表示学习"><a href="#基于深度学习的表示学习" class="headerlink" title="基于深度学习的表示学习"></a>基于深度学习的表示学习</h2><p>算法概览</p><ul><li><a href="https://link.zhihu.com/?target=https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">SDNE：Structural Deep Network Embedding</a>【<a href="https://link.zhihu.com/?target=https://github.com/shenweichen/GraphEmbedding">代码</a>】</li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1609.02907.pdf">GCN：Semi-Supervised classification with graph convolutional networks</a>【<a href="https://link.zhihu.com/?target=https://github.com/tkipf/pygcn">代码</a>】</li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1706.02216.pdf">GraphSAGE：Inductive Representation Learning on Large Graphs</a>【<a href="https://link.zhihu.com/?target=https://github.com/williamleif/GraphSAGE">代码</a>】</li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1710.10903.pdf">GAT：Graph Attention Networks</a>【<a href="https://link.zhihu.com/?target=https://github.com/PetarV-/GAT">代码</a>】</li></ul><p>基于深度学习的图表示核心思想是对邻居节点进行聚合，如下图所示，左边是原始图网络，右图是模型的基本思想，对每个节点的邻居进行聚合，其中黑箱子可以利用不同的神经网络，此处为3层，Layer0可以输入节点特征，Layer K聚合上一层的邻居信息。基础的模型为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+h_v%5E0&amp;=x_v+%5C%5C+h_v%5Ek&amp;=%5Csigma(W_k%5Csum_%7Bu%5Cin+N(v)%7D%5Cfrac%7Bh_u%5E%7Bk-1%7D%7D%7B%7CN(v)%7C%7D+B_kh_v%5E%7Bk-1%7D)+%5C%5C+z_v&amp;=h_v%5EK+%5Cend%7Balign%7D" alt="[公式]"></p><p>此处利用了上一层邻居的平均来进行聚合， <img src="https://www.zhihu.com/equation?tex=W_k" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=B_k" alt="[公式]"> 是要学习的参数，可以利用无监督和有监督的方法学习参数。由于聚合参数在所有节点中是共享的，所以此方法可以快速泛化到训练中未出现过的节点中</p><p><img src="https://pic4.zhimg.com/80/v2-c013d963e495fa7e13c0e3f282e64a6f_720w.jpg" alt="img"></p><ol><li><strong>SDNE</strong></li></ol><p>SDNE(Structural Deep Network Embedding)将深度学习用于网络表示，利用非线性映射保留网络结构，也可以看作是一种自编码器，其中与LINE类似，利用一阶相似度保存临近节点之间的局部相似关系，利用二阶相似度保存每对节点的邻居节点间的相似度从而保存全局网络结构，通过这样的编码-解码利用非线性函数把输入转化到表示空间。</p><p>下图为SDNE的模型结构，核心框架是自编码器Encoder-Decoder, 首先对输入的节点进行编码，保存到y，然后再对y进行解码，尽可能的还原原始输入，其中根据一阶相似度与二阶相似度建立损失函数，通过最小化损失函数来进行参数更新，从而得到节点的低维向量表示。</p><p><img src="https://pic1.zhimg.com/80/v2-ed635c3a6074ab32d2b50147c223f6ec_720w.jpg" alt="img"></p><p>（1）自编码器Encoder-Decoder</p><p>如下图所示，在编码器Encoder中，对输入节点x进行编码，经历K层，存储在背景向量 <img src="https://www.zhihu.com/equation?tex=y_i%5E%7B(K)%7D" alt="[公式]">中，背景向量 <img src="https://www.zhihu.com/equation?tex=y_i%5E%7B(K)%7D" alt="[公式]"> 尽可能保留了原始输入节点的信息，然后将背景向量 <img src="https://www.zhihu.com/equation?tex=y_i%5E%7B(K)%7D" alt="[公式]"> 作为解码器Decoder的输入，建立模型还原初始输入x。</p><p><img src="https://pic3.zhimg.com/80/v2-344949693b1a5c39e625b409b3524e12_720w.jpg" alt="img"></p><p>（2）一阶相似度</p><p>为了捕捉图相邻节点的局部的相似结构，认为有直接相连的节点之间是相似的，利用编码后的临近节点有监督学习其一阶相似度：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7B1st%7D=%5Csum_%7Bi,j=1%7D%5Ens_%7Bij%7D%7C%7Cy_i%5E%7B(K)%7D-y_j%5E%7B(K)%7D%7C%7C_2%5E2=%5Csum_%7Bi,j=1%7D%5Ens_%7Bij%7D%7C%7Cy_i-y_j%7C%7C%5E2_2" alt="[公式]"></p><p>其中S为邻接矩阵，代表邻居的结构。</p><p>（3）二阶相似度</p><p>一阶相似度可以使有邻居节点有相似的向量表示，但是不连接并不能代表没有相似性，因此希望通过自编码器，可以尽可能的还原初始的网络结构，即保持网络全局的结构，建立损失函数：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7B2nd%7D=%5Csum_%7Bi=1%7D%5En%7C%7C%5Chat%7Bx%7D_i-x_i%7C%7C%5E2_2" alt="[公式]"></p><p>然而，邻接矩阵中0是大部分，把所有函数学成0不是理想的结果，因此可以对非零元素进行惩罚建立带权的损失函数：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7B2nd%7D=%5Csum_%7Bi=1%7D%5En%7C%7C(%5Chat%7Bx%7D_i-x_i)%5Codot+b_i%7C%7C%5E2_2=%7C%7C(%5Chat%7BX%7D-X)%5Codot+B%7C%7C%5E2_2" alt="[公式]"></p><p>其中 <img src="https://www.zhihu.com/equation?tex=b_%7Bij%7D=%5Cbegin%7Bcases%7D+1,+&amp;+if+%5C+s_%7Bij%7D=0;%5C%5C+%5Cbeta,+&amp;+if+%5C+s_%7Bij%7D+%5Cneq+0+%5Cend%7Bcases%7D,+%5Cbeta+%3E1" alt="[公式]"> 。</p><p>综上所述，可以优化目标函数：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+L_%7Bmix%7D&amp;=%5Calpha+L_%7B1st%7D+L_%7B2nd%7D+vL_%7Breg%7D%5C%5C&amp;=%5Calpha%5Csum_%7Bi,j=1%7D%5Ens_%7Bij%7D%7C%7Cy_i-y_j%7C%7C%5E2_2+%7C%7C(%5Chat%7BX%7D-X)%5Codot+B%7C%7C_2%5E2+%5Cfrac%7Bv%7D%7B2%7D%5Csum_%7Bk=1%7D%5EK(%7C%7CW%5E%7B(k)%7D%7C%7C%5E2_F+%7C%7C%5Chat%7BW%7D%5E%7B(k)%7D%7C%7C%5E2_F)+%5Cend%7Balign%7D" alt="[公式]"></p><p>对其中的参数利用梯度下降法进行求解，从而得到每个节点的向量表示。通过结合一阶相似度与二阶相似度，既保留了相似邻居的局部信息，也保留了网络结构的全局信息。SDNE的完整流程如下：</p><p><img src="https://pic3.zhimg.com/80/v2-eddd6f4fc72338b84f9938a1c2943572_720w.jpg" alt="img"></p><p><strong>2. GCN</strong></p><p>GCN的细节原理关于卷积核，傅里叶变化，拉普拉斯分解细究起来确实复杂，感兴趣的可以参考<a href="https://zhuanlan.zhihu.com/p/120311352">这里</a>，此处只不求甚解说结论好了，GCN可以理解为一种神经网络，用一种特殊的编码方式捕捉图信息，如下图所示为两层的图卷积网络，利用所有数据学习节点表示，并利用有标签的数据建立损失函数，半监督对节点进行分类。</p><p><img src="https://pic2.zhimg.com/80/v2-70994face5b6a9ba36e9623596800105_720w.jpg" alt="img"></p><p>其层与层之间的传播规则为： <img src="https://www.zhihu.com/equation?tex=H%5E%7B(l+1)%7D=%5Csigma(%5Cwidetilde%7BD%7D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D%5Cwidetilde%7BA%7D%5Cwidetilde%7BD%7D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7DH%5E%7B(l)%7DW%5E%7B(l)%7D)" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cwidetilde%7BA%7D=A+I_N" alt="[公式]"> 是邻接矩阵加上自身单位矩阵， <img src="https://www.zhihu.com/equation?tex=%5Cwidetilde%7BD%7D_%7Bii%7D=%5Csum_j%5Cwidetilde%7BA%7D_%7Bij%7D" alt="[公式]"> 为度矩阵， <img src="https://www.zhihu.com/equation?tex=W%5E%7B(l)%7D" alt="[公式]"> 是每一层待训练的参数，H是每一层的特征，初始为图的特征矩阵， <img src="https://www.zhihu.com/equation?tex=%5Cwidetilde%7BD%7D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D%5Cwidetilde%7BA%7D%5Cwidetilde%7BD%7D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D" alt="[公式]"> 为对邻接矩阵的归一化，是谱图卷积的核心。</p><p>（对比最开始提到的，对邻居节点进行均值聚合 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bu%5Cin+N(v)%7D%5Cfrac%7Bh_u%5E%7Bk-1%7D%7D%7B%7CN(v)%7C%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=H%5Ek=D%5E%7B-1%7DAH%5E%7Bk-1%7D" alt="[公式]"> 等价，可以理解为GCN是对神经网络的一种堆叠，来平均邻居信息）</p><p>通过上式后就可以利用损失函数（监督/无监督）来对参数进行更新学习，得到每个节点的向量表示。</p><p><strong>3. GraphSAGE</strong></p><p>GraphSAGE全称为Graph sample and aggregate，对比深度学习表示学习最开始提到的核心思想对邻居简单聚合，GraphSAGE的聚合更加泛化。</p><ul><li>简单邻居聚合： <img src="https://www.zhihu.com/equation?tex=h_v%5Ek=%5Csigma(W_k%5Csum_%7Bu%5Cin+N(v)%7D%5Cfrac%7Bh_u%5E%7Bk-1%7D%7D%7B%7CN(v)%7C%7D+B_kh_v%5E%7Bk-1%7D)+" alt="[公式]"></li><li>GraphSAGE聚合： <img src="https://www.zhihu.com/equation?tex=h_v%5Ek=%5Csigma(%5BW_k%5Ccdot+AGG(%5C%7Bh_u%5E%7Bk-1%7D,%5Cforall+u%5Cin+N(v)%5C%7D),+B_kh_v%5E%7Bk-1%7D%5D)" alt="[公式]"></li></ul><p>其中对于函数AGG有多种选择，比如说：</p><ul><li>均值聚合： <img src="https://www.zhihu.com/equation?tex=AGG=%5Csum_%7Bu%5Cin+N(v)%7D%5Cfrac%7Bh_u%5E%7Bk-1%7D%7D%7B%7CN(v)%7C%7D" alt="[公式]"></li><li>池化聚合： <img src="https://www.zhihu.com/equation?tex=AGG=%5Cgamma+(%5C%7BWh_u%5E%7Bk-1%7D,%5Cforall+u%5Cin+N(v)%5C%7D)" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="[公式]"> 为max pool/mean pool</li><li>LSTMuh聚合： <img src="https://www.zhihu.com/equation?tex=AGG=LSTM(%5Bh_u%5E%7Bk-1%7D,+%5Cforall+u%5Cin+%5Cpi+(N(v))%5D)" alt="[公式]"></li></ul><p>GraphSAGE的完整流程如图所示：</p><p><img src="https://pic2.zhimg.com/80/v2-1e87d42bec44d020b257987501275935_720w.jpg" alt="img"></p><p><strong>4. Graph Attention Networks（GAT）</strong></p><p>在GraphSAGE中认为，邻居节点对当前节点的影响是相同的，而GAT引入注意力机制，对不同的邻居节点赋予不同的权重</p><ul><li>Step1：对于图中每个节点，通过线性映射定义节点u对节点v的重要性：<img src="https://www.zhihu.com/equation?tex=e_%7Bvu%7D=a(W_kh_u%5E%7Bk-1%7D,+W_kh_v%5E%7Bk-1%7D)" alt="[公式]"></li><li>Step2：计算注意力得分：<img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bvu%7D=%5Cfrac%7Bexp(e_%7Bvu%7D)%7D%7B%5Csum_%7Bk%5Cin+N(v)%7Dexp(e_%7Bvk%7D)%7D" alt="[公式]"></li><li>Step3：得到最终的隐藏层：<img src="https://www.zhihu.com/equation?tex=h_v%5Ek=%5Csigma(%5Csum_%7Bu%5Cin+N(v)%7D%5Calpha_%7Bvu%7DW_kh_u%5E%7Bk-1%7D)" alt="[公式]"></li></ul><p>关于图表示学习还有很多可以研究的方向，比如说：</p><ul><li>怎样在大规模的网络中更高效的学习图表示，课程中也提到了PinSage，是一个比较典型的将GCN大规模应用于工业的算法，这个以后再补~</li><li>整个图的表示：最简单的方法是对图所有节点取平均，课程中还提到了anonymous walk embeddings方法等</li><li>关于motif：考虑利用motif来重新定义网络，来发现更多的网络信息，从而学习网络表示</li><li>异构图：对于有多种类型节点和边的网络如何表示，在推荐等领域被广泛应用</li><li>动态图：图网络往往是动态变化的，和时序相关的，比如说在异常检测问题中，检测边的变化或许可以捕捉异常，如果学习这种图的动态表示也是可以探讨的课题</li></ul>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】5. Spectral Clustering</title>
      <link href="/2020/11/24/si-tan-fu-cs224w-tu-ji-qi-xue-xi-5-spectral-clustering/"/>
      <url>/2020/11/24/si-tan-fu-cs224w-tu-ji-qi-xue-xi-5-spectral-clustering/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本节课继续讨论Community Detection算法，讲解的是谱聚类（Spectral Clustering）算法，最后还会与前面第3节课学习的<code>motif</code>知识关联起来，讲解一种基于Motif的谱聚类算法。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hl1lxsj22bc1qiqm5.jpg"><h2 id="谱聚类算法"><a href="#谱聚类算法" class="headerlink" title="谱聚类算法"></a>谱聚类算法</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hlnoatj22bc1qi1kv.jpg"><blockquote><p>上面给出了谱聚类的”三步曲”，在正式讲解“三步曲”之前，我们先来定义一下谱聚类要解决什么问题？</p></blockquote><h3 id="Part-1-问题定义"><a href="#Part-1-问题定义" class="headerlink" title="Part 1 问题定义"></a>Part 1 问题定义</h3><p>给定一个图$G=(V,E)$，如下图所示，我们要在这个图上做<strong>bi-partition</strong>任务（后面的讲解是基于二簇划分问题的，当然谱聚类可以用于多簇划分），希望把下图划分成两个不相交的群体A，B，让组内尽可能相似，组间差异尽可能大。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hlfx0oj22bc1qi1io.jpg"><blockquote><ul><li>那么怎么定义一个“好”的划分？怎样快速找到这样的划分呢？</li></ul></blockquote><h3 id="Part-2-评价指标"><a href="#Part-2-评价指标" class="headerlink" title="Part 2 评价指标"></a>Part 2 评价指标</h3><ul><li>在第四节课中，我们有介绍度量聚类效果的指标<code>modularity</code>，这里我们使用一种新的聚类度量指标<code>conductance</code>；</li><li>同样，我们之前的目标是最大化<code>modularity</code>，因此在本节课我们的目标是最大化<code>conductance</code>。</li></ul><p>对于一个好的划分，一个很自然的想法，直觉上就是<font color="red"><strong>最大化组内连接数，最小化组间连接数</strong>：</font></p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hliytij22bc1qiasg.jpg"><p>我们其实可以很形象地将图上的聚类划分问题，看做一个<strong>切割</strong>的过程：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hnt96sj22bc1qitwi.jpg"><blockquote><ul><li><code>cut</code>：这里的<code>cut</code>是一个名词，它有全新的意义。它是定义在两个簇共享的边集的<code>sum</code>值。</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hm3a3sj22bc1qiazv.jpg"><blockquote><p>但是如果我们想最小化cut有一个问题，如上图所示，当有一个节点度数为1时，切割这一条边可最小化cut，即cut=1，但是很显然这并不是最优化的划分结果，直觉上看，最优化的划分应该为蓝色线所示。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hqb1ndj22bc1qi4qp.jpg"><blockquote><ul><li><p>正是由于前面讲的那种degenerate case，我们需要进一步对度量指标<code>cut</code>进行归一化来考虑group内的connectivity信息——<code>conductance</code>:</p></li><li><p>我们想<code>conductance</code>变小，所以我们就会想让<code>vol(A)</code>和<code>vol(B)</code>都能变得尽可能大，从而避免前一张PPT上的极端情况。</p></li><li><p>由<code>conductance</code>计算公式的含义我们知道<strong>它能够产生相对均匀划分</strong>；</p></li><li><p>但是求解一个这样的最优的conductance是一个NP-hard问题。根据我们的机器学习经验，<strong>我们需要寻找一个近似解</strong><del>这个近似解就是谱聚类</del></p></li></ul></blockquote><h3 id="Part-3-谱图划分"><a href="#Part-3-谱图划分" class="headerlink" title="Part 3 谱图划分"></a>Part 3 谱图划分</h3><h4 id="重新理解-Ax-的意义——聚合邻居的信息"><a href="#重新理解-Ax-的意义——聚合邻居的信息" class="headerlink" title="重新理解$Ax$的意义——聚合邻居的信息"></a>重新理解$Ax$的意义——聚合邻居的信息</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hs4gx6j22bc1qihc1.jpg"><hr><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hrcge9j22bc1qi4qp.jpg"><blockquote><ul><li>之前我们重新审视了$Ax$，现在让我们结合重新审视的$Ax$来回想一个啥是eigenvector，eigenvalue？</li></ul><blockquote><p><strong>these are special vectors assigned to nodes such that when you sum up the value from your neighbors you get the same value as you had you just scaled miraculous！！！</strong></p></blockquote><ul><li><p><code>spectrum</code>：频谱，需要注意的是<code>spectrum</code>里面的<strong>特征值</strong>是严格按照从小到大的顺序排列的！</p><blockquote><p>这其实与电路与信号里面的傅里叶变换联系起来了，这里的频率就是特征值。</p></blockquote></li></ul></blockquote><h5 id="Example-d-Regular-Graph"><a href="#Example-d-Regular-Graph" class="headerlink" title="Example: d-Regular Graph"></a>Example: d-Regular Graph</h5><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hr5e91j22bc1qi7wh.jpg"><blockquote><p>d-regular graph</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hppnukj22bc1qib29.jpg"><h5 id="Example-Graph-on-2-Components"><a href="#Example-Graph-on-2-Components" class="headerlink" title="Example: Graph on 2 Components"></a>Example: Graph on 2 Components</h5><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hrca5ij22bc1qi7wh.jpg"><blockquote><ul><li><p>这里<code>eigenvalue</code>：$d$的<code>multiplicity</code>是2，也就是是说它由两个eigenvector。在这种类型的图中，the max eigenvalue的<strong>multiplicity</strong>可以告诉我们有多少的components。</p></li><li><p>$\lambda_n = \lambda_{n-1}$是因为multiplicity的大小</p></li><li><p>关于almost disconnected的时候，$\lambda_{n-1}$会非常接近$\lambda_n$的原因将会在后面学到。</p></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hu3m2aj22bc1qi4qp.jpg"><blockquote><ul><li><p><strong>Eigenvectors are orthogonal的原因在于Adjacency Matrix为对称矩阵！</strong></p></li><li><p>it means some coordinates should be higher than 0，一些会小于0；</p></li><li><p>再次提醒，老师这里举例子是2个partition的问题；</p></li></ul></blockquote><h3 id="Adjacency-Matrix"><a href="#Adjacency-Matrix" class="headerlink" title="Adjacency Matrix"></a>Adjacency Matrix</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hv2g92j22bc1qity7.jpg"><blockquote><ul><li>这里的n个real eigenvalues可能有重复的。</li></ul></blockquote><h3 id="Degree-Matrix"><a href="#Degree-Matrix" class="headerlink" title="Degree Matrix"></a>Degree Matrix</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ht6rnxj22bc1qinft.jpg"><h3 id="Laplacian-Matrix"><a href="#Laplacian-Matrix" class="headerlink" title="Laplacian Matrix"></a>Laplacian Matrix</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hy23xnj22bc1qi1kx.jpg"><blockquote><ul><li>0是我们这个问题中最小的eigenvalue（？）</li><li>Laplacian Matrix$L$是一个半正定矩阵，详细证明如下：</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hvuhghj22bc1qi4qp.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hvaa0pj22bc1qi4qp.jpg"><blockquote><p>注意是对于每一个symmetric matrix都有这样一个fact！</p><p>optimization problem</p><p>$x^TLx$就是二次型的定义，所以要回顾一下二次型才是~</p><p>只遍历边（i,j），所以需要乘以2。</p><p>再观察一下，我们的$x_i^2$一共计算了$D_{ii}$次，也就是度的次数，于是我们根据它的边来拆分$D_{ij}$次，可以得到一个完全平方式。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hw4qusj22bc1qi7wh.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i319kaj22bc1qi4qp.jpg"><blockquote><ul><li>eigenvector的长度为1，这个是可以Normalize的</li><li>restate之前的equation</li><li>我们可以想象$x$向量代表的是label，那么0两端的话，值会很大。</li><li>因为sum要为1，我们尽量让两端的点的数量相等。</li><li>如上所示，$\lambda_2$有着非常神奇的eigenvector；</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hz82u6j22bc1qi4qp.jpg"><blockquote><ul><li><code>Fiedler</code>的这个想法和我们之前得到的那个restate其实是差不多的，但是他的这个严格性太强了！这个最终结果会统计横跨0左右两端的数量。</li><li>在前一页PPT里面我们不会去enforce两个集合相等</li><li>我们称这里的vector为Fiedler vector</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i10cujj22bc1qi4qp.jpg"><blockquote><ul><li>a spectral term spectral graph theory</li><li>同样是之前的那个式子，这里是enforce其中的向量的总和为0，然后y的长度为1.</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i15d0yj22bc1qib29.jpg"><blockquote><p>这里的证明展示了这种基于图的Laplacian矩阵的spectral clustering会给你一个近似最优解</p><p>它会小于最优解的两倍，说明还是很有用的</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09hzct2pj22bc1qihcb.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i0kbplj22bc1qihce.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i36bk1j22bc1qinjx.jpg"><p>前面是mathematical intuition，接下来是实打实的算法~</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i30m1tj22bc1qikhf.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i40lpoj22bc1qi1kx.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i50baxj22bc1qi4qp.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i47lurj22bc1qits5.jpg"><blockquote><p>老师说 how beautifully it captures the graph structure~左图和右图完美匹配。</p><p>如果有四个类别怎么办？</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i5awj4j22bc1qih66.jpg"><blockquote><p>如上所示，我们依然使用之前的方法得到$x_2$，虽然它是用于二分类问题的，但是你可以发现，在0以上和0以下，依然还可以在划分为两个类别。</p><p>如何$\lambda_2$和$\lambda_1$是相等的，老师说自己的理论就会不成立。那么为什么不连通的图，他的两个就是相等的呢？好像我可以分析出来。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i887mjj22bc1qie2f.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iaepxwj22bc1qi4qp.jpg"><blockquote><p>这里k的数量不等于最后聚类的数量。</p></blockquote><h2 id="Why-Use-Multiple-Eigenvectors？"><a href="#Why-Use-Multiple-Eigenvectors？" class="headerlink" title="Why Use Multiple Eigenvectors？"></a>Why Use Multiple Eigenvectors？</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i7a3l4j22bc1qib29.jpg"><blockquote><ul><li></li><li>emphasize cohesive clusters</li><li>well-separated space</li><li>amplified 放大</li><li>attenuate 减弱</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i83r45j22bc1qi4nb.jpg"><p>注意是降序的。。。</p><p>如果我们的图是disconnected，那么$\lambda_1$和$\lambda_2$就是相等的了，如果是近似disconnected，那么$\lambda_1$和$\lambda_2$就应该是差不多大小。</p><p>for a general graph，the maximum eigenvalue will correspond to the density of the graph，now we create the graph laplacian that kind of flipped the spectrum around，so the largest eigenvalue D mapped to the trivial eigenvalue of 0。</p><h2 id="基于motif的谱聚类算法"><a href="#基于motif的谱聚类算法" class="headerlink" title="基于motif的谱聚类算法"></a>基于motif的谱聚类算法</h2><p>接下来，介绍一种super exciting extension of the spectral clustering method：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i64o35j22bc1qi153.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iars1wj22bc1qi7td.jpg"><p>能否基于motif进行聚类？</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09i8zt3tj22bc1qiwz1.jpg"><p>划分出上图红圈中的cluster。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ibojulj22bc1qi7v1.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ibrzffj22bc1qib0u.jpg"><blockquote><p>将前面提到的cut和volume的概念迁移到motif上：</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ibpb0hj22bc1qie0g.jpg"><blockquote><ul><li>motif cut=1</li><li>motif volume=10</li></ul></blockquote><p>next,how do i automatedly find good cuts,how do I find clusters of motifs?</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ibhozvj22bc1qie30.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ief5rwj22bc1qi1f4.jpg"><blockquote><p>$W^{M}$的值随着$M$的不同而变化。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iglfjzj22bc1qikja.jpg"><blockquote><p>要将其变成无向图</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09if38ywj22bc1qikfd.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iewp9dj22bc1qitxx.jpg"><blockquote><p>尽管我们最后变成了无向图，看起来有点像edge cut，但是实际上还是原来的motif cut</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ie9a17j22bc1qiqlp.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iebrh7j22bc1qi7q3.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ij5rsxj22bc1qi7q6.jpg"><blockquote><p>分情况寻找划分的点。</p><p>sweep procedure：扫描过程</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ihdbc5j22bc1qinke.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09igz3gnj22bc1qitvk.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ij3ubsj22bc1qitqt.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ijil96j22bc1qihdt.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ikmbuuj22bc1qittm.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iiys1ij22bc1qikbb.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iklftqj22bc1qi4n6.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09iow8l0j22bc1qiu0x.jpg"><p>aquatic layer</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09imqpv5j22bc1qi1kx.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09imjdnnj22bc1qix0x.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09ip4jpnj22bc1qinpd.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09imyozdj22bc1qikcv.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gl09io5cqvj22bc1qi4qp.jpg"><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><h3 id="聚类度量指标Modularity和Conductance的对比理解"><a href="#聚类度量指标Modularity和Conductance的对比理解" class="headerlink" title="聚类度量指标Modularity和Conductance的对比理解"></a>聚类度量指标<code>Modularity</code>和<code>Conductance</code>的对比理解</h3>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
            <tag> 特征值与特征向量的应用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习框架-动态图和静态图</title>
      <link href="/2020/11/24/shen-du-xue-xi-kuang-jia-dong-tai-tu-he-jing-tai-tu/"/>
      <url>/2020/11/24/shen-du-xue-xi-kuang-jia-dong-tai-tu-he-jing-tai-tu/</url>
      
        <content type="html"><![CDATA[<p><code>Tensorflow</code>,<code>Pytorch</code>,<code>PaddlePaddle</code>等等深度学习框架，在介绍框架的时候都会提及到<strong>动态图</strong>和<strong>静态图</strong>。其实动态图和静态图都属于计算图，本文就来讲讲什么是动态图和静态图。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>不论是动态图还是静态图，它们都属于<strong>计算图</strong>。计算图是用来<strong>描述运算的有向无环图</strong>，它有两个主要元素：</p><ul><li>结点（Node）<ul><li><strong>结点表示数据</strong>，如向量、矩阵、张量。</li></ul></li><li>边（Edge）<ul><li><strong>边表示运算</strong>，如加、减、乘、除、卷积等。</li></ul></li></ul><p>采用计算图来描述运算的好处不仅是<strong>让运算流的表达更加简洁清晰</strong>，还有一个更重要的原因是<strong>方便求导计算梯度</strong>。</p><p><img src="https://img-blog.csdnimg.cn/20200924175655155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3podWd1aXFpbjE=,size_16,color_FFFFFF,t_70"></p><p>上图表示的是$ y = (w + x) * (w + 1)$代表的计算图，若要计算$y$对$w$的导数，那么结合链式求导法则，就在计算图中<font color="red"><strong>反向从y找到所有到w的路径每条路径上各段的导数相乘就是该路径的偏导，最后再将所有路径获得的偏导求和即可。</strong></font></p><p><strong>叶子节点</strong>是用户创建的变量，如上图的$x$与$w$，<strong>在Pytorch的实现中，为了节省内存，在梯度反向传播结束后，非叶子节点的梯度都会被释放掉。</strong></p><h2 id="动态图"><a href="#动态图" class="headerlink" title="动态图"></a>动态图</h2><p><strong>动态图</strong>意味着<strong>计算图的构建和计算同时发生（define by run）</strong>。这种机制由于能够实时得到中间结果的值，使得<strong>调试更加容易</strong>，同时我们将大脑中的想法转化为代码方案也变得更加容易，<strong>对于编程实现来说更友好</strong>。<code>Pytorch</code>使用的就是动态图机制，因此它更易上手，风格更加pythonic，大受科研人员的喜爱。</p><h2 id="静态图"><a href="#静态图" class="headerlink" title="静态图"></a>静态图</h2><p><strong>静态图</strong>则意味着<strong>计算图的构建和实际计算是分开（define and run）的</strong>。在静态图中，会事先了解和定义好整个运算流，这样之后再次运行的时候就不再需要重新构建计算图了（可理解为编译），因此速度会比动态图更快，从<strong>性能上来说更加高效，</strong>但这也意味着你所期望的程序与编译器实际执行之间存在着更多的代沟，代码中的错误将难以发现，无法像动态图一样随时拿到中间计算结果。<code>Tensorflow</code>默认使用的是静态图机制，这也是其名称的由来，先定义好整个计算流（flow），然后再对数据（tensor）进行计算。</p><h2 id="动态图-vs-静态图"><a href="#动态图-vs-静态图" class="headerlink" title="动态图 vs 静态图"></a>动态图 vs 静态图</h2><p>通过一个例子来对比下动态图和静态图机制在编程实现上的差异，分别基于<code>Pytorch</code>和<code>Tensorflow</code>实现，先来看看<code>Pytorch</code>的动态图机制：</p><pre><code class="python">import torchfirst_counter=torch.Tensor([0])second_counter=torch.Tensor([10])while(first_counter &lt; second_counter)    first_counter+=2    second_counter+=1print(first_counter)print(second_counter)    </code></pre><p>程序执行结果：</p><pre><code class="python">tensor([20.])tensor([20.])</code></pre><p>以看到，这与普通的Python编程无异。</p><p>再来看看在基于<code>Tensorflow</code>的静态图机制下是如何实现上述程序的：</p><pre><code class="python">import tensorflow as tffirst_counter = tf.constant(0)second_counter = tf.constant(10)def cond(first_counter,second_conter,*args):    return first_counter&lt;second_counterdef body(first_counter,second_conter):      first_counter = tf.add(first_counter,2)      second_conter = tf.add(second_counter,1)      return first_counter, second_counterc1,c2 = tf.while_loop(cond,body,[first_counter,second_counter])with tf.compat.v1.Session() as sess:      counter_1_res, counter_2_res = sess.run([c1,c2])      print(counter_1_res,counter_2_res)</code></pre><p>这段代码对应的是TensorFlow1.x版本的，程序在<code>with tf.compat.v1.Session() as sess</code>：之前都是定义计算图，定义了数据和操作步骤，在执行过程中实际上是没有值的，实际上在<code>sess.run</code>之后才有真正计算出了结果。</p><p>(⊙o⊙)… 对<code>Tensorflow</code>不熟悉的童鞋来说,第一反应可能会是:这什么鬼!?确实,看上去会有点难受。。。</p><p><code>Tensorflow</code>在静态图的模式下，每次运算使用的计算图都是同一个，因此不能直接使用 Python 的 while 循环语句，而是要使用其内置的辅助函数 <code>tf.while_loop</code>，而且还要<code>tf.Session().run()</code>之类的乱七八糟..</p><p>而<code>Pytorch</code>是动态图的模式，每次运算会构建新的计算图，在编程实现上不需要额外的学习成本（当然首先你得会Python）。</p><h2 id="动静结合"><a href="#动静结合" class="headerlink" title="动静结合"></a>动静结合</h2><p>在最近开源的框架<strong>MegEngine</strong>中，集成了两种图模式，并且可以进行相互切换，下面举例说明将动态图转换为静态图编译过程中进行的<strong>内存和计算优化</strong>：</p><p>$y = w*x + b$的动态计算图如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/20481399-666b2532e0573b0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/299/format/webp" alt="img"></p><p>可以看到，中间的运算结果是被保留下来的，如p=w*x，这样就一共需要5个变量的存储空间。若切换为静态图，由于事先了解了整个计算流，因此可以让y复用p的内存空间，这样一共就只需要4个变量的存储空间。</p><p>另外，MegEngine 还使用了<strong>算子融合 （Operator Fuse）</strong>的机制，用于<strong>减少计算开销</strong>。对于上面的动态计算图，切换为静态图后可以将乘法和加法融合为一个三元操作（假设硬件支持）：乘加（如下图所示），从而降低计算量。</p><p><img src="https://upload-images.jianshu.io/upload_images/20481399-27b74b544285569e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/251/format/webp" alt="img"></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://blog.csdn.net/zhuguiqin1/article/details/108775731">深度学习框架 动态图和静态图</a></li><li><a href="https://www.jianshu.com/p/505e1e0142c1">深度学习框架 の 动态图 vs 静态图</a></li><li><a href="https://zhuanlan.zhihu.com/p/139386731">第三节：计算图与动态图机制（1周作业2）</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型评估指标AUC和ROC，这是我看到的最透彻的讲解</title>
      <link href="/2020/11/22/mo-xing-ping-gu-zhi-biao-auc-he-roc-zhe-shi-wo-kan-dao-de-zui-tou-che-de-jiang-jie/"/>
      <url>/2020/11/22/mo-xing-ping-gu-zhi-biao-auc-he-roc-zhe-shi-wo-kan-dao-de-zui-tou-che-de-jiang-jie/</url>
      
        <content type="html"><![CDATA[<blockquote><p> 文章来源：<a href="https://blog.csdn.net/liweibin1994/article/details/79462554">模型评估指标AUC（area under the curve）</a></p></blockquote><p>AUC在机器学习领域中是一种模型评估指标。根据维基百科的定义，AUC(area under the curve)是ROC曲线下的面积。所以，在理解AUC之前，要先了解ROC是什么。而ROC的计算又需要借助混淆矩阵，因此，我们先从混淆矩阵开始谈起。</p><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>假设，我们有一个任务：给定一些患者的样本，构建一个模型来预测肿瘤是不是恶性的。在这里，肿瘤要么良性，要么恶性，所以这是一个典型的二分类问题。</p><p>假设我们用y=1表示肿瘤是良性，y=0表示肿瘤是恶性。则我们可以制作如下图的表格： </p><p><img src="https://img-blog.csdn.net/20180306192541902?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="混淆矩阵"></p><blockquote><ul><li><p><code>T</code>:True;</p></li><li><p><code>F</code>:False;</p></li><li><p><code>P</code>:Positive;</p></li><li><p><code>N</code>:Negative;</p></li></ul></blockquote><p>如上图：</p><ul><li>TP表示预测为良性，而实际也是良性的样例数； </li><li>FN表示预测为恶性，而实际是良性的样例数； </li><li>FP表示预测为良性，而实际是恶性的样例数； </li><li>TN表示预测为恶性，而实际也是恶性的样例数；</li></ul><p>所以，上面这四个数就形成了一个矩阵，称为<strong>混淆矩阵</strong>。</p><p>那么接下来，我们如何利用混淆矩阵来计算ROC呢？ </p><p>首先我们需要定义下面两个变量：<br>$$<br>FPR=\frac {FP} {FP+TN}<br>$$<br>FPR表示，在所有的恶性肿瘤中，被预测成良性的比例。称为伪阳性率。伪阳性率告诉我们，随机拿一个恶性的肿瘤样本，有多大概率会将其预测成良性肿瘤。<strong>显然我们会希望<code>FPR</code>越小越好。</strong><br>$$<br>TPR=\frac {TP} {TP+FN}<br>$$<br>TPR表示，在所有良性肿瘤中，被预测为良性的比例。称为真阳性率。真阳性率告诉我们，随机拿一个良性的肿瘤样本时，有多大的概率会将其预测为良性肿瘤。<strong>显然我们会希望<code>TPR</code>越大越好。</strong></p><p>如果以FPR为横坐标，TPR为纵坐标，就可以得到下面的坐标系： </p><p><img src="https://img-blog.csdn.net/20180306194149907?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="坐标系"></p><p>可能看到这里，你会觉得有点奇怪，用FPR和TPR分别作横纵坐标有什么用呢？我们先来考察几个特殊的点。</p><ul><li><strong>点（0,1）</strong>，即FPR=0，TPR=1。FPR=0说明FP=0，也就是说，没有假正例。TPR=1说明，FN=0，也就是说没有假反例。这不就是最完美的情况吗？所有的预测都正确了。良性的肿瘤都预测为良性，恶性肿瘤都预测为恶性，分类百分之百正确。这也体现了FPR 与TPR的意义。就像前面说的我们本来就希望FPR越小越好，TPR越大越好。</li><li><strong>点（1,0）</strong>，即FPR=1，TPR=0。这个点与上面那个点形成对比，刚好相反。所以这是最糟糕的情况。所有的预测都预测错了。</li><li><strong>点（0,0）</strong>，即FPR=0，TPR=0。也就是FP=0，TP=0。所以这个点的意义是所有的样本都预测为恶性肿瘤。也就是说，无论给什么样本给我，我都无脑预测成恶性肿瘤就是了。</li><li><strong>点（1,1）</strong>，即FPR=1，TPR=1。显然，这个点跟点（0,0)是相反的，这个点的意义是将所有的样本都预测为良性肿瘤。</li></ul><p>考察完这四个点，我们可以知道：</p><blockquote><p><strong>如果一个点越接近左上角，那么说明模型的预测效果越好。如果能达到左上角（点（0,1)），那就是最完美的结果了。</strong></p></blockquote><h2 id="ROC曲线和AUC"><a href="#ROC曲线和AUC" class="headerlink" title="ROC曲线和AUC"></a>ROC曲线和AUC</h2><p>介绍了混淆矩阵之后，我们就可以了解一下ROC（receiver operating characteristic curve）曲线是怎么定义的。</p><p>我们知道，在二分类（0，1）的模型中，一般我们最后的输出是一个概率值，表示结果是1的概率。那么我们最后怎么决定输入的x是属于0或1呢？我们需要一个阈值，超过这个阈值则归类为1，低于这个阈值就归类为0。所以，不同的阈值会导致分类的结果不同，也就是混淆矩阵不一样了，FPR和TPR也就不一样了。所以当阈值从0开始慢慢移动到1的过程，就会形成很多对(FPR, TPR)的值，将它们画在坐标系上，就是所谓的ROC曲线了。</p><p>我们来举一个例子。比如我们有5个样本：<br>真实的类别（label）为y = c（1,1,0,0,1）.<br>一个分类器预测样本为1的概率为p=c（0.5, 0.6, 0.55, 0.4, 0.7）。</p><p>看到这里，也许我们还有一点难理解。这里要注意: </p><ol><li><p>阈值的范围是[0,1]，当阈值从1到0慢慢移动时，FPR会越来越大。因为FP(假正例）会越来越多。</p></li><li><p>如果在给定的样本中，我都随机预测，也就是0.5概率预测为良性肿瘤，0.5概率预测为恶性肿瘤。那么这条曲线会是怎样的呢？可以想象，如果数据是均匀，那么这条曲线就是y=x。</p><blockquote><p>这里需要理解：<font color="red"><strong>为什么在随机预测的情况下，无论阈值$\alpha$取何值，<code>FPR</code>和<code>TPR</code>一直都是相等的？</strong></font></p><ul><li>首先，这里的<strong>随机预测</strong>表示的含义是对于<code>ground-truth</code>为<code>True</code>的样本集，最终其样本点的预测值都是均匀分布在区间<code>[0,1]</code>。对于<code>ground-truth</code>为<code>False</code>的样本集同样是如此；</li><li>因此，无论阈值$\alpha$取什么，低于$\alpha$的<code>groudtruth</code>为<code>True</code>和<code>groudtruth</code>为<code>False</code>的样本数量应该是一样多的；</li><li>综上，<code>FPR</code>和<code>TPR</code>一直都是相等的。</li></ul></blockquote></li><li><p>注意曲线一定是从（0,0)开始最终到达（1,1)的。</p></li><li><p>事实上，ROC曲线不是光滑的，而是阶梯型的。为什么呢？因为样本的数量是有限的，而FPR和TPR的变化需要至少有一个样本变化了，在没有变化的间隙里，就不会有变化。也就是说，步进是1/样本数。</p></li></ol><p>理解了上面四个点的意义就知道了。</p><p><strong>得到了ROC曲线，我们就可以计算曲线下方的面积了——计算出来的面积就是AUC值了。</strong></p><h2 id="AUC值的意义"><a href="#AUC值的意义" class="headerlink" title="AUC值的意义"></a>AUC值的意义</h2><p>知道了如何计算AUC值，我们当然是要来问一下AUC值的意义了。为什么我们要这么大费周章地搞出这个AUC值？</p><p>假设我们有一个分类器，输出是样本输入正例的概率，所有的样本都会有一个相应的概率，这样我们可以得到下面这个图： </p><p><img src="https://img-blog.csdn.net/20180307105425516?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>其中，横轴表示预测为正例的概率，纵轴表示样本数。<br>所以，蓝色区域表示所有负例样本的概率分布，红色样本表示所有正例样本的概率分布。显然，如果我们希望分类效果最好的话，那么红色区域越接近1越好，蓝色区域越接近0越好。</p><p>为了验证你的分类器的效果。你需要选择一个阈值，比这个阈值大的预测为正例，比这个阈值小的预测为负例。如下图： </p><p><img src="https://img-blog.csdn.net/20180307110141306?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>在这个图中，阈值选择了0.5于是左边的样本都被认为是负例，右边的样本都被认为是正例。可以看到，红色区域与蓝色区域是有重叠的，所以当阈值为0.5的时候，我们可以计算出准确率为90%。</p><p>好，现在我们来引入ROC曲线。 </p><p><img src="https://img-blog.csdn.net/2018030711171716?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>图中左上角就是ROC曲线，其中横轴就是前面说的**<code>FPR(False Positive Rate)</code><strong>，纵轴就是</strong><code>TPR(True Positive Rate)</code>**。 </p><p>然后我们选择不同的阈值时，就可以对应坐标系中一个点。</p><p><img src="https://img-blog.csdn.net/20180307112223804?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>当阈值为0.8时，对应上图箭头所指的点。</p><p><img src="https://img-blog.csdn.net/20180307112311768?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>当阈值为0.5时，对应上图箭头所指的点。</p><p>这样，不同的阈值对应不同的点。最后所有的点就可以连在一起形成一条曲线，就是ROC曲线。</p><p>现在我们来看看，如果蓝色区域与红色的区域发生变化，那么ROC曲线会怎么变呢？ </p><p><img src="https://img-blog.csdn.net/20180307112622811?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>上图中，蓝色区域与红色区域的重叠部分不多，所以可以看到ROC曲线距离左上角很近。</p><p><img src="https://img-blog.csdn.net/20180307112730574?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>但是，当蓝色区域与红色区域基本重叠时，ROC曲线就和接近y=x这条线了。</p><p>综上两个图，如果我们想要用ROC来评估分类器的分类质量，我们就可以通过计算AUC（ROC曲线下的面积）来评估了，这就是AUC的目的。</p><h2 id="为什么选择ROC和AUC？"><a href="#为什么选择ROC和AUC？" class="headerlink" title="为什么选择ROC和AUC？"></a>为什么选择ROC和AUC？</h2><p>其实，<strong>AUC表示的是正例排在负例前面的百分比。</strong> </p><p><img src="https://img-blog.csdn.net/20180306205648438?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>比如上图，第一个坐标系的AUC值表示，所有的正例都排在负例的前面。第二个AUC值，表示有百分之八十的正例排在负例的前面。</p><p><strong>我们知道阈值可以取不同，也就是说，分类的结果会受到阈值的影响。如果使用AUC的话，因为阈值变动考虑到了，所以评估的效果更好。</strong></p><p>另一个好处是，ROC曲线有一个很好的特性：<font color="red"><strong>当测试集中的正负样本分布发生变化了，ROC曲线可以保持不变。</strong></font>在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。</p><p><img src="https://img-blog.csdn.net/20180306210417620?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p><p>在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。</p><p>参考：<br><a href="https://www.zybuluo.com/frank-shaw/note/152851">https://www.zybuluo.com/frank-shaw/note/152851</a><br><a href="https://www.zhihu.com/question/39840928?from=profile_question_card">https://www.zhihu.com/question/39840928?from=profile_question_card</a><br><a href="http://blog.csdn.net/cherrylvlei/article/details/52958720">http://blog.csdn.net/cherrylvlei/article/details/52958720</a><br><a href="http://www.dataschool.io/roc-curves-and-auc-explained/">http://www.dataschool.io/roc-curves-and-auc-explained/</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> AUC </tag>
            
            <tag> ROC </tag>
            
            <tag> 混淆矩阵 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】homework 0</title>
      <link href="/2020/11/20/si-tan-fu-cs224w-tu-ji-qi-xue-xi-homework-0/"/>
      <url>/2020/11/20/si-tan-fu-cs224w-tu-ji-qi-xue-xi-homework-0/</url>
      
        <content type="html"><![CDATA[<pre><code class="python">import snap</code></pre><h2 id="Question-1-Analyzing-the-Wikipedia-voters-network-27-points"><a href="#Question-1-Analyzing-the-Wikipedia-voters-network-27-points" class="headerlink" title="Question 1. Analyzing the Wikipedia voters network [27 points]"></a>Question 1. Analyzing the Wikipedia voters network [27 points]</h2><p>Download the Wikipedia voting network wiki-Vote.txt.gz: <a href="http://snap.stanford.edu/data/wiki-Vote.html">http://snap.stanford.edu/data/wiki-Vote.html</a>.</p><p>Using one of the network analysis tools above, load the Wikipedia voting network. Note that Wikipedia is a directed network. Formally, we consider the Wikipedia network as a directed graph G = (V, E), with node set V and edge set E ⊂ V × V where (edges are ordered pairs of nodes). An edge (a, b) ∈ E means that user a voted on user b.</p><p>To make our questions clearer, we will use the following small graph as a running example: Gsmall = (Vsmall, Esmall), where Vsmall = {1, 2, 3} and Esmall = {(1, 2), (2, 1), (1, 3), (1, 1)}.</p><p>Compute and print out the following statistics for the wiki-Vote network:</p><h3 id="0-从txt中读取图模型"><a href="#0-从txt中读取图模型" class="headerlink" title="0. 从txt中读取图模型"></a>0. 从txt中读取图模型</h3><pre><code class="python">G = snap.LoadEdgeList(snap.PNGraph,"Wiki-Vote.txt",0,1)</code></pre><h3 id="1-The-number-of-nodes-in-the-network-Gsmall-has-3-nodes"><a href="#1-The-number-of-nodes-in-the-network-Gsmall-has-3-nodes" class="headerlink" title="1.    The number of nodes in the network. (Gsmall has 3 nodes.)"></a>1.    The number of nodes in the network. (Gsmall has 3 nodes.)</h3><pre><code class="python">numOfNodes = G.GetNodes()print("The number of nodes in the network is %d"%(numOfNodes))</code></pre><pre><code>The number of nodes in the network is 7115</code></pre><h3 id="2-The-number-of-nodes-with-a-self-edge-self-loop-i-e-the-number-of-nodes-a-∈-V-where-a-a-∈-E-Gsmall-has-1-self-edge"><a href="#2-The-number-of-nodes-with-a-self-edge-self-loop-i-e-the-number-of-nodes-a-∈-V-where-a-a-∈-E-Gsmall-has-1-self-edge" class="headerlink" title="2.    The number of nodes with a self-edge (self-loop), i.e., the number of nodes a ∈ V where (a, a) ∈ E. (Gsmall has 1 self-edge.)"></a>2.    The number of nodes with a self-edge (self-loop), i.e., the number of nodes a ∈ V where (a, a) ∈ E. (Gsmall has 1 self-edge.)</h3><pre><code class="python">numOfSelfLoopNodes = snap.CntSelfEdges(G)print("The number of nodes with a self-edge is %d"%(numOfSelfLoopNodes))</code></pre><pre><code>The number of nodes with a self-edge is 0</code></pre><h3 id="3-The-number-of-directed-edges-in-the-network-i-e-the-number-of-ordered-pairs-a-b-∈-E-for-which-a--b-Gsmall-has-3-directed-edges"><a href="#3-The-number-of-directed-edges-in-the-network-i-e-the-number-of-ordered-pairs-a-b-∈-E-for-which-a--b-Gsmall-has-3-directed-edges" class="headerlink" title="3.    The number of directed edges in the network, i.e., the number of ordered pairs (a, b) ∈ E for which a = b. (Gsmall has 3 directed edges.)"></a>3.    The number of directed edges in the network, i.e., the number of ordered pairs (a, b) ∈ E for which a = b. (Gsmall has 3 directed edges.)</h3><pre><code class="python">numOfEdges = snap.CntUniqDirEdges(G)print("The number of directed edges in the network is %d"%(numOfEdges))</code></pre><pre><code>The number of directed edges in the network is 103689</code></pre><h3 id="4-The-number-of-undirected-edges-in-the-network-i-e-the-number-of-unique-unordered-pairs-a-b-a--b-for-which-a-b-∈-E-or-b-a-∈-E-or-both-If-both-a-b-and-b-a-are-edges-this-counts-a-single-undirected-edge-Gsmall-has-2-undirected-edges"><a href="#4-The-number-of-undirected-edges-in-the-network-i-e-the-number-of-unique-unordered-pairs-a-b-a--b-for-which-a-b-∈-E-or-b-a-∈-E-or-both-If-both-a-b-and-b-a-are-edges-this-counts-a-single-undirected-edge-Gsmall-has-2-undirected-edges" class="headerlink" title="4.    The number of undirected edges in the network, i.e., the number of unique unordered pairs (a, b), a = b, for which (a, b) ∈ E or (b, a) ∈ E (or both). If both (a, b) and (b, a) are edges, this counts a single undirected edge. (Gsmall has 2 undirected edges.)"></a>4.    The number of undirected edges in the network, i.e., the number of unique unordered pairs (a, b), a = b, for which (a, b) ∈ E or (b, a) ∈ E (or both). If both (a, b) and (b, a) are edges, this counts a single undirected edge. (Gsmall has 2 undirected edges.)</h3><pre><code class="python">print("The number of undirected edges in the network is %d"%(snap.CntUniqUndirEdges(G)))</code></pre><pre><code>The number of undirected edges in the network is 100762</code></pre><h3 id="5-The-number-of-reciprocated-edges-in-the-network-i-e-the-number-of-unique-unordered-pairs-of-nodes-a-b-a--b-for-which-a-b-∈-E-and-b-a-∈-E-Gsmall-has-1-reciprocated-edge"><a href="#5-The-number-of-reciprocated-edges-in-the-network-i-e-the-number-of-unique-unordered-pairs-of-nodes-a-b-a--b-for-which-a-b-∈-E-and-b-a-∈-E-Gsmall-has-1-reciprocated-edge" class="headerlink" title="5.    The number of reciprocated edges in the network, i.e., the number of unique unordered pairs of nodes (a, b), a = b, for which (a, b) ∈ E and (b, a) ∈ E. (Gsmall has 1 reciprocated edge.)"></a>5.    The number of reciprocated edges in the network, i.e., the number of unique unordered pairs of nodes (a, b), a = b, for which (a, b) ∈ E and (b, a) ∈ E. (Gsmall has 1 reciprocated edge.)</h3><pre><code class="python">print("The number of reciprocated edges in the network is %d"%(snap.CntUniqBiDirEdges(G)))</code></pre><pre><code>The number of reciprocated edges in the network is 2927</code></pre><h3 id="6-The-number-of-nodes-of-zero-out-degree-Gsmall-has-1-node-with-zero-out-degree"><a href="#6-The-number-of-nodes-of-zero-out-degree-Gsmall-has-1-node-with-zero-out-degree" class="headerlink" title="6.    The number of nodes of zero out-degree. (Gsmall has 1 node with zero out-degree.)"></a>6.    The number of nodes of zero out-degree. (Gsmall has 1 node with zero out-degree.)</h3><pre><code class="python">snap.CntOutDegNodes(G,0)print("The number of nodes of zero out-degree is %d"%(snap.CntOutDegNodes(G,0)))</code></pre><pre><code>The number of nodes of zero out-degree is 1005</code></pre><h3 id="7-The-number-of-nodes-of-zero-in-degree-Gsmall-has-0-nodes-with-zero-in-degree"><a href="#7-The-number-of-nodes-of-zero-in-degree-Gsmall-has-0-nodes-with-zero-in-degree" class="headerlink" title="7.    The number of nodes of zero in-degree. (Gsmall has 0 nodes with zero in-degree.)"></a>7.    The number of nodes of zero in-degree. (Gsmall has 0 nodes with zero in-degree.)</h3><pre><code class="python">snap.CntInDegNodes(G,0)print("The number of nodes of zero in-degree is %d"%(snap.CntInDegNodes(G,0)))</code></pre><pre><code>The number of nodes of zero in-degree is 4734</code></pre><h3 id="8-The-number-of-nodes-with-more-than-10-outgoing-edges-out-degree-gt-10"><a href="#8-The-number-of-nodes-with-more-than-10-outgoing-edges-out-degree-gt-10" class="headerlink" title="8.    The number of nodes with more than 10 outgoing edges (out-degree > 10)."></a>8.    The number of nodes with more than 10 outgoing edges (out-degree &gt; 10).</h3><pre><code class="python">cnt = snap.TInt(0)for outDegree in range(0,11):    cnt += snap.CntOutDegNodes(G,outDegree)G.GetNodes()-cntprint("The number of nodes with more than 10 outgoing edges is %d"%(G.GetNodes()-cnt))</code></pre><pre><code>The number of nodes with more than 10 outgoing edges is 1612</code></pre><h3 id="9-The-number-of-nodes-with-fewer-than-10-incoming-edges-in-degree-lt-10"><a href="#9-The-number-of-nodes-with-fewer-than-10-incoming-edges-in-degree-lt-10" class="headerlink" title="9. The number of nodes with fewer than 10 incoming edges (in-degree < 10)."></a>9. The number of nodes with fewer than 10 incoming edges (in-degree &lt; 10).</h3><pre><code class="python">cnt = snap.TInt(0)for inDegree in range(0,10):    cnt += snap.CntInDegNodes(G,inDegree)cntprint("The number of nodes with fewer than 10 incoming edges is %d"%(cnt))</code></pre><pre><code>The number of nodes with fewer than 10 incoming edges is 5165</code></pre><p>关于第8小问和第9小问也可以像下面这样写：</p><pre><code class="python">DegToCntV = snap.TIntPrV()snap.GetOutDegCnt(G, DegToCntV)Out10_cnt = 0for item in DegToCntV:    if item.GetVal1() &gt; 10:        Out10_cnt += 1print('There are {} nodes having &gt;10 out-degree'.format(Out10_cnt))</code></pre><pre><code>There are 227 nodes having &gt;10 out-degree</code></pre><pre><code class="python">DegToCntV = snap.TIntPrV()snap.GetInDegCnt(G, DegToCntV)In10_cnt = 0for item in DegToCntV:    if item.GetVal1() &lt; 10:        In10_cnt += 1print('There are {} nodes having &lt;10 in-degree'.format(In10_cnt))</code></pre><pre><code>There are 10 nodes having &lt;10 in-degree</code></pre><h2 id="Question-2-Further-Analyzing-the-Wikipedia-voters-network-33-points"><a href="#Question-2-Further-Analyzing-the-Wikipedia-voters-network-33-points" class="headerlink" title="Question 2. Further Analyzing the Wikipedia voters network [33 points]"></a>Question 2. Further Analyzing the Wikipedia voters network [33 points]</h2><p>For this problem, we use the Wikipedia voters network. If you are using Python, you might<br>want to use NumPy, SciPy, and/or Matplotlib libraries.</p><ol><li>(18 points) Plot the distribution of out-degrees of nodes in the network on a log-log scale.<br>Each data point is a pair (x; y) where x is a positive integer and y is the number of nodes<br>in the network with out-degree equal to x. Restrict the range of x between the minimum<br>and maximum out-degrees. You may filter out data points with a 0 entry. For the log-log<br>scale, use base 10 for both x and y axes.</li></ol><pre><code class="python">OutDegV = snap.TIntPrV()snap.GetOutDegCnt(G,OutDegV)</code></pre><pre><code class="python"># 可以使用自带的方法绘制图像~snap.PlotOutDegDistr(G,"Q2-1","Directed graph - out-degree Distribution")</code></pre><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkwk9tqsggj20rs0m8glo.jpg"><ol start="2"><li>(15 points) Compute and plot the <code>least-square regression</code> line for the out-degree distribution in the log-log scale plot. Note we want to ﬁnd coeﬃcients $a$ and $b$ such that the function $log_{10} y = a · log_{10} x + b$, equivalently, $y = 10^b · x^a$, best ﬁts the out-degree distribution. What are the coeﬃcients a and b? For this part, you might want to use the method called <code>polyfit</code> in NumPy with <code>deg</code> parameter equal to 1.</li></ol><pre><code class="python">import numpy as np</code></pre><pre><code class="python"># 需要将OutDegV里面的数字对都取logx=[]y=[]for pair in OutDegV:    if(pair.GetVal1()&gt;0 and pair.GetVal2()&gt;0):#剔除0 entry        x.append(pair.GetVal1())        y.append(pair.GetVal2())logx=np.log10(x)logy=np.log10(y)#获取斜率和截距slope,intercept=np.polyfit(logx,logy,1)print("The best fit line is given by equation "      "log(y) = %s * log(x) + %s" % (slope, intercept))</code></pre><pre><code>The best fit line is given by equation log(y) = -1.2810647056745657 * log(x) + 3.1324547044999123</code></pre><pre><code class="python">#接下来就是绘制图像了import matplotlib.pyplot as plt</code></pre><pre><code class="python">predict=lambda x:10**(intercept)*x**slopefig = plt.plot(x,y,'bo--',              x,predict(x),'g',              markersize=2)[0]fig.axes.set_xscale('log')fig.axes.set_yscale('log')fig.axes.set_xlim(min(x),max(x))fig.axes.set_ylim(min(y),max(y))fig.axes.set_title("Log-Log Degree Distribution Plot And Fit for WikiGraph")fig.axes.set_xlabel("Node Degree")fig.axes.set_ylabel("Node Count")</code></pre><pre><code>Text(0, 0.5, 'Node Count')</code></pre><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkwrgxq7p5j20aw07ujrm.jpg" alt="output_36_1.png"></p><h2 id="Question-3-Finding-Experts-on-the-Java-Programming-Language-on-StackOverow-40-points"><a href="#Question-3-Finding-Experts-on-the-Java-Programming-Language-on-StackOverow-40-points" class="headerlink" title="Question 3. Finding Experts on the Java Programming Language on StackOverow [40 points]"></a>Question 3. Finding Experts on the Java Programming Language on StackOverow [40 points]</h2><p>Download the StackOverflow network stackoverflow-Java.txt.gz: <a href="http://snap.stanford.edu/class/cs224w-data/hw0/stackoverflow-Java.txt.gz">http://snap.stanford.edu/class/cs224w-data/hw0/stackoverflow-Java.txt.gz</a>. An edge (a; b) in the network<br>means that person a endorsed an answer from person b on a Java-related question.</p><p>Using one of the network analysis tools above, load the StackOverflow network. Note that StackOverflow is a directed network.</p><p>Compute and print out the following statistics for the stackoverflow-Java network:</p><pre><code class="python">GS = snap.LoadEdgeList(snap.PNGraph,"stackoverflow-Java.txt",0,1)</code></pre><ol><li>The number of weakly connected components in the network. This value can be calculated in Snap.py via function GetWccs.</li></ol><pre><code class="python">Components = snap.TCnComV()snap.GetWccs(GS,Components)print("There are %d weakly connected components in the network."%(Components.Len()))</code></pre><pre><code>There are 10143 weakly connected components in the network.</code></pre><ol start="2"><li>The number of edges and the number of nodes in the largest weakly connected component.The largest weakly connected component is calculated in Snap.py with function GetMxWcc.</li></ol><pre><code class="python">GS_MxWcc = snap.GetMxWcc(GS)print("There are %d edges in the largest weakly connected component."%(GS_MxWcc.GetEdges()))print("There are %d nodes in the largest weakly connected component."%(GS_MxWcc.GetNodes()))</code></pre><pre><code>There are 322486 edges in the largest weakly connected component.There are 131188 nodes in the largest weakly connected component.</code></pre><ol start="3"><li>IDs of the top 3 most central nodes in the network by PagePank scores. PageRank scores are calculated in Snap.py with function GetPageRank.</li></ol><pre><code class="python">PRankH=snap.TIntFltH()snap.GetPageRank(GS,PRankH)PRankH.SortByDat(False)cnt=1for key in PRankH:    if(cnt&gt;3):        break    print("The ID of Top %d most central nodes is %d"%(cnt,key))    cnt+=1</code></pre><pre><code>The ID of Top 1 most central nodes is 992484The ID of Top 2 most central nodes is 135152The ID of Top 3 most central nodes is 22656</code></pre><ol start="4"><li>IDs of the top 3 hubs and top 3 authorities in the network by HITS scores. HITS scores are calculated in Snap.py with function GetHits.</li></ol><pre><code class="python">NIdHubH = snap.TIntFltH()NIdAuthH = snap.TIntFltH()snap.GetHits(GS, NIdHubH, NIdAuthH)</code></pre><pre><code class="python">NIdHubH.SortByDat(False)cnt=1for key in NIdHubH:    if(cnt&gt;3):        break    print("The ID of Top %d hubs is %d"%(cnt,key))    cnt+=1</code></pre><pre><code>The ID of Top 1 hubs is 892029The ID of Top 2 hubs is 1194415The ID of Top 3 hubs is 359862</code></pre><pre><code class="python">NIdAuthH.SortByDat(False)cnt=1for key in NIdAuthH:    if(cnt&gt;3):        break    print("The ID of Top %d authorities is %d"%(cnt,key))    cnt+=1</code></pre><pre><code>The ID of Top 1 authorities is 22656The ID of Top 2 authorities is 157882The ID of Top 3 authorities is 571407</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://github.com/LFhase/Learning_CS224w/tree/master/Homework/HW0">LFhase/Learning_CS224w</a></li><li><a href="https://github.com/kandluis/cs224w">kandluis/cs224w</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】4. Community Structure in Networks</title>
      <link href="/2020/11/20/si-tan-fu-cs224w-tu-ji-qi-xue-xi-4-community-structure-in-networks/"/>
      <url>/2020/11/20/si-tan-fu-cs224w-tu-ji-qi-xue-xi-4-community-structure-in-networks/</url>
      
        <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvvoq03dwj21hv0g340q.jpg" alt="Community Structure in Networks"></p><blockquote><p>讲解两个知识点：</p><ol><li>community structure in networks（What）</li><li>how to detection of them（How）</li></ol></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyisykqj20u00mijvn.jpg"><p>回顾一下在lecture 3中学到的roles和communities的区别：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyixkkqj21qs1b3aw1.jpg"><p>承上启下，上节课讲解Roles，从大家的社会常识来看，不同的角色汇聚一起构成了社区（社会）community，这节课顺水推舟讲解communities</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyis448j20uk0mxwgq.jpg"><blockquote><p>上图是<strong>从community的角度</strong>来看待networks，感觉这样的方式才是我们最常用的吧，不然直接怼一张啥都没有的network给用户，用户会懵逼？所以，community之于network是非常重要的！</p></blockquote><p>艺术来源于生活，科研也是~我们的community观点也来自生活，下面来看看社会学是怎么给我们motivations？😮</p><h2 id="Motivations——From-Society"><a href="#Motivations——From-Society" class="headerlink" title="Motivations——From Society"></a>Motivations——From Society</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyir6q6j20u00migmm.jpg"><p><strong>网络其实是信息流（flow of information）</strong>，那么对于网络中信息流有一个基本问题：信息是如何在网络中流通（传播）的？比如咱们流言蜚语是怎么在我们的社交网络上散播开来的？</p><p>由于图是由node和edge构成，所以分解出的两个子问题：</p><ol><li>节点在信息流中到底扮演了哪些不同的角色？</li><li>不同的连接（比如长的或者短的）在信息流中扮演什么角色？</li></ol><p>这里通过一个实践问题来研究——人们怎么找到新工作？（这是Mark Granovetter, part of his PhD in 1960s在他博士论文中的部分工作）</p><p>在大家的认知里，应该是通过亲近的朋友那里可以获得更多的信息。<strong>通过研究发现：新工作的信息来源自acquaintances(一般认识的人），而不是close friends。</strong></p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyituhzj20u00migmh.jpg"><p>上图中提出从两种角度来考虑friendship：</p><ul><li>单纯从<strong>结构（也就是网络拓扑）</strong>上来说，将friendship看作一个边，能够跨越网络的不同部分；<ul><li>如上所说，我们可以得知friendship的structural role是Triadic Closure。</li></ul></li><li>考虑了<strong>人际关系</strong>的层面，我们知道人们之间的friendship有深厚和浅薄。</li></ul><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyixs26j20u00mi75b.jpg"><blockquote><p>Granovetter在friendship的两个角度之间架起了联系的桥梁：</p><pre><code class="text">从信息角度上来看：(1)长的edge可以将“较远”的人群和目前所在人群连接起来，从而获取更“不一样”的信息（找到新工作）；(2)而结构性强的连接（edges）在信息层面上就显得比较冗余。（这也是人们找到新工作来自接触较少的人的信息的原因所在）</code></pre></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyiyr5sj20u00miq3w.jpg"><blockquote><ul><li><p>所谓的Triadic closure可以简单地理解为两个人拥有一个common friend；</p></li><li><p><strong>结构上的三角闭合（Triadic closure) = 高聚合系数（high clustering coefficient）</strong></p><p>从我们平时的认知是可以比较直观的理解triadic closure的：如果B和C有一个共同朋友A，那么：</p><p>（1）B很可能会接触到C（在与A 的接触时间中）</p><p>（2）因为B、C有共同的朋友，所以会相互信任</p><p>（3）A很可能会介绍B和C认识</p><p>【一个来自Bearman and Moody的研究】表明：青少年女孩如果社交网络的聚合系数很低，则容易有自杀的倾向</p></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyiyu0dj20u00miaau.jpg"><p>they want to define the <strong>structural strength</strong> of a friendship,which is called <code>edge overlap</code>：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyj48a7j21e011itbr.jpg"><blockquote><p>从edge覆盖的定义：分子是i,j节点周围<strong>共同</strong>节点的集合（除去i,j两个node），分母是i,j周围所有node的集合（同样剔除i,j两个node）</p><p>所以从这个定义很显然，当i和j这两个node是两个小“block”的一个连接时候，覆盖=0（i和j的连接的这条edge看成是两个区域连接的一座桥），而Oij=1则意味相互的连接是很强的，同时这些连接可能也有点冗余。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyj3obnj21040r3wgs.jpg"><blockquote><p>先看蓝色的线，横坐标是打电话的次数（特别close的人，比如我和我妈妈，天天打电话，次数很多），电话次数越多，覆盖的值越大。那红色的线，是把网络中结构保持不变，而edge strength随机重分配后得到的线，是平的，也就是和打电话的次数无关。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyj76q6j20u00min26.jpg"><p>这里不同颜色表示打电话的次数，红色表示打的次数较多，可以看到红色的部分集中在一些小区域中，而其他（例如绿色和蓝色）的edges就表示弱很多的联系。那如果我们保持网络结构不变，但是随机的干扰edge strengths（随机扰动），会出现怎样的结果？（如下图）</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyjb197j20u00mitdo.jpg"><p>regardless of the network structure，随机赋值，这时候，红色的edges就不是像前面那个图只是集中在一些小区域了。</p><p>我们知道如果我们去掉两个cluster之间的那个连接，这两个部分就分开了，下面做这样的尝试：<strong>把edges从弱到强，和从强到弱连接的去除，去除后对这个网络的影响是怎样的</strong>（见下图）</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyj88bsj20u00mimzx.jpg"><p>当把那个弱连接的edge去掉后，网络中最大component的规模一下子就掉下去了，同样，<strong>去掉覆盖值小的edge，也得到类似的结果。</strong>：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyj9z29j20u00mi41c.jpg"><p>无论是最初的社会学研究，还是之后的电话网络，无不证明了我们可以以下面这种角度来看待我们的网络：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyjctyaj20uk0mx0vc.jpg"><h2 id="Network-Communities"><a href="#Network-Communities" class="headerlink" title="Network Communities"></a>Network Communities</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyjc6cpj20u00mi765.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyjmn8kj211s0scdip.jpg"><blockquote><ul><li>在这一章节中，communities，clusters，groups以及modules都是一个概念，老师会都用到。</li><li>前面介绍了这么多，那么现在来研究网络community，什么是community？那就是前面所说的：<font color="red"><strong>网络中内部联系很多（很强）而外部（网络的其余部分）联系较少（较弱）的nodes集合</strong></font></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyk6kvaj20zu0qwte1.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyjqhiij20u00mi77w.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyk4igtj20u00mi77w.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyldes1j20uu0minb0.jpg"><p><strong>所以我们需要学习的是最后的输出是（如下图：除了个别节点判断错误，其余都是正确）:</strong></p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyl2oaaj21360ttdu8.jpg"><p>下面进入正题：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdykdcn5j21400u0ad4.jpg"><blockquote><ul><li><p><strong>什么是社区？</strong></p><p>社区是紧密连接的节点集合</p></li><li><p><strong>什么是划分partition？</strong></p><p>一个整体划分成不同的不相交的子部分。（这和聚类的思想很接近）</p></li><li><p><strong>什么是Modularity Q？</strong></p><p>Q是用来衡量网络是否较好的被划分成多个社区的。</p></li><li><p><strong>那这个Q如何衡量划分是否够好呢？</strong></p><p>如上图的计算公式。组s内的edges数量和预期的edges数量之差，如果差别大，那么应该是分对组了。既然说到这个预期的edges数量，就需要定义一个空模型。</p></li></ul></blockquote><h3 id="Modularity"><a href="#Modularity" class="headerlink" title="Modularity"></a>Modularity</h3><p>我们要计算$Q$，所以我们需要一个Null Model，在这里它也是一个Configuration Model：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyk310rj20u00miwfl.jpg"><blockquote><ul><li><p><strong>什么是空模型？</strong></p><p>模型的定义其实就是为了“<strong>对比划分的结果和随机网络的差异”，</strong>给定一个m个连接，n个节点的网络G，构造一个重新分配的随机网络G’。</p></li><li><p>这里的G‘与lecture 3中的G’不相同，这里的G‘可以为<code>multigraph</code>；</p></li><li><p>老师解释了<code>expected number of edges</code>计算方法的正确性，是通过计算整体边的期望值证明的，具体可以参见iPad笔记。</p></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdykc9kij20u00mit9e.jpg"><blockquote><ul><li>我们之前是从cluster这样一个宏观的角度计算$Q$的，在上图中，我们细化到cluster里面的pair中，同时对$Q$进行正则化，保证它的范围在<code>[-1,1]</code>之间！！！（其中的数学看iPad笔记）</li><li>对于$A_{i,j}=1$这件事，我觉得并不太严谨。</li></ul></blockquote><p>这个计算公式太多求和符号，看上去不是很好处理，所以我们对其进行一个等价的变化：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdylazhyj21400u0whe.jpg"><p>这个modularity除了作为度量指标，其实还有一个非常大的用处，就是：<font color="red"><strong>对Q最大化——得到最好的社区划分。</strong></font></p><p>Modularity Q 可以用来衡量网络划分为社区的好坏程度，那么最重要的还是原始问题，到底怎么发现社区？</p><h2 id="Detecting-Non-Overlapping-Communities-Louvain-Algorithm"><a href="#Detecting-Non-Overlapping-Communities-Louvain-Algorithm" class="headerlink" title="Detecting Non-Overlapping Communities: Louvain Algorithm"></a>Detecting Non-Overlapping Communities: Louvain Algorithm</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdykmd0cj20u00mijt2.jpg"><blockquote><p><font color="red"><strong>Louvain Algorithm的核心思想：Maximize the modularity</strong></font></p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyl8kbpj219a0xzn16.jpg"><blockquote><ul><li>一种<strong>贪心算法</strong>，时间复杂度为$O(nlogn)$，可能是因为树形结构的原因~</li><li>支持带权图</li><li>a heuristic algorithm：具有启发式，探索的算法；</li><li>dendrogram：树枝状图</li><li>这个有点像<strong>分层聚类（hierarchical clustering）</strong>的味道，老师也说Community Detection也算是一种<strong>聚类任务</strong>！</li></ul></blockquote><p>算法的概述如下：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyl5bqrj22441l3gx5.jpg"><blockquote><ul><li><p>关于Phase 1，老师的解释是：</p><blockquote><p>if I move a node from it’s current community to another community，will the modularity increase？——keep moving every until I get stuck~</p></blockquote></li><li><p>Partitioning and restructuring</p></li><li><p>老师说这个贪心算法最后一定会收敛的。。。</p></li></ul></blockquote><h3 id="Phase-1-Partitioning"><a href="#Phase-1-Partitioning" class="headerlink" title="Phase 1: Partitioning"></a>Phase 1: Partitioning</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdylacfxj20u00mi0tw.jpg"><blockquote><ul><li>注意输出的结果跟操作时结点的顺序是相关的。</li></ul></blockquote><p>下面介绍一种全新的计算Modularity Gain的方法，它由两个部分构成，下面是第一部分：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyltjjyj21860x50xa.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyln7x7j21400u0tbq.jpg"><blockquote><p>我们以原来的Modularity of C来讲解一下式子是怎么得来的：<br>$$<br>\frac 1 {2m} \sum_{i \in C} \sum_{j \in C} (A_{i,j}-\frac {k_{i}k_{j}} {2m})\<br>= \frac 1 {2m} \sum_{i \in C}( (\sum_{j \in C} A_{i,j})-\frac {k_{i}} {2m}\sum_{j \in C} (k_{j}))\<br>=\frac 1 {2m} \sum_{i \in C}( (\sum_{j \in C} A_{i,j})-\frac {k_{i}} {2m}*\sum_{tot})\<br>=\frac 1 {2m} (\sum_{in} - (\frac {\sum_{tot}^2} {2m})) \<br>$$</p></blockquote><h3 id="Phase-2-Restructuring"><a href="#Phase-2-Restructuring" class="headerlink" title="Phase 2: Restructuring"></a>Phase 2: Restructuring</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyllx5cj20u00midgw.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdym9mxvj20ve0n545q.jpg"><blockquote><p>上面的伪代码描述的非常详细，可以细细地品读~</p></blockquote><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyn40xgj21720wbaoe.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyuyvqvj23rk2thu12.jpg"><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyluk4ej20u00mi3z6.jpg"><h2 id="Detecting-Overlapping-Communities-BigCLAM"><a href="#Detecting-Overlapping-Communities-BigCLAM" class="headerlink" title="Detecting Overlapping Communities: BigCLAM"></a>Detecting Overlapping Communities: BigCLAM</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdylyezjj20u00midi7.jpg"><h3 id="Non-Overlapping-vs-overlapping-communities"><a href="#Non-Overlapping-vs-overlapping-communities" class="headerlink" title="Non-Overlapping vs. overlapping communities"></a>Non-Overlapping vs. overlapping communities</h3><p>我们之前考虑的community都是要求每一个node只能从属于一个community，但是我们现实生活中的情景比这复杂多了，比如有的人拥有双国籍？🤔所以我们要考虑overlapping的情况！接下来就是讲解怎么得到具有overlapping性质的community~</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyo0c3yj21040r3du7.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyolnkej219u0yeqtt.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdynllwbj20x20otaqf.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdynn28dj21540wiqfh.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdynf6upj20u00min45.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyodj4pj21040r3q88.jpg"><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdynxy62j20u00migm8.jpg"><h3 id="Affiliation-Graph-Model-AGM"><a href="#Affiliation-Graph-Model-AGM" class="headerlink" title="Affiliation Graph Model(AGM)"></a>Affiliation Graph Model(AGM)</h3><blockquote><p><strong>another way to generate random graph</strong></p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyodbucj21900xracy.jpg"><blockquote><ul><li>如上图所示，这个模型包含两个层次：Community Affiliation和Graph，是通过Community Affiliation得到Graph~</li><li>模型的参数包括：<ul><li>Nodes $V$</li><li>Communities $C$</li><li>Memberships $M$</li><li>Each community $c$ has a single probability $p_c$：how likely nodes in this community to link to each other？</li></ul></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyosfz3j219k0y6tbo.jpg"><blockquote><p>上面的计算公式涉及到了概率论里面的计算技巧，正面很难算，那么从反面出发~</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyp9krbj20u00mijz4.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyoxxlwj20u00miadk.jpg"><p>接下来我们本末倒置，通过graph生成community affiliation，这样才是我们这节课的目标。</p><h3 id="Detecting-Communities"><a href="#Detecting-Communities" class="headerlink" title="Detecting Communities"></a>Detecting Communities</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyozugyj21hc140tck.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdypf5tuj22k81x6aky.jpg"><blockquote><ul><li>老师说概率$P(G|F)$可以理解为model $F$的<code>generated graph</code>与<code>real graph</code>$G$之间的相似度~但是，我还是想以概率论中MLE的概念进行理解；</li><li>优化式子有了，接下来就是逐个解决两个问题：<ul><li>$P(G|F)$的定义</li><li>怎么得到最大的$P(G|F)$</li></ul></li></ul></blockquote><h3 id="Graph-Likelihood-P-G-F"><a href="#Graph-Likelihood-P-G-F" class="headerlink" title="Graph Likelihood $P(G|F)$"></a>Graph Likelihood $P(G|F)$</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdypde3ij20u00miwfh.jpg"><h3 id="“Relaxing”-AGM-and-BigCLAM-Model"><a href="#“Relaxing”-AGM-and-BigCLAM-Model" class="headerlink" title="“Relaxing” AGM and BigCLAM Model"></a>“Relaxing” AGM and BigCLAM Model</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyph71fj212m0szdil.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdypgc5rj20u00miwg7.jpg"><blockquote><p>优化的步骤：</p><ol><li>随机选取一组$F$的参数；</li><li>每次固定其他结点，只有话一个结点；</li></ol></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdyppn8zj20u00mitbe.jpg"><blockquote><ul><li>这里的求梯度，好像涉及到了向量求导，喵喵喵~</li><li>最后Jure讲到的一点就是上面的这个求导的计算是很慢的，不过有一个Dynamic Programming的技巧，可以让这个求导的时间变成线性的，从而加快速度，使得这个算法能实际使用。</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdypjlg6j20u00mimzb.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdypoxu4j20u00miq44.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdypq7qbj20u00midh6.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gkvdypptinj20u00mi0tq.jpg"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/121710980">CS224W 4.1–Community Structure in Networks</a></li><li><a href="https://blog.csdn.net/infovisthinker/article/details/104724677">CS224W笔记-第四课</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】3. Motifs and Structural Roles in Networks</title>
      <link href="/2020/11/19/si-tan-fu-cs224w-tu-ji-qi-xue-xi-3-motifs-and-structural-roles-in-networks/"/>
      <url>/2020/11/19/si-tan-fu-cs224w-tu-ji-qi-xue-xi-3-motifs-and-structural-roles-in-networks/</url>
      
        <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkvdr3rgn1j21840r241r.jpg" alt="Motifs and Structural Roles in Networks"></p><blockquote><p>本节开篇从subgraph出发，指出subgraph是一个非常重要的概念，能够描述一个network的特征。于是，顺水推舟地讲解了两个利用到subgraph的度量指标：motifs and graphlets；</p><p>最后又讲解了如何提取network中每个node的structural roles；</p><p>但是很奇怪的是老师似乎并未将<code>motifs，graphlets</code>和<code>structural roles</code>之间的联系讲解清楚🤔</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku998qx23j20u00mi78l.jpg"><h2 id="前导知识：Subnetworks"><a href="#前导知识：Subnetworks" class="headerlink" title="前导知识：Subnetworks"></a>前导知识：Subnetworks</h2><h3 id="概念及其研究意义"><a href="#概念及其研究意义" class="headerlink" title="概念及其研究意义"></a>概念及其研究意义</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku998umnoj21ng18l7a5.jpg"><blockquote><ul><li>你可以想象一个network是像搭乐高一样，由一个一个subnetworks搭起来的。</li><li>从小至大的思想（或者称为分解思想）：<br>较大的问题=若干个小问题之和，<br>较难的问题=若干个简单的子问题之和<br>如同乐高</li><li><font color="red">subgraph的好处在于<strong>能够刻画和区分网络</strong>，所以它是十分有用的，这也是这节课的出发点所在。后面讲的<code>motif</code>和<code>graphlet</code>都是基于subgraph的！！！</font></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku998worpj20u00mi448.jpg"><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku998r56bj20u00mijsa.jpg"><blockquote><p><strong>non-isomorphic</strong>：非同构的</p></blockquote><h3 id="Subgraphs’-Significance"><a href="#Subgraphs’-Significance" class="headerlink" title="Subgraphs’ Significance"></a>Subgraphs’ Significance</h3><p>我们接下来会给每种类型的子图赋予上一个值，这个值刻画了这个子图在这个图中的重要性：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku998svq1j20u00miq3z.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku998zysmj212m0sz7kk.jpg"><p>在上图中：</p><ul><li>横坐标是前面提到的13个subgraph；</li><li>纵坐标是significance profile，比如对于第一个subgraph，它在Language networks是over-representation（表现充足），而在Web and social是under-representation（表现不足）；</li><li>右边表示的是在对应domain里面选择不同的networks，可以发现只要是在相同的domain里面，即使是不同的networks也表现出大致相同的significance profile。</li></ul><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999uibfj20vo0oek5y.jpg"><p>通过上面的例子，我们知道subgraph及其significance是非常重要的，那么如何计算significance呢？接下来我们来学习如何去计算significance。</p><p>下图是这节课的安排：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999n8ysj22vx25xn27.jpg"><blockquote><p>首先给出两个notion——motifs和graphlets，然后利用这两个notion来得到structural roles of nodes，最后讲解structural roles 的应用~</p></blockquote><h2 id="Motifs"><a href="#Motifs" class="headerlink" title="Motifs"></a>Motifs</h2><blockquote><p>motif的中文意思是模块</p></blockquote><h3 id="What-is-Motifs？"><a href="#What-is-Motifs？" class="headerlink" title="What is Motifs？"></a>What is Motifs？</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku9993ya1j20u00miaap.jpg"><blockquote><ul><li><strong>网络主题(Motifs)<strong>是网络中</strong>反复出现的重要的连接模式</strong>（recurring, significant patterns of interconnections ）（有点抽象~）</li><li>从上面的定义中，我们可以抽取3个重要部分：<ul><li><code>Pattern</code>：指<code>induced subgraph</code>，也就是离散数学中学的导出子图~</li><li><code>Recurring</code>：意味着出现的频率很高；</li><li><code>Significant</code>：这是一个通过比较出来的概念，需要一些图模型作为baseline~</li></ul></li><li>随机图的概念再次出现，后续可以发现随机图给我们研究现实网络提供了一个很好的参照作用</li></ul></blockquote><h4 id="Why-Do-We-Need-Motifs？"><a href="#Why-Do-We-Need-Motifs？" class="headerlink" title="Why Do We Need Motifs？"></a>Why Do We Need Motifs？</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99962sbj20u00midgs.jpg"><blockquote><p><strong>motif的用途：</strong></p><ol><li>弄明白网络如何工作；</li><li>在给定情境下预测网络的操作和反应</li></ol><p><strong>上图中展示了几种不同类型的motifs</strong>：</p><ul><li>Feed-forward loops ：老师说有点像ResNet中的skip connection</li><li>Parallel loops：在food web中发现的，说明这个motif在food web中是非常常见的！</li><li>Single-input modules：在基因控制网络中发现的，说明它在其中非常重要！</li></ul></blockquote><h3 id="Pattern-induced-subgraph"><a href="#Pattern-induced-subgraph" class="headerlink" title="Pattern: induced subgraph"></a>Pattern: induced subgraph</h3><p>下面讲解一下啥叫<code>induced subgraph</code>：</p><p>首先，我们事先定义了我们想要研究的一个motif——<code>aka motif</code>，然后在图中寻找，寻找的方式严格按照<strong>导出子图</strong>来定义：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999ncxvj20u00mi42v.jpg"><h3 id="Recurrence"><a href="#Recurrence" class="headerlink" title="Recurrence"></a>Recurrence</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999l56dj20ve0nrwfh.jpg"><blockquote><p>最重要的概念是允许<font color="red"><strong>overlap</strong></font></p></blockquote><h3 id="Significance"><a href="#Significance" class="headerlink" title="Significance"></a>Significance</h3><p>下图再次说明了significance的出现就是一个比较的概念！！！：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999mkjej20z00q9765.jpg"><blockquote><p>从上图中可以看出，在real network中feed-forward loop是比randomized networks更加over-representation的！</p></blockquote><hr><p><strong>既然说到significance是通过和随机图的对比来看出现频率的，那么如何转换为数学语言来表示这个significance？</strong></p><p>接下来是计算significance的算法：</p><h3 id="Significance计算方法📊"><a href="#Significance计算方法📊" class="headerlink" title="Significance计算方法📊"></a>Significance计算方法📊</h3><p>首先是计算z-score<a href="https://youtu.be/5S-Zfa-vOXs">^1</a>：</p><blockquote><p>z-score：# of $\sigma$s from $\mu$ for a particular data point.</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999mtscj20u00mit9l.jpg"><blockquote><ul><li>这个$SP_i$的公式是人为定义的，不必过于纠结。</li><li>注意$SP_i$的作用~这是算法继z-score后的第二次normalized！</li></ul></blockquote><p><strong>现在还有一个问题就是<code>randomized networks</code>是怎么生成的？</strong></p><h3 id="怎么生成Randomized-Networks？"><a href="#怎么生成Randomized-Networks？" class="headerlink" title="怎么生成Randomized Networks？"></a>怎么生成Randomized Networks？</h3><p>在第2节课中，我们学习了<code>Erdös-Rényi</code>随机图模型（我们知道这个随机图模型与现实中的网络还是有所差别的），所以我们想能不能认为对这个随机图加一些限制，使其更能够模拟现实网络的模型？</p><p>我们想增加的限制有：</p><ul><li><strong>$G^{rand}$具有和$G^{real}$相同点的数量</strong></li><li><strong>$G^{rand}$具有和$G^{real}$相同边的数量</strong></li><li><strong>$G^{rand}$中每个点具有和$G^{real}$相同的度数</strong></li></ul><h4 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm 1"></a>Algorithm 1</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999sijsj20u00mimy2.jpg"><blockquote><ul><li>steps：<ol><li>给定节点和其“触角”</li><li>随机配对</li><li>产生图</li></ol></li><li>注意上面的操作过程，最后的图中B只有3个度，老师说实验证明，这样的噪声结果不会太影响实验效果！即<code>ignore double edges and self-loops</code>对实验结果没影响！</li></ul></blockquote><h4 id="Algorithm-2"><a href="#Algorithm-2" class="headerlink" title="Algorithm 2"></a>Algorithm 2</h4><p>接下来是另外一种生成随机图的方法：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku999vml2j20u00migmq.jpg"><blockquote><ul><li><p>其实就是一个<code>cross</code>的操作~如果做了很多次这样的cross操作，那么就能够生成random graph。</p></li><li><p>老师说<code>cross</code>是操作非常耗时的，cross操作的起点是原图。</p></li><li><p><strong>其实建模随机图主要看你的问题是什么，老师说他这里只是考虑了度，你当然可以考虑更多，只要你认为是有意义的即可！</strong></p></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99brsqrj212m0szdw0.jpg"><p>上图使用的是switching的方式生成的random graph。</p><h3 id="Motif小节"><a href="#Motif小节" class="headerlink" title="Motif小节"></a>Motif小节</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99hbemyj24o23i24qp.jpg"><p>那么需要生成多少个随机图？—基本上是成千上万，甚至更多（也取决于真实图的大小）</p><p>其实motif是一个研究甚多的领域，比如它的定义就有很多的变体：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99a42xlj21ih14u78i.jpg"><blockquote><ul><li>canonical definition：规范定义</li></ul></blockquote><p>skip this：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99boi9vj20zk0qo47k.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99bhe5nj20yq0q2alm.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99ey8y9j251e3s2u0x.jpg"><h2 id="Graphlets"><a href="#Graphlets" class="headerlink" title="Graphlets"></a>Graphlets</h2><p>接下来讲解<code>motif</code>的一个扩展——<code>graphlets</code>。</p><p><code>motif</code>和<code>graphlet</code>的联系：</p><ul><li>我们使用<code>motif</code>来描述一个图；</li><li>而我们使用<code>graphlets</code>来描述每一个node；</li></ul><p>也就是说我们之前在讨论整个network的性质，现在我们讨论每个node的性质，主要的考虑是每个node的neighborhood。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99ajigmj20u00miwgn.jpg"><p><strong>Graphlet：连通的非同构子图</strong></p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99b7rgmj20u00mi7ak.jpg"><blockquote><ul><li><p>在上图中，注意其中的标号，每个标号代表一个新的点的位置，即后面会提到的<code>orbit</code>！</p></li><li><p>graphlet同样是induced subgraph</p></li><li><p>结合上图，我们的研究点就是：</p><p>如果我现在是一个node，那么我就要想，我在哪个graphlets上呀，我在哪个graphlets的哪个位置上呢？</p></li></ul></blockquote><h3 id="Graphlet-Degree-Vector（GDV）"><a href="#Graphlet-Degree-Vector（GDV）" class="headerlink" title="Graphlet Degree Vector（GDV）"></a>Graphlet Degree Vector（GDV）</h3><p>之前我们用<code>motif</code>得到了一个度量指标——<code>significance</code>，现在我们用<code>graphlets</code>来作为一个在节点层面的子图度量。</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99bewgrj20u00mi755.jpg"><ul><li>graphlets的作用就是作为计算<code>node-level subgraph metric</code>的输入。</li><li>通过类比Degree，得到Graphlet degree vector的概念：<ul><li>degree是一个节点上的边的个数</li><li>一个节点touch的graphlets的个数</li></ul></li></ul><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99cg2y1j215e0v2791.jpg"><p>注意是导出子图！！！</p><p><strong>所以graphlet度向量表示的是给定节点touch的给定轨迹的子图个数：</strong></p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99bsvdaj20u00mimye.jpg"><blockquote><p>1个73维的向量，实际描述的是1个node的邻居信息。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99bxjykj20u00mimyo.jpg"><p><strong>现在学习如何找motifs和graphlets：</strong></p><h2 id="Finding-Motifs-and-Graphlets"><a href="#Finding-Motifs-and-Graphlets" class="headerlink" title="Finding Motifs and Graphlets"></a>Finding Motifs and Graphlets</h2><p>这里涉及两个步骤：</p><p>（1）列举所有size-k连通的子图 </p><p>（2）数每一个子图类型出现的次数</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99c3tesj20u00mijsf.jpg"><blockquote><p>look at这两个步骤就可以看出来工作量很大</p><p>所以基本上<strong>可行的模块(motif)规模是比较小</strong>的：3–8</p></blockquote><h3 id="Counting-Subgraphs——ESU"><a href="#Counting-Subgraphs——ESU" class="headerlink" title="Counting Subgraphs——ESU"></a>Counting Subgraphs——ESU</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99ce6c3j20u00mi0tl.jpg"><blockquote><p>我们讲解ESU（exact subgraph enumeration）</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99cddwxj20u00miwfn.jpg"><ul><li><p>id大的原因在于不想重复之前产生的subgraph</p></li><li><p><code>be neighbored to some newly added node</code>的原因也是为了不做重复的事情！</p></li></ul><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99esiqsj22081ie1kx.jpg"><p>主要还是看下面的图说话：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99con5wj20u00mi0um.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99d1oc5j20u00mijsl.jpg"><p>如何counting呢？步骤如下：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99d2redj20u00mi0tv.jpg"><blockquote><ul><li><p>bijection：双射</p></li><li><p>在数个数这一个步骤存在一个问题：如何分类—即要把子图分为不同构的类型（同构的图属于一个类型）—用的是McKay的nauty algorihtm，具体参见以下网站</p><p><a href="https://link.zhihu.com/?target=http://users.cecs.anu.edu.au/~bdm/nauty/">https://link.zhihu.com/?target=http%3A//users.cecs.anu.edu.au/~bdm/nauty/</a></p></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99cy8n7j21400u0wgu.jpg"><blockquote><p>图G和图H是同构的，如果存在双射f，使得在G中相邻的节点在H中也是相邻的。</p><p>从定义上看检验两个图是否同构核心在于找到这个映射f，但是实际操作上等于要对每两两节点要去判断，计算量是很大的。</p></blockquote><h2 id="Structural-roles-in-networks"><a href="#Structural-roles-in-networks" class="headerlink" title="Structural roles in networks"></a>Structural roles in networks</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99fifktj264k4lf1bk.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99dhpb2j20u00mignj.jpg"><h3 id="What-are-Roles？"><a href="#What-are-Roles？" class="headerlink" title="What are Roles？"></a>What are Roles？</h3><p>关于<code>role</code>这个<code>idea</code>的来源也是来自于我们的生活的：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99n0dlbj22p820x1ky.jpg"><p>在接下来的讲解中，我们主要考虑3中role：</p><ul><li>center of stars： Are you a center of a star？</li><li>member of cliques： Are you a member of fully connected graph？</li><li>peripheral nodes：是否是外围节点？</li></ul><blockquote><p><strong>一个无向图的一个派系是指</strong>：这个无向图的顶点集有这样一个子顶点集，子顶点集里的任意两个顶点都有一条边相连（也即子顶点集中的任意两个顶点都是相邻的），那么这个子顶点集及其边构成的图就是这个无向图的一个派系。如果这个派系的顶点有k个，就称这个派系为<strong>k-派系（k-clique）</strong>。</p><p>其实也就是<strong>一个无向图里的一个子完全图就是这个无向图的一个派系</strong>。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99h56dnj23022927wh.jpg"><hr><p>我们首先来区分一下<code>role</code>和<code>groups</code>，虽然它们都是集合，但是它们还是有很大区别的！</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99dr58mj20u00migmp.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99e04h7j20u00mimy7.jpg"><p>这里要注意区分role（角色）和group的概念：</p><ul><li>role是根据在network中相似的功能来决定：例如公司中作为测试工程师的每个人，因为做着相似的工作所以扮演相同的role，可是在公司这个network中，这些人<strong>不一定互相连接。即role取决于相似性而不是相互连接性</strong></li><li>group/community则是互相连接的个体（节点），核心在于<strong>连接性</strong></li></ul><p>举个例子：学生、教师这是role，AI实验室、 Info实验室这是group/community</p><p>roles和groups是一种互补的概念</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99fo4zyj21qs1b3qpp.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99fcn50j20u00midgn.jpg"><p>结构等价性（structural equivalence)——两个节点称为结构等价的，如果它们和<strong>所有其他节点</strong>都有着相同的关系</p><blockquote><p>这是从社会网络中引用过来的一个概念</p></blockquote><p><strong>上面的定义很严格，要求的是all other nodes，必须要一模一样的nodes关系！</strong></p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99fk5roj20u00mit9n.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99gs9y6j264k4lfatj.jpg"><h3 id="RolX"><a href="#RolX" class="headerlink" title="RolX"></a>RolX</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99g3hblj20u00mijtm.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99gm2vvj20u00migmj.jpg"><p>那么怎么去发现网络中的roles？这里介绍RoIX</p><p>RoIX是一种<strong>无监督</strong>学习方式来自动探测网络中节点的结构角色，具备以下优点：</p><ul><li>无需先验信息</li><li>给每个节点分配<strong>a mixed-membership of roles</strong>（这个的最终体现就是一个概率分布）</li></ul><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99gvp6aj20u00mit9z.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99h05ilj20u00mimy7.jpg"><p>根据上图来分析：</p><p><strong>step 1：输入节点信息</strong></p><p><strong>step 2: 递归特征提取</strong></p><p><strong>step 3: 得到节点特征矩阵（例如度、平均权重等）</strong></p><p><strong>step 4: 提取role</strong></p><p><strong>step 5: 输出节点角色矩阵和角色特征矩阵</strong></p><h4 id="Recursive-Feature-Extraction"><a href="#Recursive-Feature-Extraction" class="headerlink" title="Recursive Feature Extraction"></a>Recursive Feature Extraction</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99hi84zj20u00mitbd.jpg"><ul><li><p>这里的<code>feature</code>列自左到右，所summarize的范围越来越大！local-&gt;egonet-&gt;recursive</p></li><li><p>egonet的定义：ego network is simply if u have a node it’s a network composed of the node its neighbors and the edges between them.</p></li></ul><p>具体的思想和算法如下：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99ho4y5j22r622en6c.jpg"><blockquote><p>递归特征提取的基本思想是聚合节点的特征，并使用它们生成新的递归特征。通过这种方式，我们可以将网络连接变成结构化的功能。</p><p>节点的邻域特征的基本集合包括：</p><ol><li>局部特征，它们都是节点度的度量。</li><li>Egonet功能是在节点的egonet上计算的，可能包括在egonet内的边数量以及进入/离开egonet的边数量。这里节点的egonet包括节点本身，其邻居和这些节点上的诱导子图中的任何边。</li></ol></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99hcg7mj20u00mi75h.jpg"><blockquote><ul><li>为了生成递归特征，首先我们从节点特征的基本集合开始，然后使用当前节点特征的集合来生成其他特征并迭代此操作。每次递归迭代时，可能的递归特征的数量呈指数增长。</li><li>graphlet  degree vector也可以作为feature</li></ul></blockquote><h4 id="Role-Extraction"><a href="#Role-Extraction" class="headerlink" title="Role Extraction"></a>Role Extraction</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99lsr0yj255k3v6u0x.jpg"><blockquote><p>RoleX使用的聚类算法是<code>non-negative matrix factorization</code>，这个算法老师在之后的课程会讲解。</p></blockquote><h2 id="Application：Structural-Similarity"><a href="#Application：Structural-Similarity" class="headerlink" title="Application：Structural Similarity"></a>Application：Structural Similarity</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99hrwdyj20u00mi0tu.jpg"><blockquote><p>注意得到的是一个distribution！</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99mdmbaj24vb3nox6p.jpg"><blockquote><p>Gray rectangle是排除其他三种节点之后的任意节点。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gku99l4d2wj264k4lfnel.jpg"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://youtu.be/5S-Zfa-vOXs">可汗学院——z-score</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w-图机器学习】Recitation: Snap.py and Google Cloud tutorial</title>
      <link href="/2020/11/18/si-tan-fu-cs224w-tu-ji-qi-xue-xi-recitation-snap-py-and-google-cloud-tutorial/"/>
      <url>/2020/11/18/si-tan-fu-cs224w-tu-ji-qi-xue-xi-recitation-snap-py-and-google-cloud-tutorial/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这节课主要讲解两个工具：snap.py和google cloud</p></blockquote><h2 id="Introduction-to-SNAP（Standford-Network-Analysis-Platform）"><a href="#Introduction-to-SNAP（Standford-Network-Analysis-Platform）" class="headerlink" title="Introduction to SNAP（Standford Network Analysis Platform）"></a>Introduction to SNAP（Standford Network Analysis Platform）</h2><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhy28vkj20u00migq5.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhxywwyj20u00mimxu.jpg"><p>上图是本节的outline。</p><h3 id="What‘s-SNAP？"><a href="#What‘s-SNAP？" class="headerlink" title="What‘s SNAP？"></a>What‘s SNAP？</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhy1tpvj20u00mijsq.jpg"><blockquote><ul><li><p>SNAP能够在大型网络上进行工作。</p></li><li><p>原始的SNAP是用C++写的，后来又用Python在C++的基础上套了一层壳，写了一个Python接口，其实底层还是C++实现的~这很重要！</p></li><li><p>自带了大量的网络数据集，便于我们学习；</p></li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhy180kj20u00miwfe.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhy53p4j20u00migmj.jpg"><p>下面是一些自带的网络数据集：</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhy3if2j20u00miab7.jpg"><h3 id="What‘s-Snap-py"><a href="#What‘s-Snap-py" class="headerlink" title="What‘s Snap.py?"></a>What‘s Snap.py?</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhy74bqj20u00miaas.jpg"><p>由于C++代码运行速度更快，而python对用户使用更友好，所以采用折中的方式~trade-off</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhy8rg6j20u00mit99.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyadbbj213g0tl45i.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhycwiej20u00mi74s.jpg"><h3 id="Snap-py-Tutorial"><a href="#Snap-py-Tutorial" class="headerlink" title="Snap.py Tutorial"></a>Snap.py Tutorial</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhye3ykj20u00miq3j.jpg"><p>关于snap.py的tutorial主要包括上面的部分，这节课也会简单的介绍这些东西。</p><blockquote><ol><li>基本数据类型</li><li>vectors，hash tables和pairs</li><li>图和网络</li><li>生成图</li><li>添加和遍历节点和边</li><li>保存和加载图</li><li>绘图和可视化</li></ol></blockquote><h4 id="Snap-py中的一些命名规定"><a href="#Snap-py中的一些命名规定" class="headerlink" title="Snap.py中的一些命名规定"></a>Snap.py中的一些命名规定</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhydkgaj20u00mijsh.jpg"><blockquote><ul><li>所有以<code>T</code>开头的变量，表示的是<code>class type</code>；</li><li>所有以<code>P</code>开头的变量，表示的是<code>graph object</code>；</li></ul></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyeupoj20u00mijsd.jpg"><blockquote><p>老师说这个库很好，他保证我们的作业里面的代码不会超过50行，hhh</p></blockquote><h4 id="Basic-Types-in-Snap-py"><a href="#Basic-Types-in-Snap-py" class="headerlink" title="Basic Types in Snap.py"></a>Basic Types in Snap.py</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyglv3j20u00mimya.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyjw9mj20u00mitdc.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyouv3j20u00miwiz.jpg"><h4 id="Vector-Types"><a href="#Vector-Types" class="headerlink" title="Vector Types"></a>Vector Types</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhykisej20u00miq3m.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyky2xj20u00mi74w.jpg"><h4 id="Hash-Type-Types"><a href="#Hash-Type-Types" class="headerlink" title="Hash Type Types"></a>Hash Type Types</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhym3ivj20u00midgm.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhypto7j20u00mi3z4.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhysa19j20u00mi0te.jpg"><h4 id="Pair-Types"><a href="#Pair-Types" class="headerlink" title="Pair Types"></a>Pair Types</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhys38bj20u00mimxp.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyst80j20u00mi74v.jpg"><h4 id="Basic-Graph-and-Networks-Classes"><a href="#Basic-Graph-and-Networks-Classes" class="headerlink" title="Basic Graph and Networks Classes"></a>Basic Graph and Networks Classes</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyuontj20u00mijsk.jpg"><blockquote><p>上图中给出了使用<code>T</code>和<code>P</code>的指南。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhywaeej20u00miwfj.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhyx55yj20u00miwf4.jpg"><h4 id="图的保存以及加载"><a href="#图的保存以及加载" class="headerlink" title="图的保存以及加载"></a>图的保存以及加载</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhz0lq4j20u00mimy2.jpg"><blockquote><p>根据老师的说法，text文件加载的速度要明显慢于binary文件，所以建议存储为binary文件</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhz096xj20u00midgd.jpg"><h4 id="Plotting-amp-Visualizing-in-Snap-py"><a href="#Plotting-amp-Visualizing-in-Snap-py" class="headerlink" title="Plotting &amp; Visualizing in Snap.py"></a>Plotting &amp; Visualizing in Snap.py</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhz5vbmj20u00miwf5.jpg"><blockquote><ul><li>使用<code>Gnuplot</code>来绘制图的<strong>各种属性</strong></li><li>使用<code>Graphviz</code>来<strong>绘制图的形状</strong></li></ul></blockquote><h4 id="Plotting-with-Snap-py——Gnuplot"><a href="#Plotting-with-Snap-py——Gnuplot" class="headerlink" title="Plotting with Snap.py——Gnuplot"></a>Plotting with Snap.py——Gnuplot</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhz1ui8j20u00mi751.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhz42saj21cc10940l.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzabsuj20u00migma.jpg"><blockquote><p>不知道为啥，我的电脑执行完函数之后，只会生成<code>.plt</code>和<code>.tab</code>文件，后面需要自己点积<code>plt</code>生成<code>.png</code>，就很神奇。</p></blockquote><h4 id="Drawing-Graphs——graphviz"><a href="#Drawing-Graphs——graphviz" class="headerlink" title="Drawing Graphs——graphviz"></a>Drawing Graphs——graphviz</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhz60c4j20u00mimxq.jpg"><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzccifj20u00mijst.jpg"><h4 id="Print-Graph-Information"><a href="#Print-Graph-Information" class="headerlink" title="Print Graph Information"></a>Print Graph Information</h4><blockquote><p>当我们在网络上找到一个图的数据集，我们想要知道这个图的一些properties，可以使用<code>PrintInfo()</code>函数来获得图的基本信息。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzddoij20u00mi0tp.jpg"><h4 id="Basic-Graph-Generators"><a href="#Basic-Graph-Generators" class="headerlink" title="Basic Graph Generators"></a>Basic Graph Generators</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzdqmcj214u0unq6g.jpg"><h4 id="Advanced-Graph-Generators"><a href="#Advanced-Graph-Generators" class="headerlink" title="Advanced Graph Generators"></a>Advanced Graph Generators</h4><blockquote><p>也可以生成一些高级的图形，其实我也不太懂。。。</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzku3lj21f412cn6f.jpg"><h4 id="Subgraphs-and-Conversions"><a href="#Subgraphs-and-Conversions" class="headerlink" title="Subgraphs and Conversions"></a>Subgraphs and Conversions</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzjb6lj20u00miq54.jpg"><h4 id="Connected-Components"><a href="#Connected-Components" class="headerlink" title="Connected Components"></a>Connected Components</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzl7xoj20u00miwfi.jpg"><h4 id="Node-Degrees"><a href="#Node-Degrees" class="headerlink" title="Node Degrees"></a>Node Degrees</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzoa7ej22bc1qidki.jpg"><h4 id="Node-Centrality"><a href="#Node-Centrality" class="headerlink" title="Node Centrality"></a>Node Centrality</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzplevj20u00mi74y.jpg"><h4 id="Triads-and-Clustering-Coefficient"><a href="#Triads-and-Clustering-Coefficient" class="headerlink" title="Triads and Clustering Coefficient"></a>Triads and Clustering Coefficient</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzyfjxj20u00migmo.jpg"><h4 id="Breadth-and-Depth-First-Search"><a href="#Breadth-and-Depth-First-Search" class="headerlink" title="Breadth and Depth First Search"></a>Breadth and Depth First Search</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzsp2xj20u00mimy6.jpg"><h4 id="Community-Detection"><a href="#Community-Detection" class="headerlink" title="Community Detection"></a>Community Detection</h4><blockquote><p>在sanp中实现了大多出名的community detection算法</p></blockquote><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfhzzlm5j20u00mimxz.jpg"><h4 id="Spectral-properties-of-a-Graph"><a href="#Spectral-properties-of-a-Graph" class="headerlink" title="Spectral properties of a Graph"></a>Spectral properties of a Graph</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfi021wfj20u00miab1.jpg"><h4 id="K-core-decomposition"><a href="#K-core-decomposition" class="headerlink" title="K-core decomposition"></a>K-core decomposition</h4><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gktfi02kh1j214k0ufgr6.jpg"><h2 id="Google-Cloud"><a href="#Google-Cloud" class="headerlink" title="Google Cloud"></a>Google Cloud</h2><p>中国的话，用不了。。。</p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w 图机器学习】2. Properties of Networks and Random Graph Models</title>
      <link href="/2020/11/16/si-tan-fu-cs224w-tu-ji-qi-xue-xi-2-properties-of-networks-and-random-graph-models/"/>
      <url>/2020/11/16/si-tan-fu-cs224w-tu-ji-qi-xue-xi-2-properties-of-networks-and-random-graph-models/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在本节课中，我们首先讲述4个关于网络的重要属性，它们是我们研究图的重要指标。然后介绍几种常见的随机图模型。</p></blockquote><h2 id="Measuring-Networks-via-Network-Properties"><a href="#Measuring-Networks-via-Network-Properties" class="headerlink" title="Measuring Networks via Network Properties"></a>Measuring Networks via Network Properties</h2><p>在本节中，我们将研究四个关键网络属性以表征图形：</p><ul><li><strong>度分布——$P(k)$</strong></li><li><strong>路径长度——$h$</strong></li><li><strong>聚类系数——$C$</strong> </li><li><strong>连接组件——$s$</strong></li></ul><p> 这些定义主要是针对无向图的，但可以轻松地将其扩展为有向图。</p><h3 id="Degree-Distribution"><a href="#Degree-Distribution" class="headerlink" title="Degree Distribution"></a>Degree Distribution</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gks9ixekdkj20t607u3yy.jpg" alt="度分布示意图"></p><p>度分布 $P(k)$ 表示随机选择一个节点，它具有度 $k$ 的概率是多少。对于一个已知的图 $G$ ，它的度分布可以通过归一化的直方图来概括，其中我们通过节点总数来归一化直方图。</p><p>我们可以通过 $P(k)=N_{k}/N$ 计算图的度分布。其中，$N_{k}$ 是度为 $k$ 的节点数，$N$ 为节点总数。可以将度分布视为随机选择的节点具有度 $k$ 的概率。</p><p>如果要将这些定义扩展为有向图，需要分别计算入度和出度的分布。</p><h3 id="Paths-in-a-Graph"><a href="#Paths-in-a-Graph" class="headerlink" title="Paths in a Graph"></a>Paths in a Graph</h3><h4 id="path"><a href="#path" class="headerlink" title="path"></a>path</h4><p><strong>路径</strong>是一系列节点，其中每个节点都链接到下一个节点：<br>$$<br>P_{n} = \lbrace {i_{0},i_{1},i_{2},…,i_{n}} \rbrace<br>$$<br>其中 $\lbrace{(i_{0},i_{1}),(i_{1},i_{2}),(i_{2},i_{3}),…(i_{n-1},i_{n})\rbrace} \in E$</p><h4 id="distance"><a href="#distance" class="headerlink" title="distance"></a>distance</h4><p>一对节点之间的<strong>距离</strong>(也称最短路径，geodesic)定义为沿着连接这对节点的最短路径的边数。 </p><blockquote><p>如果两个节点未连接，则距离通常定义为无限(或零)。 人们还可以将距离视为遍历从一个节点到另一个节点所需的最少节点数。</p></blockquote><p>在有向图中，路径需要遵循箭头的方向。 因此，有向图的距离不是对称的。 对于具有加权边的图，距离是从一个节点到另一个节点所需要遍历的最小边权重。</p><h4 id="Average-path-length"><a href="#Average-path-length" class="headerlink" title="Average path length"></a>Average path length</h4><p><strong>图的平均路径长度（Average path length）</strong>是所有连接的节点之间中最短路径的平均值。 我们将计算平均路径长度定义为 $$ \hat{h}=\frac{1}{2 E_{max }} \sum_{i, j \neq i} h_{i j} $$ 其中 $E_{max}$ 是边或节点对的最大数目；也就是说 $E_{max}=n(n-1)/2$ 和 $h_{ij}$ 是从 $i$ 节点到 $j$ 节点的距离。注意，我们仅计算连接的节点对上的平均路径长度，因此忽略了无限长度的路径。</p><h3 id="Clustering-Coefficient"><a href="#Clustering-Coefficient" class="headerlink" title="Clustering Coefficient"></a>Clustering Coefficient</h3><p>聚类系数(针对无向图)用于衡量节点 $i$ 的邻居相互联系的程度。</p><p>对于度数为 $k_{i}$的节点 $i$，我们计算聚类系数为 $$ C_{i}=\frac{2e_{i}}{k_{i}(k_{i}-1)} $$ 其中 $e_{i}$ 是节点 $i$ 的相邻节点之间的边数。 注意 $C_{i}\in[0,1]$ 。</p><blockquote><p> 此外，对于度数为0或1的节点，聚类系数是不确定的。</p></blockquote><p>同样，可以计算平均聚类系数为： $$ C=\frac{1}{N}\sum_{i}^{N}C_{i} $$ 平均聚类系数使我们能够看到边在网络的某些部分是否显得更加密集。</p><p>在社交网络中，平均聚类系数趋于很高，表明如我们期望的那样，朋友的朋友倾向于彼此认识。</p><h3 id="Connectivity-Connected-Components"><a href="#Connectivity-Connected-Components" class="headerlink" title="Connectivity/Connected Components"></a>Connectivity/Connected Components</h3><p>图的连通性指的是图中最大连通组件的大小。 </p><blockquote><p> 最大的连通组件是可以通过路径将任意两个顶点连接在一起的图的最大的集合。</p></blockquote><p>寻找连接的组件：</p><ol><li>从随机节点开始并执行广度优先搜索(BFS)</li><li>标记BFS访问的节点</li><li>如果访问了所有节点，则表明网络是连通的</li><li>否则，找到一个未访问的节点并重复BFS</li></ol><h2 id="A-real-world-network-example——MSN-Messenger"><a href="#A-real-world-network-example——MSN-Messenger" class="headerlink" title="A real-world network example——MSN Messenger"></a>A real-world network example——MSN Messenger</h2><p>下面是关于MSN Messenger这个数据集的描述：</p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gksa2270e7j20t00iywji.jpg"><p>既然是个图，则需要两个最重要的东西：</p><ul><li>Nodes——180M people</li><li>Edges——1.3B edges</li></ul><h3 id="Degree-Distribution-1"><a href="#Degree-Distribution-1" class="headerlink" title="Degree Distribution"></a>Degree Distribution</h3><p>通过计算得到下面的度分布：</p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gksa55v0hxj20rs0itdha.jpg" style="zoom:50%;"><p>我们可以看到，数据是紧紧地贴在坐标轴上的，这不太便于我们观察，我们使用logarithmic axis[^1]来绘制图像：</p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gksa833ewjj20s50jxwgv.jpg" style="zoom:50%;"><h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gksacdlot2j20q20jfwhc.jpg" style="zoom:50%;"><h3 id="Connected-Components"><a href="#Connected-Components" class="headerlink" title="Connected Components"></a>Connected Components</h3><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gksacdkxbjj20qm0iwjum.jpg" style="zoom:50%;"><h3 id="Diameter-of-WCC"><a href="#Diameter-of-WCC" class="headerlink" title="Diameter of WCC"></a>Diameter of WCC</h3><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gksacdn9xrj20v20o0dl8.jpg" style="zoom:50%;"><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gksdkw5l00j20u00miab5.jpg"><p><font color="red"><strong>我们得到了MSN的这些性质，那么这些性质是意料之中的呢？还是出乎意料的？此时，我们需要一个model来衡量，如果MSN的这些性质值与我们的model的性质值相同，那么就是意料之中的，没啥好研究，如果不同就是意料之外，值得我们研究~</strong></font></p><p>接下来，我们首先用Erdös-Rényi随机图模型来衡量一下：</p><h2 id="Erdos-Renyi随机图模型（Random-Graph-Model）"><a href="#Erdos-Renyi随机图模型（Random-Graph-Model）" class="headerlink" title="Erdös-Rényi随机图模型（Random Graph Model）"></a>Erdös-Rényi随机图模型（Random Graph Model）</h2><p><strong>Erdös-Rényi随机图模型是最简单的图模型。</strong> 这个简单的模型具有经过验证的网络属性，<strong>并且是比较实际现实世界图属性的良好基准。</strong></p><p>此随机图模型有两个变体：</p><ol><li>$G_{np}$: 具有 $n$ 个节点的无向图，并且每条边 $(u,v)$ 出现的概率符合概率为 $p$ 的独立同步分布；</li><li>$G_{nm}$: 具有n个节点的无向图，随机地均匀地选择 $m$ 条边；</li></ol><p>请注意，$G_{np}$ 和 $ G_{nm}$图不是唯一确定的，而是随机产生的。 每次生成图会产生不同的结果。</p><p>接下来，我们以$G_{np}$进行讲解，首先是计算它的度分布，距离，聚类系数和最大连接组件：</p><h3 id="Degree-Distribution-of-G-np"><a href="#Degree-Distribution-of-G-np" class="headerlink" title="Degree Distribution of $G_{np}$"></a>Degree Distribution of $G_{np}$</h3><p>由之前$G_{np}$ 的定义，我们可以很快地得出，对于$G_{np}$中的一个顶点，它的度分布符合<strong>二项分布</strong>。</p><p>设 $P(k)$ 表示某个顶点具有度为 $k$ 的概率，则<br>$$<br>P(k)=\begin{pmatrix} n-1 \ k \ \end{pmatrix} p^{k}(1-p)^{n-1-k}<br>$$<br>二项分布的均值和方差分别为 $\bar{k}=p(n-1)$，$\sigma^{2}=p(1-p)(n-1)$。</p><p>下面，我们提供了不同参数的二项分布图。<strong>注意，二项分布与高斯的离散形式类似，并且具有钟形。</strong></p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gksdkwcqqqj21dq0vh76a.jpg"><p>二项式分布的一个特性是，<strong>根据大数定律</strong>，随着网络规模的增加，分布变得越来越狭窄。 因此，我们越来越有信心，节点的度在 $\bar k$ 附近。 如果图具有无限个节点，则所有节点将具有相同的度数。</p><h3 id="The-Clustering-Coefficient-of-G-np"><a href="#The-Clustering-Coefficient-of-G-np" class="headerlink" title="The Clustering Coefficient of $G_{np}$"></a>The Clustering Coefficient of $G_{np}$</h3><p>回顾一下，聚类系数的计算公式为 $C_{i}=\frac{2e_{i}}{k_{i}(k_{i}-1)}$ 其中 $e_{i}$ 是节点 $i$ 的相邻节点之间的边数。因为在 $G_{np}$ 中边出现的概率符合概率为 $p$ 的独立同分布，所以图 $G_{np}$ 中期望的 $e_{i}$ 为 $$ \mathbb{E}\left[e_{i}\right]=p \frac{k_{i}\left(k_{i}-1\right)}{2} $$ 这是因为 $\frac {k_{i}(k_{i}-1)} {2}$ 是度为 $k_{i}$ 的节点 $i$ 的邻居的不同对的数量，并且每一对以概率 $p$ 相连接。</p><p>因此，期望的聚类系数为： $$ \mathbb{E}\left[C_{i}\right]=\frac{p \cdot k_{i}\left(k_{i}-1\right)}{k_{i}\left(k_{i}-1\right)}=p=\frac{\bar{k}}{n-1} \approx \frac{\bar{k}}{n} $$, $\bar{k}$ 表示平均度，从上面公式可以得到，$G_{np}$ 的聚类系数非常小，如果我们以固定的平均度生成 $\bar{k}$ 一个非常非常大的图，那么 $C$ 随着规模 $n$ 的增大而减小。$\mathbb{E}\left[C_{i}\right] \rightarrow 0$ as $n \rightarrow \infty$。</p><h3 id="The-Path-Length-of-G-np"><a href="#The-Path-Length-of-G-np" class="headerlink" title="The Path Length of $G_{np}$"></a>The Path Length of $G_{np}$</h3><p>讨论 $G_{np}$ 的路径长度，我们首先介绍<strong>扩展系数</strong>的概念。</p><p>图 $G(V,E)$ 对于 $\forall S \subset V$ 具有扩展系数 $\alpha$ ，剩下的边的数量 $S \geq \alpha \cdot \min (|S|,|V \backslash S|)$。</p><p>扩展系数回答了一个问题，即<font color="red"><strong>“如果我们随机选择一组节点，那么至少有多少条边要离开该组？”</strong> </font></p><p>扩展系数是一种鲁棒性的度量：要断开 $ℓ$ 个节点，必须切断 $≥α⋅ℓ$条边。</p><p>同样的，我们也可以认为图 $G(V,E)$ 具有扩展系数<br>$$<br>\alpha=\min_{S \subset V} \frac{ # \text { edges leaving } S}{\min (|S|,|V \backslash S|)}<br>$$<br>关于扩展系数的一个重要事实是：</p><p><strong>在具有 $n$ 个节点且扩展系数为 $α$ 的图中，对于所有的节点对，一定都会有长度为 $O((\log n)/\alpha)$ 的路径连接他们。</strong></p><p>对于一个随机的 $G_{np}$ 图，$\log n&gt;n p&gt;c$，所以，$diam(G_{np})=O(\log n/ \log(np))$。</p><blockquote><p>$np$表示的是average degree。</p></blockquote><p>因此，我们可以看到随机图具有良好的扩展性，因此BFS访问所有节点的步数为对数。</p><p><a href="https://camo.githubusercontent.com/67a08f6aecf7dc9d095853eee7db7680c84945b3b2e67cc4ebc1a76d748f6bf4/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30352f31372f4d36723245474e6a6b63316c735a562e706e67"><img src="https://camo.githubusercontent.com/67a08f6aecf7dc9d095853eee7db7680c84945b3b2e67cc4ebc1a76d748f6bf4/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30352f31372f4d36723245474e6a6b63316c735a562e706e67" alt="img"></a></p><p>因此 $G_{np}$ 的路径长度为 $O(\log n)$。从这个结果中，我们可以看到 $G_{np}$ 可以增长得很大，但是节点之间仍然相距几跳。</p><p><a href="https://camo.githubusercontent.com/aa139412c3a46d454199122ebf23ff8ebfe9fc6c4e4605e45610d09ed4c72718/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30352f31372f75776a4d4a437a48534f6b374265662e706e67"><img src="https://camo.githubusercontent.com/aa139412c3a46d454199122ebf23ff8ebfe9fc6c4e4605e45610d09ed4c72718/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30352f31372f75776a4d4a437a48534f6b374265662e706e67" alt="img"></a></p><h3 id="The-Connectivity-of-G-np"><a href="#The-Connectivity-of-G-np" class="headerlink" title="The Connectivity of $G_{np}$"></a>The Connectivity of $G_{np}$</h3><p>下图显示了随机图 $G_{np}$ 的演变。我们可以看到当平均度 $\bar{k}=2E/n$ 或 $p=\bar{k}/(n-1)$ 时，出现了一个巨大的连通组件。如果 $k=1-\epsilon$ ，则所有组件的大小均符合 $\Omega(\log n)$。如果 $\bar{k}=1+\epsilon$ ，则存在1个连通组件的大小为 $\Omega(n)$，而所有其他组件大小都为 $\Omega(\log n)$。换句话说，如果 $\bar{k}&gt;1$，我们希望具有一个大的连通组件。另外，在这种情况下，每个节点在期望中至少具有一条边。</p><p><a href="https://camo.githubusercontent.com/4335ec1544de8b2c16fe8eec834c3114b528f8ee2333400b88f50b5ee79e1045/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f4d7839614b5867713445637952376b2e706e67"><img src="https://camo.githubusercontent.com/4335ec1544de8b2c16fe8eec834c3114b528f8ee2333400b88f50b5ee79e1045/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f4d7839614b5867713445637952376b2e706e67" alt="evol_of_rand_grapg.png"></a></p><p>当聚合系数=0的时候：也就是没有edge，这时候是一个空图</p><p>当聚合系数=1度时候：就是一个“全连接”的图</p><p>那么当聚合系数从0变化到1的过程中发生了什么变化？</p><p>【当平均度=1的时候，因为k=p(n-1)，此时聚合系数c=p=1/（n-1），出现了一个大连接元，以此类推】</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>学习完了随机图中的四个重要属性，下面来看随机图的应用性如何，看看和MSN数据的对比</p><p><img src="https://pic2.zhimg.com/80/v2-de389244a2632105f93c9c5762215181_720w.jpg" alt="img"></p><ul><li><strong>之前说过度分布的直方图有利于判断图的结构，MSN和随机图的直方图差距还是很大的；</strong></li><li>平均路径：MSN和随机图的数据差不多</li><li><strong>平均聚合系数：差距很大，随机图的非常小</strong></li><li>最大连接元：很接近</li></ul><p><strong>综上来看，随机图的实际应用性如何？—-不好！</strong></p><p><img src="https://pic2.zhimg.com/80/v2-e88cb601fc14ca4d39ab1b8f3d65d9b5_720w.jpg" alt="img"></p><p>从上面的属性比较可以看出：实际上的网络并不是随机的。</p><p>那么问题来了，既然如此又为什么要学习随机图呢？<strong>因为这是最简单也是最有效的学习和评估网络的方法！</strong></p><blockquote><p><strong>随机性包罗万象，我们可以根据实际网络的特性来修改随机图来适应实际网络的需要</strong></p></blockquote><p><font color="red"><strong>那么，如何让随机图实际应用性变强呢？</strong></font></p><h2 id="The-Small-World-Model"><a href="#The-Small-World-Model" class="headerlink" title="The Small-World Model"></a>The Small-World Model</h2><h3 id="Analyzing-the-Properties-of-G-np"><a href="#Analyzing-the-Properties-of-G-np" class="headerlink" title="Analyzing the Properties of $G_{np}$"></a>Analyzing the Properties of $G_{np}$</h3><p>在网格网络中，我们实现了三角闭合和高聚类系数，但是平均路径长度较长。</p><p><a href="https://camo.githubusercontent.com/acb703a2a3dab41bfcd9905e0db3074708d6a7334eede2e62473ec58f9f0be09/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f4a78757a4358386f3669546b47416a2e706e67"><img src="https://camo.githubusercontent.com/acb703a2a3dab41bfcd9905e0db3074708d6a7334eede2e62473ec58f9f0be09/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f4a78757a4358386f3669546b47416a2e706e67" alt="grid_network.png"></a></p><p>在随机网络中，我们实现了较短的平均路径长度，但聚类系数较低。</p><p><a href="https://camo.githubusercontent.com/9b4d8e5c5911176896a017e1c0130c5ed29e8b105e54b9f53da15c5d0172ed66/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f73426c676e316b41536452786554372e706e67"><img src="https://camo.githubusercontent.com/9b4d8e5c5911176896a017e1c0130c5ed29e8b105e54b9f53da15c5d0172ed66/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f73426c676e316b41536452786554372e706e67" alt="random_network"></a></p><p>基于以上两个图结构，<strong>似乎不能直观地得到一个具有较短的平均路径长度，同时也具有较高的聚类系数的图</strong>。</p><p>但是，大多数现实世界网络具有如下表所示的属性，其中 $h$ 是平均最短路径长度，$c$ 是平均聚类系数，为了便于比较，随机图的平均度与实际网络相同。</p><table><thead><tr><th>网络类型</th><th><strong>$h_{actual}$</strong></th><th><strong>$h_{random}$</strong></th><th><strong>$c_{actual}$</strong></th><th><strong>$c_{random}$</strong></th></tr></thead><tbody><tr><td>电影演员</td><td>3.65</td><td>2.99</td><td>0.79</td><td>0.00027</td></tr><tr><td>电力网络</td><td>18.70</td><td>12.40</td><td>0.080</td><td>0.005</td></tr><tr><td>C.elegans</td><td>2.65</td><td>2.25</td><td>0.28</td><td>0.05</td></tr></tbody></table><p>同时满足以上标准的高聚类系数和小平均路径长度的网络（数学上定义为 $L\propto \log N$，其中 $L$ 是平均路径长度，$N$ 是网络中的节点的总数）称为小型世界网络。</p><h3 id="The-Small-World-Random-Graph-Model"><a href="#The-Small-World-Random-Graph-Model" class="headerlink" title="The Small World Random Graph Model"></a>The Small World Random Graph Model</h3><p>1998年，Duncan J. Watts和Steven Strogatz提出了一个模型，该模型用于构建具有高聚类和较短平均路径长度的网络。他们将此模型称为“小世界模型”。要创建这样的模型，我们采用以下步骤：</p><ol><li>从低维度的常规环开始，通过将每个节点连接到右侧的 $k$ 个邻居和左侧的 $k$ 个邻居， $k\geq2$ 。</li><li>通过将端点移动到随机选择的节点，以概率 $p$ 重新连接每条边(重新布线)。</li></ol><p><a href="https://camo.githubusercontent.com/127d2e2feb9b71c64c33b760d36fb24cfb737e66c66df8293f3a81e6482ba6bc/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f4d51596136755771453966414956352e706e67"><img src="https://camo.githubusercontent.com/127d2e2feb9b71c64c33b760d36fb24cfb737e66c66df8293f3a81e6482ba6bc/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f4d51596136755771453966414956352e706e67" alt="small_world"></a></p><p>然后，我们进行以下观察：</p><ul><li>在 $p=0$ 没有发生重新连接边的地方，这仍然是具有高簇集，大直径的网格网络。</li><li>对于 $0&lt;p&lt;1$ ，某些边缘已经进行了重新连线，但是大部分结构仍然保留。这意味着**(localoity)<strong>和</strong>(shortcuts)**。这允许高聚类和低直径。</li><li>对于 $p=1$，所有边缘都进行了随机重新连接，这是一个具有低聚类，低直径的Erdős-Rényi (ER)随机图。</li></ul><p><a href="https://camo.githubusercontent.com/b4f57cdf08a5142f7ccd194dd3754571b506760f5ff29556dc5e2018cfe70261/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f5147683342314954504436564561722e706e67"><img src="https://camo.githubusercontent.com/b4f57cdf08a5142f7ccd194dd3754571b506760f5ff29556dc5e2018cfe70261/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f5147683342314954504436564561722e706e67" alt="clustering_path.png"></a></p><p>小世界模型通过重新连接概率 $p \in[0,1]$ 来参数化。通过观察聚类系数和平均路径长度如何随 $p$ 的变化，我们看到平均路径长度随着 $p$ 增加而下降得更快，而聚类系数仍然相对较高。重新布线引入了shortcuts，这使得在结构保持相对坚固（高度聚类）的情况下，平均路径长度也可以减小。</p><p>从社交网络的角度来看，这种现象是直观的。虽然我们的大多数朋友都是本地人，但我们在不同国家/地区也有一些远距离的友谊，这足以使人类社交网络的直径崩溃，从而解释了流行的“六度分离”概念。</p><p>Watts-Strogatz小世界模型的两个局限性在于其度的分布与现实网络的幂律分布不匹配，并且由于假定了网络的大小，因此无法对网络的增长进行建模。</p><h2 id="The-Kronecker-Random-Graph-Model"><a href="#The-Kronecker-Random-Graph-Model" class="headerlink" title="The Kronecker Random Graph Model"></a>The Kronecker Random Graph Model</h2><blockquote><p>前面一直都是在讨论随机图，上一节还说到通过对随机图引入随机“捷径”可以将随机图变为small-world model，那么这部分来讲讲如何生成大的真实图。</p></blockquote><p>图生成的模型已被广泛研究。这些模型使我们能够在收集实际图困难时生成用于仿真和假设检验的图，并且还使我们可以检查生成模型应遵循的某些现实属性。</p><p>在制定图生成模型时，有两个重要的考虑因素。首先是生成现实网络的能力，其次是模型的数学易处理性，这允许对网络属性进行严格的分析。</p><p>Kronecker图模型是一个递归图生成模型，结合了数学易处理性和实际的静态和时态网络属性。Kronecker图模型的直观感受是<strong>自相似性</strong>，<strong>整体具有一个或多个部分的形状相同</strong>。</p><blockquote><p>比如不同的friendship的建立往往基于相同的文化、相似的爱好等等。</p></blockquote><p><a href="https://camo.githubusercontent.com/7e98d2ee551e9c6a455de04a064e2e571eaeb6d92b3bf4b056d844a6a214869b/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f395266654b57453264436f787038742e706e67"><img src="https://camo.githubusercontent.com/7e98d2ee551e9c6a455de04a064e2e571eaeb6d92b3bf4b056d844a6a214869b/68747470733a2f2f692e6c6f6c692e6e65742f323032302f30362f31322f395266654b57453264436f787038742e706e67" alt="community_growth.png"></a></p><p><strong>Kronecker积是一种非标准的矩阵运算，是一种生成自相似矩阵的方法。</strong></p><h3 id="The-Kronecker-Product"><a href="#The-Kronecker-Product" class="headerlink" title="The Kronecker Product"></a>The Kronecker Product</h3><p>Kronecker积使用 $\otimes$ 来表示。对于两个任意矩阵 $A \in \mathbb{R}^{m \times n}$ 和 $B \in \mathbb{R}^{p \times q}$， $A\otimes B \in \mathbb{R}^{mp \times nq}$，即： </p><p><img src="https://pic1.zhimg.com/80/v2-591620176d3353b64ceb49f066ff52c4_720w.jpg" alt="img"></p><p>例如，</p><img src="http://ww1.sinaimg.cn/mw690/9b63ed6fgy1gksg972cuuj20wv03mdga.jpg"><p>这个积的定义基本上大家在很多数学书上都有看到，这个积的结果是明显放大了原有的矩阵的阶。那么对于在图中的推广就是利用<strong>图的邻近矩阵来做kronecker积</strong></p><p><strong>那么什么是kronecker 图？</strong><br>答：初始图（初始邻近矩阵）的循环kronecker积。如下图，就是Kronecker图的示例：</p><p><img src="https://pic2.zhimg.com/80/v2-550e8a82ec12e4a14f84ea5939de3b15_720w.jpg"></p><p>这样的方法即自然又简单，但是呢，我们观察下面这个图：</p><p><img src="https://pic1.zhimg.com/80/v2-b214b8f270d62a0c7f05ddb943602908_720w.jpg"></p><pre><code class="text">会发现得到的图邻近矩阵的结构是很规则的（因为邻近矩阵的元素都是1或者0），类似于具有规则结构的网格一样，这样的当然好（利于我们分析），可是这样的图结构就很难capture真实复杂网络的信息了，那么要怎么办？-----答案是引入随机性！</code></pre><p>这里在初始矩阵引入随机性的意思是：放松初始矩阵–邻近矩阵只有0或者1元素的条件，而是可以有[0,1]之间的元素，也就是：</p><p>（1）初始矩阵的每个元素反应的是特定边出现的概率</p><p>（2）对初始矩阵进行Kronecker积运算，以获得较大的随机邻接矩阵，在该矩阵中，大型矩阵的每个元素数值再次给出了特定边出现在大图中的概率，<strong>这样的随机邻接矩阵定义了所有图的概率分布</strong></p><p><img src="https://pic2.zhimg.com/80/v2-b62431e95084e9100f9ddd49e111ede9_720w.jpg"></p><p>那么这个方法存在一个缺陷：费时间！</p><p>所以下面给出了一种快速方法—基于的思想：还是<strong>kronecker积的本质–循环性</strong></p><p><img src="https://pic4.zhimg.com/80/v2-f9c9f67ddb2be51bd3cf3e7551763e5f_720w.jpg"></p><p><img src="https://pic4.zhimg.com/80/v2-c5966adec966b4ba3ae4d05e9396e577_720w.jpg"></p><p><img src="https://pic4.zhimg.com/80/v2-bd8451f9d2bf576fbc5c5e7922f85667_720w.jpg"></p><p><strong>总结一下</strong>：随机kronerker图从数学上用公式来表示就是：</p><p><img src="https://pic4.zhimg.com/80/v2-fa33f6d94baa4384bb125d4eaea9df2f_720w.jpg"></p><p><img src="https://pic3.zhimg.com/80/v2-73348221a7c821bf5fb8623f896a42e6_720w.png"></p><p>这给了我们对随机Kronecker图的非常自然的解释：<strong>每个节点由一系列分类属性值或特征来描述。然后，两个节点链接的可能性取决于各个属性相似性的乘积。</strong></p><h3 id="Fast-Generation-of-Stochastic-Kronecker-Graphs"><a href="#Fast-Generation-of-Stochastic-Kronecker-Graphs" class="headerlink" title="Fast Generation of Stochastic Kronecker Graphs"></a>Fast Generation of Stochastic Kronecker Graphs</h3><p>存在一种快速启发式生成图形的过程，该过程所需时间随着边数量线性变化。</p><p>总体思路可以描述如下：对于每个边缘，我们以概率 $p_{uv} \in \Theta_1$递归地选择大随机矩阵的子区域，直到我们下降到大随机矩阵的单个单元为止。我们将边缘放置在那里。对于Kronecker图的 $k^{th}$ 幂 $\Theta_k$，将需要 $k$ 次下降步骤。</p><p>例如，考虑 $\Theta_1$ 是一个 $2 \times 2$ 的矩阵 $$ \Theta =\begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix} $$ 对于具有 $n=2^k$ 个节点的图 $G$</p><ul><li>创建归一化矩阵 $L_{uv}=\frac{p_{uv}}{\sum_{u,v}p_{uv}},\quad p_{uv} \in \Theta_1$</li><li>对于每个边缘：<ul><li>For $i=1 \dots k:$<ul><li>初始 $x=0,y=0$</li><li>以概率 $L_{uv}$ 选择行和列</li><li>下降到 $G$ 的第 $i$ 级象限$(u,v)$<ul><li>$x=x+u \cdot 2^{k-1}$</li><li>$y=y+v \cdot 2^{k-1}$</li></ul></li><li>将边 $(x,y)$ 添加到 $G$</li></ul></li></ul></li></ul><p>如果 $k=3$，且对于每一步 $i$，选择象限 $b_{(0,1)},c_{(0,1)},d_{(0,1)}$ 分别基于 $L$ 的归一化概率，有 $$ x=0\cdot 2^{3-1}+1\cdot 2^{3-2}+1\cdot 2^{3-3}=0\cdot 2^2+1\cdot 2^1+1\cdot2^{0}=3\ y=1\cdot 2^{3-1}+0\cdot 2^{3-2}+1\cdot 2^{3-3}=1\cdot 2^2+0\cdot 2^1+1\cdot2^{0}=5 $$ 因此，我们将边$(3,5)$添加到图中。</p><p>在实践中，随机Kronecker图模型能够生成与现实网络的属性非常匹配的图。要阅读有关Kronecker图模型的更多信息，请参阅 *J Leskovec et al., Kronecker Graphs: An Approach to Modeling Networks (2010)*。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[^1]:<a href="https://www.khanacademy.org/math/algebra-home/alg-exp-and-log/alg-logarithmic-scale/v/logarithmic-scale">关于logarithmic scale/axis的理解</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【斯坦福cs224w 图机器学习】1. Introduction; Structure of Graphs</title>
      <link href="/2020/11/14/si-tan-fu-cs224w-tu-ji-qi-xue-xi-1-introduction-structure-of-graphs/"/>
      <url>/2020/11/14/si-tan-fu-cs224w-tu-ji-qi-xue-xi-1-introduction-structure-of-graphs/</url>
      
        <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>两种常见的关于图的分类：</p><p><img src="https://pic3.zhimg.com/80/v2-6e8c501e83e7e9d8fe52c9e9a10cd442_720w.jpg" alt="概念的区分"></p><ol><li><p>Networks (also known as Natural Graphs)</p><p>其实就是我们实际生活中会遇到的真实的图，比如社会人际关系、基因组、我们的想法</p><p><strong>本质上这些是给定了一个domain，上面的所有信息可以建立一个networks，我们好利用networks/graphs更好的理解这个domain</strong></p></li><li><p>information graphs</p><p>这一类更关心的是<strong>各个个体之间的联系</strong>，从而可以来做分类，预测等任务。</p></li></ol><p><img src="https://pic4.zhimg.com/80/v2-320eddd58a088fafc6c37a0894a0e01b_720w.jpg"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr8lz76b7j20nu0hswkj.jpg"></p><p><strong>研究graph的目的/意义：</strong></p><p><img src="https://pic3.zhimg.com/80/v2-f29fb4d3d1e1af099b007729fb55b1ca_720w.jpg" alt="img"></p><blockquote><ul><li>model就是之前说的natural graph，而predict就是之前说的information graph。</li><li>举一个笔者个人认为很重要的现实意义：目前新型冠状病毒，因为春运的原因，人员走动非常的广，没有办法人工去追踪，那么这时候利用大数据建立一个social networks，可以较好的将所有潜在的感染者找出来，从而切断传染源。</li></ul></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr8o8hi0dj20nf0gtjt9.jpg"></p><h2 id="Networks-and-Application"><a href="#Networks-and-Application" class="headerlink" title="Networks and Application"></a>Networks and Application</h2><blockquote><p>这一节主要讲解一些常见的networks和这些networks的应用</p></blockquote><p>我们分析networks，主要分析哪些方面呢？也就是研究者们主要研究哪些方面呢？</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr8q00wpij20nh0eimys.jpg"></p><table><thead><tr><th>Networks</th><th>Application</th></tr></thead><tbody><tr><td>Social Networks</td><td>Social Circle Detection</td></tr><tr><td>Infrastructure Networks</td><td>Aug 15，2003 blackout</td></tr><tr><td>Knowledge Networks</td><td>Link Prediction</td></tr><tr><td>Online Media</td><td>Polarization on Twitter、Misinformation、Predicting Virality、Product Adoption</td></tr><tr><td>Biomedicine</td><td>Side effects</td></tr></tbody></table><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr8xzjcd1j20om0hw0va.jpg" alt="Embedding Nodes"></p><p>what are the images that are nearby in this embedding space？接下来的五节课我们都会学习怎么来映射到高维空间中。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr8ypkgdoj20ob0bjgrq.jpg" alt="比较1"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr8z78btej20oe09wq8u.jpg" alt="比较2"></p><p>上面的结果是只是用了image feature，没有使用graph。而下面的是使用了graph的。明显看到下面的结果会更好。 </p><h2 id="Course-Outline"><a href="#Course-Outline" class="headerlink" title="Course Outline"></a>Course Outline</h2><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr929btt6j20p809madz.jpg" alt="course outline"></p><ul><li>蓝色部分：Algorithm for analyzing networks</li><li>黄色部分：Statistical machine learning on networks</li><li>绿色部分：public applications and lectures focused on the use cases those applications</li></ul><h2 id="Structure-of-Graphs"><a href="#Structure-of-Graphs" class="headerlink" title="Structure of Graphs"></a>Structure of Graphs</h2><p>怎么描述一个网络？首先理解图的基本单元有哪些：</p><p><img src="https://pic4.zhimg.com/80/v2-3558d0afbab4fc445f7807aef2b54c9b_720w.jpg">Network和Graphs是在课程中是不同的概念。但是大家也不要太在意这些区别：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr944y46bj20mz0hngne.jpg" alt="Networks or Graphs"></p><h3 id="How-do-you-define-a-networks"><a href="#How-do-you-define-a-networks" class="headerlink" title="How do you define a networks?"></a>How do you define a networks?</h3><p>其实是network的一种数学表示</p><ul><li>networks可以视为一种通用的语言，用来描述不同domain下个体之间的联系。</li><li>根据什么样的属性来考虑个体之间的联系，就称为xxxnetwork（xxx表示的就是你基于的联系属性）。</li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr963wl50j20nv0hxjtb.jpg"></p><h3 id="Choice-of-Network-Representation"><a href="#Choice-of-Network-Representation" class="headerlink" title="Choice of Network Representation"></a>Choice of Network Representation</h3><p><img src="https://pic3.zhimg.com/80/v2-d90b9beaf3c2687ade99cbba0dc24cf2_720w.jpg" alt="img"></p><ul><li>undirected graphs（无向图）：比如像微博上，我关注了你，你并没有关注我或者你也关注了我。即互相之间的关系是无所谓主从关系</li><li>directed graphs（有向图）： 电话-一定有一人拨打电话和另一断接听电话，是明确知道方向，我打给了你和你打给我是不一样的箭头方向</li></ul><p><img src="https://pic1.zhimg.com/80/v2-1acda607b157e2c719d51cbff4ed7d84_720w.jpg" alt="img"></p><p>节点的度：（1）在undirected graph中这个度表明的就是这个节点连接的其他节点数量</p><pre><code class="text">我们也可以理解为微博中，你的粉丝数量+关注数量-互粉数量</code></pre><p>（2）在directed graph中这个度反映的就更加精准：分为in-degree和out-degree</p><pre><code class="text">in-degree可以理解为粉丝数量，out-degree可以理解为关注数量（你关注别人）。注意到这里ppt上的C是没有双向箭头的。</code></pre><p>完全图</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr98pxrarj20nl0dmju9.jpg" alt="完全图"></p><p>二分图</p><p><img src="https://pic4.zhimg.com/80/v2-154ea94eaf4755a6655ebbe44e47485b_720w.jpg" alt="img"></p><p>这种图结构很有用：</p><ul><li><p>当你的nodes是不同的类型的时候，按照类型分类(disjoint sets)。如作者作为U类，论文作为V类。</p></li><li><p>同时二分图也可以衍生出新的图</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9a8kx6nj20nv0giae9.jpg" alt="新图"></p></li></ul><h3 id="怎么表示-存储一个图？"><a href="#怎么表示-存储一个图？" class="headerlink" title="怎么表示/存储一个图？"></a>怎么表示/存储一个图？</h3><h4 id="Adjacency-Matrix"><a href="#Adjacency-Matrix" class="headerlink" title="Adjacency Matrix"></a>Adjacency Matrix</h4><p>通俗的理解就是将每个节点之间是否存在连接（1 or 0)通过矩阵形式表示出来</p><p><img src="https://pic2.zhimg.com/80/v2-26feec271b1a0cfa7973b19ae9b02411_720w.jpg" alt="img"></p><p>我们可以试一下将0表示为白色，1表示为黑色点将邻近矩阵画出来：可以发现邻近矩阵是<strong>稀疏</strong>的</p><p><img src="https://pic2.zhimg.com/80/v2-e2f5830d4470cea1a121fdd34eb91ad1_720w.jpg" alt="img"></p><p><strong>note</strong></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9dww9k0j20lf0ctq4t.jpg"></p><p><strong>现实中大部分networks都是稀疏的（稀疏–非常好的性质）</strong></p><pre><code class="text">这样我们把graph表示成矩阵-稀疏矩阵，所占内存将减少很多</code></pre><h4 id="Edge-List"><a href="#Edge-List" class="headerlink" title="Edge List"></a>Edge List</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9ckish3j20lq0bpjs0.jpg"></p><h4 id="Adjacency-List"><a href="#Adjacency-List" class="headerlink" title="Adjacency List"></a>Adjacency List</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9d0q1ejj20n60dugme.jpg"></p><h3 id="Edge-Attributes"><a href="#Edge-Attributes" class="headerlink" title="Edge Attributes"></a>Edge Attributes</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9erg717j20mc0a70ti.jpg"></p><p>下面是示例：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9g6ae27j20no0eqq4l.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9g6bzu0j20o00extai.jpg"></p><h3 id="连通性"><a href="#连通性" class="headerlink" title="连通性"></a>连通性</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9lqosk6j20p50e70uc.jpg"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9m341lyj20mf0f4wj2.jpg"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9mbmmlqj20na0e5mym.jpg"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9n0b8bzj20mr0edq4p.jpg"></p><h3 id="Network-Representations"><a href="#Network-Representations" class="headerlink" title="Network Representations"></a>Network Representations</h3><p>不同的network具有不同的graph：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkr9o1oorbj20oe0dgt9t.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
            <tag> 课程笔记 </tag>
            
            <tag> cs224w </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Heterogeneous Graph Neural Networks for Malicious Account Detection</title>
      <link href="/2020/11/13/heterogeneous-graph-neural-networks-for-malicious-account-detection/"/>
      <url>/2020/11/13/heterogeneous-graph-neural-networks-for-malicious-account-detection/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出了<strong>GEM（Graph Embedding for Malicious accounts）模型</strong>，是一个异质图神经网络方法，用于支付宝恶意账户的检测。</p><p>本文的方法受到<strong>连通子图方法</strong>的启发，基于攻击者的两个基本弱点（device aggregation和activity aggregation），从异质的<strong>账户-设备</strong>（account-device）图中自适应地学习到embedding。</p><p>使用<strong>注意力机制</strong>，为不同类型的节点分配不同的注意力。聚合每种节点的信息时，使用的是求和的方式。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>要能检测出恶意账户，首先要研究恶意账户的攻击<strong>特征</strong>。现有的研究主要从三个方面展开：</p><ol><li>基于规则的方法：使用复杂的规则，识别恶意账户</li><li>基于图的方法：考虑用户之间的关联，恶意账户和正常账户之间有关联</li><li>基于机器学习的方法：利用大量的历史数据，建立统计模型</li></ol><p>攻击者的攻击策略是会不断变化的，所以需要一个能够适应这种变化的检测系统。</p><p>作者总结了来自攻击者的两个主要特征：</p><ol><li><p>设备聚集（device aggregation）</p><p>攻击者要承受计算资源带来的成本，所以大多数攻击者只在少数计算资源上注册或频繁地登录。</p></li><li><p>行为聚集（activity aggregation）</p><p>攻击者受攻击时间的限制，通常要在很短的时间内完成既定目标，所以恶意账户的行为可能在有限的时间内爆发。</p></li></ol><p>虽然我们已经广泛地分析了攻击者的弱点，但保证识别的高准确率和高召回率还是非常具有挑战性的。</p><p>现有的方法通常假阳率（FP，模型判断是恶意账户，实际上不是）很低，也就是假阴率（FN，模型判断不是恶意账户，实际上是）很高。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gknv7smwwzj20m203odfx.jpg" alt="准确率，召回率与FP，FN之间的关系"></p><p>这样虽然对用户友好，避免误伤，但是可能<strong>会错过识别出更多可疑账户的机会</strong>。产生这种情况的原因在于大量的良性账户和少量的可以账户交织在一起，形成了<strong>低信噪比</strong>。</p><p>因此，在不同设备构成的异构图中同时考虑”设备聚集”和”行为聚集”是很重要的。</p><blockquote><p>Representation Learning  on Graphs: Methods and Applications.</p></blockquote><p>本文提出**GEM模型(Graph Embeddings for Malicious accounts)**，同时考虑了异质图中的“设备聚集”和“行为聚集”，是一种基于图网络的图表示学习方法。</p><p>本文提出的方法本质上是对异质的account-device图进行建模，同时考虑了局部结构中账户的行为特征。</p><p>模型的基本思想是：账户是正常的还是恶意的，取决于其他账户是如何通过设备与该设备聚集的，以及那些与该账户共享同一设备的账户的行为表现是什么样子的。</p><p>本文的贡献如下：</p><ol><li>提出基于图表示学习方法的神经网络，同时关注攻击者“设备聚集”和“行为聚集”两个特点，以实现对恶意账户的检测。是第一个使用GNN方法进行欺诈检测的工作；</li><li>本文的模型已在支付宝中应用，每天可以有效检测出上万的恶意账户。</li></ol><h2 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2 预备知识"></a>2 预备知识</h2><p>在本节中，我们首先简要介绍最近发展起来的图表示学习技术。（GNN只是图表示学习的一种技术罢了）</p><h3 id="2-1-图神经网络"><a href="#2-1-图神经网络" class="headerlink" title="2.1 图神经网络"></a>2.1 图神经网络</h3><p>第一类技术与在图、图的边或图的节点上预测标签有关。</p><p>Kipf提出的GCN是在一阶邻居上进行卷积。$X\in R^{N,D}$是节点特征向量$x_i\in R^D$组成的矩阵。无向图$G=(V,E)$,有$N$个节点$v_i\in V$，边$ (v_i,v_j)\in E$，邻接矩阵为$A\in R^{N\times N}$。卷积层计算如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gknw9f5nwtj20m902jdfn.jpg" alt="卷积核"></p><p>其中$\tilde{A}$是$A$添加self-loops后对称归一化(symmetric normalization )的结果：$\tilde{A}=\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$, $\hat{A}=A+I$,$\hat{D}$是$\hat{A}$中节点度的对角矩阵。</p><blockquote><p>关于GCN的解释可以参考这位大佬的博客，总结的非常好：<br><a href="https://blog.csdn.net/yyl424525/article/details/100058264">图卷积网络 GCN Graph Convolutional Network（谱域GCN）的理解和详细推导-持续更新</a></p></blockquote><p>GCN学习到了函数$f(X,A)$，使用$A$中节点$v_i$的邻居信息表示该节点。</p><blockquote><p>文章中还介绍了两个GNN的工作</p></blockquote><p>总之，GNN的工作可以看成是<strong>递归地聚合邻居信息</strong>的方法：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gknwf748uzj20ma024wei.jpg" alt="undefined"></p><p>GNN中大多数的工作都是围绕”<strong>感受野(receptive fileds)</strong>“的研究，也就是进行聚合的范围。图结构的数据是非欧式的，每个节点的邻居数目不确定，不像图像数据每个像素点就只有8个邻居。</p><p>有学者提出了GeniePath，可以自适应地为每个节点设定不同的感受野，而不像GCN那样预先设定好卷积的感受野。</p><p><strong>本文的工作可以看成是GCN的变形</strong>。作者使用<strong>求和(sum)的操作</strong>捕获每个<strong>节点$T$步邻居聚合</strong>来的信息，并且使用<strong>注意力机制</strong>衡量<strong>不同类型节点</strong>的重要性。</p><h3 id="2-2-Node-Embedding"><a href="#2-2-Node-Embedding" class="headerlink" title="2.2 Node Embedding"></a>2.2 Node Embedding</h3><p>第二类技术包括<strong>图嵌入方法</strong>，目的是在保留图结构带来的信息的同时学习每个节点的表示。</p><p>大多数方法目的都是最小化如下的衡量<strong>重构能力</strong>的经验损失：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gknx2mu0vvj20mf02rglr.jpg" alt="undefined"></p><h2 id="3-提出的方法"><a href="#3-提出的方法" class="headerlink" title="3 提出的方法"></a>3 提出的方法</h2><ol><li>描述在支付宝数据集上发现的两个pattern；</li><li>讨论一个基于连通子图的motivated方法；</li><li>基于2中提到的motivated的方法，利用1中发现的两个pattern构建HIN；</li><li>讲述最终的模型</li></ol><h3 id="3-1-数据分析"><a href="#3-1-数据分析" class="headerlink" title="3.1 数据分析"></a>3.1 数据分析</h3><p>本节研究了支付宝中真实数据中体现出来的“<strong>设备聚集</strong>”和“<strong>行为聚集</strong>”的特性。</p><h4 id="3-1-1-设备聚集（Device-Aggregation）"><a href="#3-1-1-设备聚集（Device-Aggregation）" class="headerlink" title="3.1.1 设备聚集（Device Aggregation）"></a>3.1.1 设备聚集（Device Aggregation）</h4><p>基本思想是：<strong>若一个账户与大量的其他账户一起注册或登录同一组设备，则这些账户就会被怀疑是恶意账户。</strong></p><p>计算连通子图的规模，通过规模的大小来衡量账户的风险。</p><h4 id="3-1-2-行为聚集（Behavior-Aggregation）"><a href="#3-1-2-行为聚集（Behavior-Aggregation）" class="headerlink" title="3.1.2 行为聚集（Behavior Aggregation）"></a>3.1.2 行为聚集（Behavior Aggregation）</h4><p>基本思想是：如果共享设备的账户成批运行，则这些账户就是可疑的。使用向量内积作为衡量标准，即$S^a_{i,i^{‘}}=&lt;x_i, x_{i^{‘}}&gt;$。</p><p>这样的度量两个账户关联性的方法，可以用于<strong>对连通子图进行进一步分割</strong>，来<strong>提高假阳性概率</strong>。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gknxknkqv4j21at0ib7cc.jpg" alt="设备聚集"></p><p><strong>上图表示设备聚集</strong>，展示了支付宝中连续7天的account-device数据图。对于正常账户，蓝色的点均匀分布在图中。对于恶意账户，点的分布表明特定的设备以不同的模式连接了大量的账户。</p><blockquote><p>也就是一个Device对应了大量的账户。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gknxkv3k5fj21b40i2wk8.jpg" alt="行为聚集"></p><p><strong>上图表示行为聚集</strong>，展示了账户在不同时间的行为模式。左图的正常账户的行为显示，每个新注册的账户在未来几天内的行为是均匀分布的。而右图中的恶意账户的行为往往只在短时间内爆发。</p><blockquote><p>正如前面所说的，为了快速完成任务，恶意账号只会在一小段时间内活跃。而正常账号则是均匀的活动~</p></blockquote><h3 id="3-2-A-Motivation-Subgraph-Components"><a href="#3-2-A-Motivation-Subgraph-Components" class="headerlink" title="3.2  A Motivation: Subgraph Components"></a>3.2  A Motivation: Subgraph Components</h3><p>上述的<strong>设备聚集</strong>和<strong>行为聚集</strong>模式启发了作者使用图的方式来解决问题。</p><p>第一步尝试称为<strong>“连通子图(connected subgraph)”</strong>。</p><p>基本思想是<strong>建立只包含账户构成的图，希望用边建立起一组账户。</strong></p><p>连通子图方法由<strong>以下三步</strong>组成：</p><ol><li><p><strong>给定图$G=(V,E)$，有$N$个节点，$M$条边。${(i,j)}$表示账户$i$在设备$j$上有登录行为。目标是构建一个只由账户节点构成的同质图</strong>。$G^a=(V^a,E^a)$，边$ (i,i^{‘})$表示账户$i$和$i^{‘}$有一段时间在同一设备上登录。</p><p>这样，同质图$G^a$就由多个连通子图所构成，每个子图表示一组相关的账户。这组账户数量越多，则为恶意账户的风险越大。</p><p><strong>但是实际操作中有很多噪声</strong>，例如不同账户登录相同的ip地址，混淆正常账户和恶意账户的现象很普遍。</p></li><li><p><strong>接着按照如下的方法删除掉一些边</strong>。由图2所示，异常账户的行为通常在<strong>特定的一天的短暂一段时间</strong>内爆发。为了衡量$G^a$中两个账户节点间的相似性，使用向量$x_i=[x_{i,1},…,x_{i,p}]^{\mathrm T}$表示账户$i$的行为，$x_{i,t}$表示账户$i$在第$t$小时行为的频率。</p><p>使用内积运算$x^{\mathrm{ T }} _ i  {x} _ { i^{ \prime } }$ 衡量两个账户之间的相似度。若$x^{\mathrm{ T }} _ i  {x} _ { i^{ \prime } } &lt; \theta$，则在图$G^a$中删除边$(i,i^{\prime})$。$\theta$是一个调节$G^a$稀疏性的<strong>超参数</strong>。</p><blockquote><p>既然是超参数，因此是需要学习的。</p></blockquote></li><li><p><strong>使用每个账户所属</strong>的<strong>子图的大小</strong>为其打分。</p></li></ol><p>尽管该方法可以在最大的连通子图中准确检测出恶意账户，但是它不能很好地在较小的连通子图中检测出恶意账户。</p><hr><p>能不能使用机器学习方法进行恶意账户识别呢？</p><p>与传统的先提取特征$X$，然后学习判别函数$f(X)$的方法不同，能否同时使用特征和图的结构，直接学习得到$f(X,G)$呢？</p><hr><p><strong>从上述构建连通子图的3步可以观察到两点：</strong></p><ol><li><p>连通子图的评分由以下两点确定：1）每个点和邻居的连通性；2）一个连通子图中的节点数目，连通性取决于$G^{ a }$（设备聚集）的结构以及节点间的向量内积（行为聚集）。子图中节点的数目反映了连通性的强度。</p></li><li><p>一个将account-device图$G$转换为account-account图$G^a$的转换函数。这一步是非常重要的，因为没有这一步，我们就不能度量不同账号之间的亲密度。<strong>但是，变成account-account图会损失很多的信息。</strong></p></li></ol><p>在下面的小节中，<strong>作者学习了一个带参数的评分函数，并且将每个节点映射到vector space中，这样可以模拟$G^a$中的连通性和。</strong></p><h3 id="3-3-Heterogeneous-Graph-Construction"><a href="#3-3-Heterogeneous-Graph-Construction" class="headerlink" title="3.3 Heterogeneous Graph Construction"></a>3.3 Heterogeneous Graph Construction</h3><p>假定$N$个节点包括账户和设备，每个设备都对应一个类型$d\in D。$给出在时间范围$[0,T)$的$M$条账户和设备之间的连边${\lbrace (i,j) \rbrace}$。每条边都表示账户$i$在设备$j$上有行为，例如注册、登录等。对于包含$N$个节点的图$G=(V,E)$，有邻接矩阵$A\in \lbrace 0,1 \rbrace ^{N,N}$。</p><p>图$G$中的一个连通子图展示如下，其中蓝色节点是正常账户，黄色节点是异常账户：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkofqaosdnj20i50mb77p.jpg" alt="一个连通子图"></p><p>为了方便，作者按照设备(device)的类型，抽取出了$|D|$个子图$\lbrace G^{(d)} =(V,E^{(d)}) \rbrace$，每个子图都包含了$G$中的所有节点，但是擦去了不是该类型设备的边。这样的操作产生了$|D|$个邻接矩阵$\lbrace A^{(d)} \rbrace$。</p><blockquote><p>注意，这里<strong>设备的概念比较宽泛</strong>，例如设备可以是</p><ul><li>IP地址</li><li>电话号码</li><li>User Machine ID（UMID）</li><li>MAC地址</li><li>IMSI（International Mobile Subscriber Identity）</li><li>APDID（Alipay Device ID）</li><li>TID</li></ul><p>这就构成了异质图。</p></blockquote><p>在这些图的基础上，进一步处理每个账户的行为。假定矩阵$X\in R^{N,p+|D|}$，若$i$是账户节点则每一行$x_i$表示了节点$i$的行为。</p><p>账户$i$在时间范围$[0,T)$内的行为可以分$p$个时间小段，每一个时间小段表示账户在这段时间产生行为的次数。</p><p>对于和该账户相关联的设备，只需使用将向量的最后$|D|$维根据所属设备，编码成one-hot向量就可以了。</p><p>最终的目的：</p><p>给定邻接矩阵$A$、在$[0,T)$时间内的行为矩阵$X$，以及在$[0,T-1)$时间内$N_0$个已标注账户是否为恶意账户的标签，学习到函数$f(,X)$，正确预测在$T$时刻的恶意账户。</p><h3 id="3-4-Models"><a href="#3-4-Models" class="headerlink" title="3.4 Models"></a>3.4 Models</h3><p>上述章节讨论了数据中发现的模式（“设备聚集”和“行为聚集”），以及异质图的构建。<font color="red"><strong>并且说明了这些模式可以通过给定$A, X$的函数学习得到。</strong></font>但仍然需要一个强大的函数来捕获这些模式。</p><blockquote><p>按照小杨的理解就是：</p><ul><li>adjacency matrix $A$可以用于求得一个个连通子图，而activities matrix $X$可以用于简化得到的连通子图，得到真正有嫌疑的连通子图。</li></ul></blockquote><p>我们希望通过聚合转换后的行为矩阵$X$，从而为每个节点$i$学习到有效的embedding $h_i$：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkoh7rezudj20mb05ywey.jpg" alt="隐藏层函数"></p><p>其中：</p><ul><li><p>$H^{(t)}\in R^{N,k}$,表示$t$层的嵌入矩阵，每行表示一个节点的embedding。</p></li><li><p>$T$表示节点跳数，也表示隐藏层的层数。</p></li><li><p>$W, $是需要优化的参数。在给定了连通性（即Adjacency Matrix $A$）和账户的活动（即Activities Matrix $X$）情况下，这两个参数用于自动地获得更好的Device Aggregation和Activity Aggregation。</p></li><li><p>我们让$x_i$出现在每一层hidder layer中，这样可以像残差网络一样连接到深层的距离层？</p></li></ul><p>随着迭代的加深(例如T步)，节点就可以在隐层聚合T-step的邻居信息，这就和<strong>连通子图</strong>中定义的<strong>打分函数</strong>（计算连通子图中的节点数）有相似之处。区别在于，我们的方法是在原始的account-device图上工作的，通过将T-step邻居的行为<strong>嵌入求和</strong>，来将节点映射到隐层空间。</p><p>最后，我们能学到一个只包含参数$W$和${V_1,…,V_{|D|}}$的函数，这些参数可以由机器学习的方法学习到。如果没有邻接矩阵A的情况下，我们的模型退化为具有“skip-connection”结构的，只依赖于特征X的深度神经网络。</p><p>损失函数定义如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkoieuo7juj20mh03bjrl.jpg" alt="损失函数"></p><p>使用EM算法优化，在e-step，基于参数$W$和${\lbrace V_d \rbrace}$使用（6）式计算embeddings；在m-step，优化（7）式中的参数并调整embeddings。</p><blockquote><p>本文的方法可以看成是<strong>GCN的变形</strong>，主要区别在于：<br>1）本文的算法可以用于HIN；<br>2）聚合函数是不同的，本文的模型对不同类型的图$G^{(d)}$中的两种模式（设备聚集和行为聚集）进行的是求和操作，然后按照图类型的数目取了均值。</p></blockquote><h3 id="3-5-注意力机制"><a href="#3-5-注意力机制" class="headerlink" title="3.5 注意力机制"></a>3.5 注意力机制</h3><p>引入注意力机制，在学习过程中自适应地为不同类型的子图分配注意力：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkoilzef87j20mc02udg2.jpg" alt="注意力机制"></p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><p><strong>数据集</strong>：Alipay(支付宝)</p><p><strong>实验任务</strong>：</p><p><strong>对比方法</strong>：</p><ol><li>连通子图：4.2中提出的方法</li><li>GBDT+Graph：一种基于机器学习的方法，GBDT全称为Gradient Boosting Decision Tree</li><li>GBDT+Node2Vec：基于随机游走的节点嵌入方法</li><li>GCN：经典的图卷积网络方法，聚合公式是（1）式</li></ol><p><strong>实验结果</strong>：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gkooynm81vj20mb0lwtdo.jpg" alt="undefined"></p><p>不同方法在测试集上，第1,2,3,4周的precision-recall曲线对比如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fly1gkooyygvnhj20x70mpwle.jpg" alt="undefined"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><p>本文提出了<strong>GEM</strong>模型，用于日常在支付宝中恶意账户的发现。</p><p>总结了攻击者的<strong>两个基本特点</strong>：设备聚集、行为聚集。</p><p>是<strong>第一个</strong>使用GNN方法实现欺诈检测的方法。</p><p>未来的工作：在随时间变化的<strong>动态图</strong>上建立恶意账户检测系统。</p><h2 id="6-存在的问题"><a href="#6-存在的问题" class="headerlink" title="6 存在的问题"></a>6 存在的问题</h2><ol><li><p>Not reproducible.<br>No open dataset or open source code.<br>Lack details of secret weapons（e.g. User Machine ID(UMID)）.</p></li><li><p>Adaptive adversary.</p><p>Fake Hardware ID by hijacking system APIs on rooted devices.</p><p>Malicious account can be more active.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>傅里叶分析之掐死教程</title>
      <link href="/2020/11/11/fu-li-xie-fen-xi-zhi-qia-si-jiao-cheng/"/>
      <url>/2020/11/11/fu-li-xie-fen-xi-zhi-qia-si-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<blockquote><p>作 者：韩 昊</p><p>知 乎：Heinrich</p><p>微 博：@花生油工人</p><p>知乎专栏：与时间无关的故事</p><p>谨以此文献给大连海事大学的吴楠老师，柳晓鸣老师，王新年老师以及张晶泊老师。</p><p><strong>转载的同学请保留上面这句话，谢谢。如果还能保留文章来源就更感激不尽了。</strong></p><p><a href="https://zhuanlan.zhihu.com/p/19763358">傅里叶分析之掐死教程（完整版）更新于2014.06.06</a></p></blockquote><p>——更新于2014.6.6，想直接看更新的同学可以直接跳到第四章————</p><p>我保证这篇文章和你以前看过的所有文章都不同，这是12年还在果壳的时候写的，但是当时没有来得及写完就出国了……于是拖了两年，嗯，我是拖延症患者……</p><p>这篇文章的核心思想就是：</p><p><strong>要让读者在不看任何数学公式的情况下理解傅里叶分析。</strong></p><p><strong>傅里叶分析不仅仅是一个数学工具，更是一种可以彻底颠覆一个人以前世界观的思维模式。</strong>但不幸的是，傅里叶分析的公式看起来太复杂了，所以很多大一新生上来就懵圈并从此对它深恶痛绝。老实说，这么有意思的东西居然成了大学里的杀手课程，不得不归咎于编教材的人实在是太严肃了。（您把教材写得好玩一点会死吗？会死吗？）所以我一直想写一个有意思的文章来解释傅里叶分析，有可能的话高中生都能看懂的那种。所以，不管读到这里的您从事何种工作，我保证您都能看懂，并且一定将体会到通过傅里叶分析看到世界另一个样子时的快感。至于对于已经有一定基础的朋友，也希望不要看到会的地方就急忙往后翻，仔细读一定会有新的发现。</p><p>————以上是定场诗————</p><p>下面进入正题：</p><p>抱歉，还是要啰嗦一句：其实学习本来就不是易事，我写这篇文章的初衷也是希望大家学习起来更加轻松，充满乐趣。但是千万！千万不要把这篇文章收藏起来，或是存下地址，心里想着：以后有时间再看。这样的例子太多了，也许几年后你都没有再打开这个页面。无论如何，耐下心，读下去。这篇文章要比读课本要轻松、开心得多……</p><p>p.s.本文无论是cos还是sin，都统一用“<strong>正弦波</strong>”（Sine Wave）一词来代表<strong>简谐波</strong>。</p><h2 id="一、什么是频域（Frequency-Domain）"><a href="#一、什么是频域（Frequency-Domain）" class="headerlink" title="一、什么是频域（Frequency Domain）"></a>一、什么是频域（Frequency Domain）</h2><p>从我们出生，我们看到的世界都以时间贯穿，股票的走势、人的身高、汽车的轨迹都会随着时间发生改变。<strong>这种以时间作为参照来观察动态世界的方法我们称其为时域分析。</strong>而我们也想当然的认为，世间万物都在随着时间不停的改变，并且永远不会静止下来。但如果我告诉你，用另一种方法来观察世界的话，你会发现世界是永恒不变的，你会不会觉得我疯了？我没有疯，这个静止的世界就叫做<strong>频域</strong>。</p><p>先举一个<strong>公式上并非很恰当</strong>，但意义上再贴切不过的例子：</p><p>在你的理解中，一段音乐是什么呢？</p><p><img src="https://pic2.zhimg.com/80/2ca39677363c65a305207a5491b75825_720w.jpg" alt="时域下音乐的表示"></p><p>这是我们对音乐最普遍的理解，一个随着时间变化的震动。但我相信对于乐器小能手们来说，音乐更直观的理解是这样的：</p><p><img src="https://pic3.zhimg.com/80/8e1fce9d7607d97cebf73e1f36f03f06_720w.jpg" alt="频域下音乐的表示"></p><p>好的！下课，同学们再见。</p><p>是的，其实这一段写到这里已经可以结束了。<strong>上图是音乐在时域的样子，而下图则是音乐在频域的样子。</strong>所以频域这一概念对大家都从不陌生，只是从来没意识到而已。</p><p>现在我们可以回过头来重新看看一开始那句痴人说梦般的话：世界是永恒的。</p><p>将以上两图简化：</p><p>时域：</p><p><img src="https://pic3.zhimg.com/80/d4fa1de0327eb491a6941ac84a56e432_720w.jpg" alt="时域"></p><p>频域：</p><p><img src="https://pic2.zhimg.com/80/1ca366b593d877a16c8a49773774b5b9_720w.jpg" alt="频域"></p><p>在时域，我们观察到钢琴的琴弦一会上一会下的摆动，就如同一支股票的走势；而在频域，只有那一个永恒的音符。</p><p>所以</p><p><strong>你眼中看似落叶纷飞变化无常的世界，实际只是躺在上帝怀中一份早已谱好的乐章。</strong></p><p>抱歉，这不是一句鸡汤文，而是黑板上确凿的公式：<font color="red"><strong>傅里叶同学告诉我们，任何周期函数，都可以看作是不同振幅，不同相位的正弦波的叠加。</strong></font>在第一个例子里我们可以理解为，利用对不同琴键不同力度，不同时间点的敲击，可以组合出任何一首乐曲。</p><p>而贯穿时域与频域的方法之一，就是传中说的傅里叶分析。傅里叶分析可分为<font color="red"><strong>傅里叶级数（Fourier Series）</strong>和<strong>傅里叶变换(Fourier Transformation)</strong></font>，我们从简单的开始谈起。</p><h2 id="二、傅里叶级数（Fourier-Series）的频谱（frequency-Spectrum）"><a href="#二、傅里叶级数（Fourier-Series）的频谱（frequency-Spectrum）" class="headerlink" title="二、傅里叶级数（Fourier Series）的频谱（frequency Spectrum）"></a>二、傅里叶级数（Fourier Series）的频谱（frequency Spectrum）</h2><p>还是举个栗子并且有图有真相才好理解。</p><p>如果我说我能用前面说的<strong>正弦波叠加出一个带90度角的矩形波</strong>来，你会相信吗？你不会，就像当年的我一样。但是看看下图：</p><p><img src="https://pic2.zhimg.com/80/055bf33bb84555a952804c5dbeb75dd9_720w.jpg" alt="img"></p><p>第一幅图是一个郁闷的正弦波$cos(x)$</p><p>第二幅图是2个卖萌的正弦波的叠加cos(x)+a.cos(3x)</p><p>第三幅图是4个发春的正弦波的叠加</p><p>第四幅图是10个便秘的正弦波的叠加</p><p>随着正弦波数量逐渐的增长，他们最终会叠加成一个标准的矩形，大家从中体会到了什么道理？</p><p>（只要努力，弯的都能掰直！）</p><p>随着叠加的递增，所有正弦波中上升的部分逐渐让原本缓慢增加的曲线不断变陡，而所有正弦波中下降的部分又抵消了上升到最高处时继续上升的部分使其变为水平线。一个矩形就这么叠加而成了。</p><p>但是要多少个正弦波叠加起来才能形成一个标准90度角的矩形波呢？不幸的告诉大家，答案是无穷多个。（上帝：我能让你们猜着我？）</p><p>不仅仅是矩形，<strong>你能想到的任何波形都是可以如此方法用正弦波叠加起来的。</strong>这是没有接触过傅里叶分析的人在直觉上的第一个难点，但是一旦接受了这样的设定，游戏就开始有意思起来了。</p><p>还是上图的正弦波累加成矩形波，我们换一个角度来看看：</p><p><img src="https://pic4.zhimg.com/80/563deb4a6599d052b3ba108661872c57_720w.jpg" alt="img"></p><p>在这几幅图中，最前面黑色的线就是所有正弦波叠加而成的总和，也就是越来越接近矩形波的那个图形。而后面依不同颜色排列而成的正弦波就是组合为矩形波的各个分量。这些正弦波按照频率从低到高从前向后排列开来，而每一个波的振幅都是不同的。一定有细心的读者发现了，每两个正弦波之间都还有一条直线，那并不是分割线，而是振幅为0的正弦波！也就是说，为了组成特殊的曲线，有些正弦波成分是不需要的。</p><p>这里，<font color="red"><strong>不同频率的正弦波我们称为频率分量</strong></font>。</p><p>好了，关键的地方来了！！</p><p>如果我们把第一个频率最低的频率分量看作“1”，我们就有了构建<strong>频域的最基本单元</strong>。</p><p>对于我们最常见的有理数轴，数字“1”就是有理数轴的基本单元。</p><p>时域的基本单元就是“1秒”，如果我们将一个角频率为$\omega_0$的正弦波$cos(\omega_0t)$看作基础，那么<strong>频域的基本单元就是$\omega_0$。</strong></p><p>有了“1”，还要有“0”才能构成世界，那么频域的“0”是什么呢？$cos(0t)$就是一个周期无限长的正弦波，也就是一条直线！所以在频域，0频率也被称为直流分量，在傅里叶级数的叠加中，它仅仅影响全部波形相对于数轴整体向上或是向下而不改变波的形状。</p><p>接下来，让我们回到初中，回忆一下已经死去的八戒，啊不，已经死去的老师是怎么定义正弦波的吧。</p><p><img src="https://pic3.zhimg.com/80/81ca9447d6c45c162c2d76df75a6690a_720w.jpg" alt="img"></p><p>正弦波就是一个圆周运动在一条直线上的投影。所以频域的基本单元也可以理解为一个始终在旋转的圆。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklh9aup1dg2074074tha.gif" alt="傅里叶图像"></p><p>介绍完了<strong>频域的基本组成单元</strong>，我们就可以看一看一个矩形波，<strong>在频域里的另一个模样了</strong>：</p><p><img src="https://pic3.zhimg.com/80/e2e3c0af3bdbcba721c5415a4c65da9e_720w.jpg" alt="img"></p><p>这是什么奇怪的东西？</p><p>这就是矩形波在频域的样子，是不是完全认不出来了？教科书一般就给到这里然后留给了读者无穷的遐想，以及无穷的吐槽，其实教科书只要补一张图就足够了：<strong>频域图像，也就是俗称的频谱</strong>，就是——</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklhar1lmpj20k0090q3u.jpg" alt="undefined"></p><p>再清楚一点：</p><p><img src="https://pic2.zhimg.com/80/40cf849e55ed95732a60b52d4019d609_720w.jpg" alt="img"></p><p>可以发现，<strong>在频谱中，偶数项的振幅都是0，也就对应了图中的彩色直线。振幅为0的正弦波。</strong></p><p>动图请戳：</p><p><a href="https://link.zhihu.com/?target=http://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif">File:Fourier series and transform.gif</a></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklhd29dotg208c06ogsv.gif" alt="undefined"></p><p>老实说，在我学傅里叶变换时，维基的这个图还没有出现，那时我就想到了这种表达方法，而且，后面还会加入维基没有表示出来的另一个谱——相位谱。</p><p>但是在讲相位谱之前，我们先回顾一下刚刚的这个例子究竟意味着什么。记得前面说过的那句“世界是静止的”吗？估计好多人对这句话都已经吐槽半天了。想象一下，世界上每一个看似混乱的表象，实际都是一条时间轴上不规则的曲线，但实际这些曲线都是由这些无穷无尽的正弦波组成。我们看似不规律的事情反而是规律的正弦波在时域上的投影，而正弦波又是一个旋转的圆在直线上的投影。那么你的脑海中会产生一个什么画面呢？</p><p>我们眼中的世界就像皮影戏的大幕布，幕布的后面有无数的齿轮，大齿轮带动小齿轮，小齿轮再带动更小的。在最外面的小齿轮上有一个小人——那就是我们自己。我们只看到这个小人毫无规律的在幕布前表演，却无法预测他下一步会去哪。而幕布后面的齿轮却永远一直那样不停的旋转，永不停歇。这样说来有些宿命论的感觉。说实话，这种对人生的描绘是我一个朋友在我们都是高中生的时候感叹的，当时想想似懂非懂，直到有一天我学到了傅里叶级数……</p><h2 id="三、傅里叶级数（Fourier-Series）的相位谱（-phase-spectrum-）"><a href="#三、傅里叶级数（Fourier-Series）的相位谱（-phase-spectrum-）" class="headerlink" title="三、傅里叶级数（Fourier Series）的相位谱（ phase spectrum ）"></a>三、傅里叶级数（Fourier Series）的相位谱（ phase spectrum ）</h2><p>上一章的关键词是：<strong>从侧面看</strong>。这一章的关键词是：<strong>从下面看</strong>。</p><hr><p>在这一章最开始，我想先回答很多人的一个问题：<strong>傅里叶分析究竟是干什么用的？</strong>这段相对比较枯燥，已经知道了的同学可以直接跳到下一个分割线。</p><p>先说一个最直接的用途。无论听广播还是看电视，我们一定对一个词不陌生——频道。频道频道，就是频率的通道，不同的频道就是将不同的频率作为一个通道来进行信息传输。下面大家尝试一件事：</p><p>先在纸上画一个sin（x），不一定标准，意思差不多就行。不是很难吧。</p><p>好，接下去画一个sin（3x）+sin（5x）的图形。</p><p>别说标准不标准了，曲线什么时候上升什么时候下降你都不一定画的对吧？</p><p>好，画不出来不要紧，我把sin（3x）+sin（5x）的曲线给你，但是前提是你不知道这个曲线的方程式，现在需要你把sin（5x）给我从图里拿出去，看看剩下的是什么。这基本是不可能做到的。</p><p><strong>但是在频域呢？则简单的很，无非就是几条竖线而已。</strong></p><p>所以很多在时域看似不可能做到的数学操作，在频域相反很容易。这就是需要傅里叶变换的地方。尤其是从某条曲线中去除一些特定的频率成分，这在工程上称为<strong>滤波</strong>，是信号处理最重要的概念之一，只有在频域才能轻松的做到。</p><p>再说一个更重要，但是稍微复杂一点的用途——求解微分方程。（这段有点难度，看不懂的可以直接跳过这段）微分方程的重要性不用我过多介绍了。各行各业都用的到。但是求解微分方程却是一件相当麻烦的事情。因为除了要计算加减乘除，还要计算微分积分。而傅里叶变换则可以让微分和积分在频域中变为乘法和除法，大学数学瞬间变小学算术有没有。</p><p>傅里叶分析当然还有其他更重要的用途，我们随着讲随着提。</p><hr><p>下面我们继续说相位谱：</p><p>通过时域到频域的变换，我们得到了一个从侧面看的频谱，但是这个频谱并没有包含时域中全部的信息。因为频谱只代表每一个对应的正弦波的振幅是多少，而没有提到相位。基础的正弦波$Asin(wt+θ)$中，振幅，频率，相位缺一不可，不同相位决定了波的位置，所以对于频域分析，仅仅有频谱（振幅谱）是不够的，我们还需要一个相位谱。那么这个相位谱在哪呢？我们看下图，这次为了避免图片太混论，我们用7个波叠加的图。</p><p><img src="https://pic3.zhimg.com/80/07199fc0250791d768771b50c098e26a_720w.jpg" alt="img"></p><p>鉴于正弦波是周期的，我们需要设定一个用来标记正弦波位置的东西。在图中就是那些小红点。小红点是距离频率轴最近的波峰，而这个波峰所处的位置离频率轴有多远呢？为了看的更清楚，我们将红色的点投影到下平面，投影点我们用粉色点来表示。当然，这些粉色的点只标注了波峰距离频率轴的距离，并不是相位。</p><p><img src="https://pic4.zhimg.com/80/e1985fe86283a7b14d1fc7e11d322fcb_720w.jpg" alt="img"></p><p>这里需要纠正一个概念：时间差并不是相位差。如果将全部周期看作$2\pi$或者360度的话，相位差则是时间差在一个周期中所占的比例。<strong>我们将时间差除周期再乘$2\pi$，就得到了相位差。</strong></p><p>在完整的立体图中，我们将投影得到的时间差依次除以所在频率的周期，就得到了最下面的相位谱。所以，频谱是从侧面看，相位谱是从下面看。下次偷看女生裙底被发现的话，可以告诉她：“对不起，我只是想看看你的相位谱。”</p><p>注意到，相位谱中的相位除了0，就是Pi。因为cos（t+Pi）=-cos（t），所以实际上相位为Pi的波只是上下翻转了而已。对于周期方波的傅里叶级数，这样的相位谱已经是很简单的了。另外值得注意的是，由于cos（t+2Pi）=cos（t），所以相位差是周期的，pi和3pi，5pi，7pi都是相同的相位。人为定义相位谱的值域为(-pi，pi]，所以图中的相位差均为Pi。</p><p>最后来一张大集合：</p><p><img src="https://pic3.zhimg.com/80/4695ce06197677bab880cd55b6846f12_720w.jpg" alt="img"></p><h2 id="四、傅里叶变换（Fourier-Transformation）"><a href="#四、傅里叶变换（Fourier-Transformation）" class="headerlink" title="四、傅里叶变换（Fourier Transformation）"></a>四、傅里叶变换（Fourier Transformation）</h2><p>相信通过前面三章，大家对频域以及傅里叶级数都有了一个全新的认识。但是文章在一开始关于钢琴琴谱的例子我曾说过，这个栗子是一个公式错误，但是概念典型的例子。所谓的公式错误在哪里呢？</p><p><font color="red"><strong>傅里叶级数的本质是将一个周期的信号分解成无限多分开的（离散的）正弦波</strong></font>，但是宇宙似乎并不是周期的。曾经在学数字信号处理的时候写过一首打油诗：</p><p><strong>往昔连续非周期，</strong></p><p><strong>回忆周期不连续，</strong></p><p><strong>任你ZT、DFT，</strong></p><p><strong>还原不回去。</strong></p><p>（请无视我渣一样的文学水平……）</p><p>在这个世界上，有的事情一期一会，永不再来，并且时间始终不曾停息地将那些刻骨铭心的往昔连续的标记在时间点上。但是这些事情往往又成为了我们格外宝贵的回忆，在我们大脑里隔一段时间就会周期性的蹦出来一下，可惜这些回忆都是零散的片段，往往只有最幸福的回忆，而平淡的回忆则逐渐被我们忘却。因为，往昔是一个连续的非周期信号，而回忆是一个周期离散信号。</p><p><strong>是否有一种数学工具将连续非周期信号变换为周期离散信号呢？抱歉，真没有。</strong></p><p><strong>傅里叶级数，在时域是一个周期且连续的函数，而在频域是一个非周期离散的函数</strong>。</p><p>而在我们接下去要讲的傅里叶变换，则是<strong>将一个时域非周期的连续信号，转换为一个在频域非周期的连续信号。</strong></p><p>算了，还是上一张图方便大家理解吧：</p><p><img src="https://pic1.zhimg.com/80/419cd0b2e965aca25d5f8a5a6362d728_720w.jpg" alt="img"></p><p>或者我们也可以换一个角度理解：<strong>傅里叶变换实际上是对一个周期无限大的函数进行傅里叶变换</strong>。</p><p>所以说，钢琴谱其实并非一个连续的频谱，而是很多在时间上离散的频率，但是这样的一个贴切的比喻真的是很难找出第二个来了。</p><p>因此在傅里叶变换在频域上就从离散谱变成了连续谱。那么连续谱是什么样子呢？</p><p><strong>你见过大海么？</strong></p><p>为了方便大家对比，我们这次从另一个角度来看频谱，还是傅里叶级数中用到最多的那幅图，我们从频率较高的方向看。</p><p><img src="https://pic3.zhimg.com/80/a185be412974fd73a7925cf1f1cc5372_720w.jpg" alt="img"></p><p>以上是离散谱，那么连续谱是什么样子呢？</p><p>尽情的发挥你的想象，想象这些离散的正弦波离得越来越近，逐渐变得连续……</p><p>直到变得像波涛起伏的大海：</p><p><img src="https://pic4.zhimg.com/80/ece53f825c6de629befba3de12f929a7_720w.jpg" alt="img"></p><p>不过通过这样两幅图去比较，大家应该可以理解如何从离散谱变成了连续谱的了吧？原来离散谱的叠加，变成了连续谱的累积。所以在计算上也从求和符号变成了积分符号。（这句话太重要了）</p><p>不过，这个故事还没有讲完，接下去，我保证让你看到一幅比上图更美丽壮观的图片，但是这里需要介绍到一个数学工具才能然故事继续，这个工具就是——</p><h2 id="五、宇宙耍帅第一公式：欧拉公式"><a href="#五、宇宙耍帅第一公式：欧拉公式" class="headerlink" title="五、宇宙耍帅第一公式：欧拉公式"></a>五、宇宙耍帅第一公式：欧拉公式</h2><p>虚数<code>i</code>这个概念大家在高中就接触过，但那时我们只知道它是-1的平方根，可是它真正的意义是什么呢?</p><p><img src="https://pic1.zhimg.com/80/42e1f6dc43e8868b4962f5ba389a5df4_720w.jpg" alt="img"></p><p>这里有一条数轴，在数轴上有一个红色的线段，它的长度是1。当它乘以3的时候，它的长度发生了变化，变成了蓝色的线段，而当它乘以-1的时候，就变成了绿色的线段，或者说线段在数轴上围绕原点旋转了180度。</p><p>我们知道乘-1其实就是乘了两次 i使线段旋转了180度，那么乘一次 i 呢——答案很简单——旋转了90度。</p><p><img src="https://pic1.zhimg.com/80/3e88e9463e4667e50ebdda51dee88358_720w.jpg" alt="img"></p><p>同时，我们获得了一个垂直的虚数轴。实数轴与虚数轴共同构成了一个复数的平面，也称<strong>复平面</strong>。这样我们就了解到，乘虚数<code>i</code>的一个功能——<strong>旋转</strong>。</p><p>现在，就有请宇宙第一耍帅公式欧拉公式隆重登场——<br>$$<br>e^{ix}=cosx+i*sinx<br>$$<br>这个公式在数学领域的意义要远大于傅里叶分析，但是称它为宇宙第一耍帅公式是因为它的特殊形式——当x等于$\pi$的时候。<br>$$<br>e^{i\pi}+1=0<br>$$<br>经常有理工科的学生为了跟妹子表现自己的学术功底，用这个公式来给妹子解释数学之美：”石榴姐你看，这个公式里既有自然底数e，自然数1和0，虚数i还有圆周率pi，它是这么简洁，这么美丽啊！“但是姑娘们心里往往只有一句话：”臭屌丝……“</p><p>这个公式关键的作用，是将正弦波统一成了简单的指数形式。我们来看看图像上的涵义：</p><p><img src="https://pic4.zhimg.com/80/974efc6a99e06dcd623193e960ccbe93_720w.jpg" alt="img"></p><p>欧拉公式所描绘的，是一个随着时间变化，在复平面上做圆周运动的点，随着时间的改变，在时间轴上就成了一条螺旋线。如果只看它的实数部分，也就是螺旋线在左侧的投影，就是一个最基础的余弦函数。而右侧的投影则是一个正弦函数。</p><p>关于复数更深的理解，大家可以参考：</p><p><a href="http://www.zhihu.com/question/23234701/answer/26017000">复数的物理意义是什么？</a></p><p>这里不需要讲的太复杂，足够让大家理解后面的内容就可以了。</p><h2 id="六、指数形式的傅里叶变换"><a href="#六、指数形式的傅里叶变换" class="headerlink" title="六、指数形式的傅里叶变换"></a>六、指数形式的傅里叶变换</h2><p>有了欧拉公式的帮助，我们便知道：<strong>正弦波的叠加</strong>，也可以理解为<strong>螺旋线的叠加</strong>在实数空间的投影。而螺旋线的叠加如果用一个形象的栗子来理解是什么呢？</p><p><strong>光波</strong></p><p>高中时我们就学过，自然光是由不同颜色的光叠加而成的，而最著名的实验就是牛顿师傅的三棱镜实验：</p><p><img src="https://pic2.zhimg.com/80/c2d7bfc819ebcbea8d6f2c8271d4791d_720w.jpg" alt="img"></p><p>所以其实我们在很早就接触到了光的频谱，只是并没有了解频谱更重要的意义。</p><p>但不同的是，傅里叶变换出来的频谱不仅仅是可见光这样频率范围有限的叠加，而是频率从0到无穷所有频率的组合。</p><p>这里，我们可以用两种方法来理解<strong>正弦波</strong>：</p><p>第一种前面已经讲过了，就是<strong>螺旋线在实轴的投影</strong>。</p><p>另一种需要借助欧拉公式的另一种形式去理解：<br>$$<br>e^{it}=cos(t)+i*sin(t)<br>$$</p><p>$$<br>e^{-it}=cos(t)-i*sin(t)<br>$$</p><p>将以上两式相加再除2，得到：<br>$$<br>cos(t)=\frac {e^{it}+e^{-it}} {2}<br>$$<br>这个式子可以怎么理解呢？</p><p>我们刚才讲过，$e^{it}$可以理解为一条逆时针旋转的螺旋线，那么$e^{-it}$则可以理解为一条顺时针旋转的螺旋线。而$cos(t)$则是这两条旋转方向不同的螺旋线叠加的一半，因为<strong>这两条螺旋线的虚数部分相互抵消掉了</strong>！</p><p>举个例子的话，就是极化方向不同的两束光波，磁场抵消，电场加倍。</p><p>这里，逆时针旋转的我们称为正频率，而顺时针旋转的我们称为负频率（注意不是复频率）。</p><p>好了，刚才我们已经看到了大海——<strong>连续的傅里叶变换频谱</strong>，现在想一想，连续的螺旋线会是什么样子：</p><p><img src="https://pic1.zhimg.com/80/f116ae26859bdc80b28ea0f8f894ccc0_720w.jpg" alt="img"></p><p>是不是很漂亮？</p><p>你猜猜，这个图形在时域是什么样子？</p><p><img src="https://pic1.zhimg.com/80/0fdfa0a9b6eea036703ab2499381080c_720w.jpg" alt="img"></p><p>哈哈，是不是觉得被狠狠扇了一个耳光。数学就是这么一个把简单的问题搞得很复杂的东西。</p><p>顺便说一句，那个像大海螺一样的图，为了方便观看，我仅仅展示了其中正频率的部分，负频率的部分没有显示出来。</p><p>如果你认真去看，海螺图上的每一条螺旋线都是可以清楚的看到的，每一条螺旋线都有着不同的振幅（旋转半径），频率（旋转周期）以及相位。而将所有螺旋线连成平面，就是这幅海螺图了。</p><p>好了，讲到这里，相信大家对傅里叶变换以及傅里叶级数都有了一个形象的理解了，我们最后用一张图来总结一下：</p><p><img src="https://pic3.zhimg.com/80/097c9051af221c171730d4bc8f436a72_720w.jpg" alt="img"></p><p>好了，傅里叶的故事终于讲完了，下面来讲讲我的故事：</p><p>这篇文章第一次被写下来的地方你们绝对猜不到在哪，是在一张高数考试的卷子上。当时为了刷分，我重修了高数（上），但是后来时间紧压根没复习，所以我就抱着裸考的心态去了考场。但是到了考场我突然意识到，无论如何我都不会比上次考的更好了，所以干脆写一些自己对于数学的想法吧。于是用了一个小时左右的时间在试卷上洋洋洒洒写了本文的第一草稿。</p><p>你们猜我的了多少分？</p><p>6分</p><p>没错，就是这个数字。而这6分的成绩是因为最后我实在无聊，把选择题全部填上了C，应该是中了两道，得到了这宝贵的6分。说真的，我很希望那张卷子还在，但是应该不太可能了。</p><p>那么你们猜猜我第一次信号与系统考了多少分呢？</p><p>45分</p><p>没错，刚刚够参加补考的。但是我心一横没去考，决定重修。因为那个学期在忙其他事情，学习真的就抛在脑后了。但是我知道这是一门很重要的课，无论如何我要吃透它。说真的，信号与系统这门课几乎是大部分工科课程的基础，尤其是通信专业。</p><p>在重修的过程中，我仔细分析了每一个公式，试图给这个公式以一个直观的理解。虽然我知道对于研究数学的人来说，这样的学习方法完全没有前途可言，因为随着概念愈加抽象，维度越来越高，这种图像或者模型理解法将完全丧失作用。但是对于一个工科生来说，足够了。</p><p>后来来了德国，这边学校要求我重修信号与系统时，我彻底无语了。但是没办法，德国人有时对中国人就是有种藐视，觉得你的教育不靠谱。所以没办法，再来一遍吧。</p><p>这次，我考了满分，而及格率只有一半。</p><p>老实说，数学工具对于工科生和对于理科生来说，意义是完全不同的。<strong>工科生只要理解了，会用，会查，就足够了。</strong>但是很多高校却将这些重要的数学课程教给数学系的老师去教。这样就出现一个问题，数学老师讲得天花乱坠，又是推理又是证明，但是学生心里就只有一句话：学这货到底干嘛用的？</p><p>缺少了目标的教育是彻底的失败。</p><p>在开始学习一门数学工具的时候，学生完全不知道这个工具的作用，现实涵义。而教材上有只有晦涩难懂，定语就二十几个字的概念以及看了就眼晕的公式。能学出兴趣来就怪了！</p><p>好在我很幸运，遇到了大连海事大学的吴楠老师。他的课全程来看是两条线索，一条从上而下，一条从下而上。先讲本门课程的意义，然后指出这门课程中会遇到哪样的问题，让学生知道自己学习的某种知识在现实中扮演的角色。然后再从基础讲起，梳理知识树，直到延伸到另一条线索中提出的问题，完美的衔接在一起！</p><p>这样的教学模式，我想才是大学里应该出现的。</p><p>最后，写给所有给我点赞并留言的同学。真的谢谢大家的支持，也很抱歉不能一一回复。因为知乎专栏的留言要逐次加载，为了看到最后一条要点很多次加载。当然我都坚持看完了，只是没办法一一回复。</p><p>本文只是介绍了一种对傅里叶分析新颖的理解方法，对于求学，还是要踏踏实实弄清楚公式和概念，学习，真的没有捷径。但至少通过本文，我希望可以让这条漫长的路变得有意思一些。</p><p>最后，祝大家都能在学习中找到乐趣。…</p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 傅里叶分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>台大李宏毅助教讲解GNN图神经网络</title>
      <link href="/2020/11/11/tai-da-li-hong-yi-zhu-jiao-jiang-jie-gnn-tu-shen-jing-wang-luo/"/>
      <url>/2020/11/11/tai-da-li-hong-yi-zhu-jiao-jiang-jie-gnn-tu-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ol><li>Introduction</li><li>Roadmap</li><li>Tasks, Dataset, and Benchmark</li><li>Spatial-based GNN</li><li>Graph Signal Processing and Spectral-based GNN</li><li>Graph Generation</li><li>GNN for NLP</li></ol><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h3><p>图在我们的日常学习生活中都无处不在：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl055lpw5j20qj0di48y.jpg"></p><p>如上图所示，Graph是由Nodes和Edges构成的。在每个不同的图中，每个Node有自己的性质，没有Edge也有自己的性质。</p><h3 id="GNN-Graph-Neural-Network"><a href="#GNN-Graph-Neural-Network" class="headerlink" title="GNN = Graph + Neural Network"></a>GNN = Graph + Neural Network</h3><h4 id="Why-do-we-need-GNN？"><a href="#Why-do-we-need-GNN？" class="headerlink" title="Why do we need GNN？"></a>Why do we need GNN？</h4><ol><li><p>classification</p><blockquote><p>input是graph~</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl0a0ie87j20nh05vmxh.jpg"></p></blockquote></li><li><p>Generation</p><blockquote><p>Generator</p><p>output是一个graph~</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl0cvpmcbj20gi0b9dhz.jpg"></p></blockquote></li></ol><p>例子：</p><h2 id="Graph-Signal-Processing-and-Spectral-based-GNN（vertex-domain-lt-gt-spectral-domain）"><a href="#Graph-Signal-Processing-and-Spectral-based-GNN（vertex-domain-lt-gt-spectral-domain）" class="headerlink" title="Graph Signal Processing and Spectral-based GNN（vertex domain <-> spectral domain）"></a>Graph Signal Processing and Spectral-based GNN（vertex domain &lt;-&gt; spectral domain）</h2><blockquote><p><strong>图信号处理</strong>和<strong>基于频谱的GNN</strong></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl8ussah3j20kk0c974q.jpg" alt="Review"></p><p>如上图，在CNN中，我们学习到一个Filter以便用于在矩阵中做convolution。那么如果我们要在Graph做到这样一种操作应该怎么做呢，怎么学习到这样一个Filter呢？下图中的流程是至关重要的~⭐⭐⭐⭐⭐</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl94i3j2lj20kl0d9wfp.jpg" alt=".png"></p><p>我们将Graph中vertex的feature值称作signal，然后对这些signal做Fourier Transform，将它们映射到Fourier domain之中，同理，将Filter也映射到Fourier domain之中，在Fourier domain之中做multiplication就可以实现convolution的效果了。最后再将Fourier domain里面的值inverse回来就好了。</p><p>这是一个很好的思想，但是问题是怎么实现呢？</p><p>我们先来看看一些<strong>信号和系统</strong>的知识。</p><p>![[~PX}1]HUOM`$J0C_$3Z0CR.png](<a href="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl99jrfmhj20dq0a2wf0.jpg">http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl99jrfmhj20dq0a2wf0.jpg</a>)</p><p>一个信号可以看做是<code>N</code>维空间里面的一个vector，在《线性代数》中，我们学过一个vector可以由这个向量空间的basis<strong>线性组合</strong>得到（如上图所示），这个过程称为<strong>合成</strong>。</p><p>当我们想知道这个线性组合中每个component（即$\hat v_k$）的$a_k$的大小是多少的时候，我们要使用<strong>分析</strong>。</p><blockquote><ul><li><p>我们假设当前使用的basis是一组<code>orthonormal basis</code>。</p></li><li><p>合成和分析是一组互逆的操作，在下面的讲解中会经常用到这个概念。</p></li></ul></blockquote><p>在time domain里面，我们常用的一组basis就是cos和sin这样一组basis。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl9oh4euvj20i00ckwfa.jpg" alt="Fourier Series Representation"></p><p>现在假设有一个周期性的信号，我们可以把它展开成一个Fourier Series。我们选用的一组basis就是$e^{jk\omega_0t}$，不同的component的大小就是由$a_k$来决定。我们有一些方式可以算出$ a_k$的大小是多少。</p><blockquote><p>harmonic components：<strong>谐波分量</strong></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl9qieiyuj20dp0avq3n.jpg" alt="计算$a_k$的方法"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkl9zu77hhj20l00d7wfx.jpg"></p><p>一个信号的表示可以体现在两个域上：<strong>时域（Time Domain）</strong>和<strong>频域（Frequency Domain）</strong>。</p><p>时域的表示就是用时域的basis线性组合。它的basis就是$\delta(t-\tau)$。上图中的积分其实也是求和的意思啦（想想积分的含义。。。）。</p><p>除了这组basis，我们还可以是用其他的basis，比如上图中的$e^{j\omega t}$。同一个信号可以用不同的basis来做<strong>合成</strong>。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkla6clysuj20m7097t9d.jpg" alt="6[H}4ZF03]ng"></p><p>Fourier transform的作用就在于找出$X(j\omega)$，上图中的式子其实也很简单，就是我们之前在Determination of $a_k$里面讲解的那种方法，只不过这里将和变成了积分~</p><p>下面是一些谱域图理论：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkla5zxlyyj20kx0bvjsn.jpg" alt="Spectral Graph Theory Part1"></p><blockquote><p>第5点中所说的<code>signal</code>，其实可以简单的理解就是vertex的属性值。例如当vertex是城市时，signal可以表示城市的人口，面积啥的~当然，signal也可以是vector。<img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklils15f5j20kx09p0ta.jpg" alt="an example for function f"></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklioc6wy3j20jm0cvt9y.jpg" alt="Spectral Graph Theory Part2"></p><blockquote><ul><li><p>Laplacian: /lɑ:’plɑ:siən/  拉普拉斯</p></li><li><p>半正定矩阵的概念：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklizialn8j20kj02yt8y.jpg" alt="半正定矩阵"></p><p><strong>而且eigen value都是大于等于0的</strong></p></li><li><p><strong>关于为何对称矩阵可以进行上述的矩阵分解，可以查阅之前上线性代数课的笔记——24_special matrix中的对称矩阵小节</strong></p></li><li><p>我们将$\lambda_l$称为频率（frequency），$u_l$是对应的basis。</p></li><li><p>根据对称矩阵的分解，eigen vectors之间一定是orthogonal的，我们再让它们orthonormal！</p></li></ul></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkljr72s9ij20mh0czab3.jpg" alt="Laplacian Matrix示例"></p><hr><p>我们将所有的basis作为signal绘制到graph上，得到下图：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkljtw32e9j20mr091gmd.jpg" alt="绘图"></p><blockquote><p>至于为什么$u$可以作为signal，我就觉得很奇怪能够。。。</p></blockquote><p>助教解释了为什么我们要将$\lambda_l$称为频率（frequency），$u_l$是对应的basis？</p><p>我们先来讲讲其他的东西，比如Discrete time Fourier basis：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklk2n4axvj20o909cn18.jpg" alt="6@JO4Q1YY4LCK2429_PXR$4.png"></p><p>上图中一个重要的定理就是：<strong>频率越大，相邻两点之间的信号变化量就越大。</strong>这个定理可以帮助理解spectral graph theory里面frequency的概念。</p><p>接下来就让我们来理解顶点<code>frequency</code>的概念吧~</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklk86af3zj20mx0du40f.jpg" alt="Interpreting vertex frequency"></p><p>接下来再看看平方的表示方法：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklkfme3drj20mv0bywg6.jpg" alt="平方表示方法"></p><p>两个顶点之间的能量差。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklkktjqwbj20oe0bhn1j.jpg" alt="UE%_A"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklkl9zwcnj20mu0bywfo.jpg"></p><p>也许上面的图中λ为0时很好理解，但是到3和4的时候比较难理解，那么我们举一个极端的例子帮助大家理解一下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklknylerjj20mt0d0dh5.jpg" alt="A special example: a line graph with 20 nodes"></p><p>上面讲解的内容都是为我们找到Fourier Transform打基础的，我们先来看看在传统的信号与系统里面是怎么进行Fourier Transform的~（time domain – &gt; frequency domain）</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkll3kcjetj20np0bxwh1.jpg" alt="Graph Fourier Transform of signal"></p><p>如果要转换回去，我们应该怎么转呢？也就是从spectral domain转换到vertex domain之上~</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkllh0h0zxj20lx0cp0w1.jpg" alt="K6QT100$JB0E_CYST}V~4$K.png"></p><p>类比到我们的Spectra Graph上面就是：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklli95b7bj20nl0d3dhx.jpg"></p><p>接下来，我们来讲解一下filtering</p><p>![`J){PE9TBO](<a href="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkllqnu3cwj20nq0dvmyf.jpg">http://ww1.sinaimg.cn/large/9b63ed6fgy1gkllqnu3cwj20nq0dvmyf.jpg</a>)</p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkllr4qhcoj20l40cxmyn.jpg" alt="5"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklltfq35bj20o909e0tj.jpg"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklmeso1x0j20nd0bf0ty.jpg" alt="24"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklmrps3kej20n20citb5.jpg" alt="GNL$7HAKFX.png"></p><hr><p>![5Z89Z1Z6`@)B)BDZB1OC5FC.png](<a href="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklmsu93rrj20nk0cggo3.jpg">http://ww1.sinaimg.cn/large/9b63ed6fgy1gklmsu93rrj20nk0cggo3.jpg</a>)</p><p>区分L和L^2的不同</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklmvp8k2gj20n9021754.jpg" alt="(E7SGC7E"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gklmw921loj20o405mmyc.jpg"></p><hr><p>什么叫做localize呢？我们再次回顾我们在CNN中学到的那些东西~</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkln395l84j20jf0bswf6.jpg"></p><p>receptive field：接受域</p><h3 id="ChebNet"><a href="#ChebNet" class="headerlink" title="ChebNet"></a>ChebNet</h3><p>第一个Spectral-based GNN就是咱们的ChebNet啦~需要注意的是，我们的每一个模型都应该能够解决我们之前提到的两个图神经网络的问题：</p><ol><li>Learning Complexity</li><li>Not Localize</li></ol><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkmcr7nsjlj20n60chjso.jpg" alt="ChebMet"></p><p>通过让$g_{\theta}(L)$是$L$的多项式函数，我们就能让$g_{\theta}(L)$是一个$K-localized$的函数。同时，此时要学的参数也是$K$个了，复杂度为$O(K)$。</p><p>现在还存在一个问题就是在计算$U(\sum^K_{k=0}\theta_k\Lambda^k)$的时候，时间复杂度（time-complexity）为$O(N^2)$，同时，计算eigenvector本身就是一个计算量特别大的问题。</p><p>综合以上种种的问题，ChebNet使用了一个特殊的多项式来解决这个问题。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkmcrxy9nnj20lk0b5q4w.jpg" alt="Chebyshev Polynomial是一个使用递归进行定义的多项式函数家族~据说是数值分析里面的？"></p><p>我们将Chebyshev Polynomial中的$x$替换成$\tilde \Lambda$，这样变形的原因就是为了满足在[-1,1]之间。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkmcz9vau7j20k30admxz.jpg" alt="我们用新的Chebyshev Polynomial来代替之前的函数得到上面的结果"></p><p>至于为什么要使用这个Chebyshev Polynomial代替呢？原因如下：</p><p>目的是为了使得$T_k(\tilde \Lambda)$是很好算的。至于为什么要这样代换呢？好像没讲清。</p><p>利用切比雪夫正交且递归的性质减少计算量。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkmdhoatlnj20ky09rwfj.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkmdi4cmb4j20mv0argmt.jpg"></p><hr><p>就想CNN里面，你可以有不同的channel，</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkmdk8pq26j20lv07j74u.jpg" alt="YI%ng"></p><hr><h3 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h3><p>迷迷糊糊没有看懂</p><h2 id="Comparison-between-the-above-GNNS——Tasks-Dataset-and-Benchmark"><a href="#Comparison-between-the-above-GNNS——Tasks-Dataset-and-Benchmark" class="headerlink" title="Comparison between the above GNNS——Tasks,Dataset,and Benchmark"></a>Comparison between the above GNNS——Tasks,Dataset,and Benchmark</h2><p>benchmark test有下面这5个：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkme6a8v4oj20fg0823z1.jpg" alt="9BV@L~HGQQMEE$80WI8~D%C.png"></p><p>因为要做benchmark，那么dataset一定要大~</p><p>下面使用的是SuperPixel MNIST 和 CIFAR10</p><p>第二个是ZINC molecule graphs dataset，预测溶解度</p><p>第三个，任务是辨认graph里面的pattern，以及聚类。</p><p>第四个，旅行商问题</p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息抽取 从零入门</title>
      <link href="/2020/11/10/xin-xi-chou-qu-cong-ling-ru-men/"/>
      <url>/2020/11/10/xin-xi-chou-qu-cong-ling-ru-men/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文链接：<a href="https://ivenwang.com/2020/06/06/knowledgeextraction/">信息抽取 从零入门</a></p></blockquote><p><strong>信息抽取</strong>包括<strong>实体抽取</strong>、<strong>关系抽取</strong>、<strong>事件抽取</strong>。</p><ul><li>实体抽取即找出文章里的专有名词（人名地名等），方法大致是先检测，再分类；</li><li>关系抽取即找出两个名词之间的动词，一般是三元组抽取，即一个谓词（predicate）带两个形参（argument）；</li><li>事件抽取即将自然语言转换成结构化的信息，抽取出事件的各要素（时间、地点、人物等）。</li></ul><h2 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1 数据集"></a>1 数据集</h2><p>最常用的数据集有 MUC、ACE、KBP、SemEval 等。其中 ACE 数据集包括 <a href="https://catalog.ldc.upenn.edu/desc/addenda/LDC2006T06.txt">英语</a>、<a href="https://catalog.ldc.upenn.edu/desc/addenda/LDC2006T06_ch.jpg">中文</a> 等。</p><h2 id="2-实体抽取"><a href="#2-实体抽取" class="headerlink" title="2 实体抽取"></a>2 实体抽取</h2><p>主要抽取的是文本中的原子信息元素，如人名、组织/机构名、地理位置、事件/日期、字符值、金额值等。实体抽取任务有两个关键词：find &amp; classify，找到命名实体，并进行分类。</p><p>传统的机器学习方法</p><ol><li>把每个 token 标记命名实体；</li><li>特征选择；</li><li>训练一个 sequence classifier 来预测数据的 label。</li></ol><p>…</p><h2 id="4-事件抽取"><a href="#4-事件抽取" class="headerlink" title="4 事件抽取"></a>4 事件抽取</h2><p>事件抽取是从描述事件信息的文本中，识别并抽取出事件信息，并以<strong>结构化的形式</strong>呈现出来，包括发生的时间、地点、参与角色以及与之相关的动作或者状态的改变。[8]</p><p>几个概念：</p><ul><li><strong>事件描述（Event Mention）</strong>：描述事件的词组/句子/句群，包含一个 trigger 以及任意数量的 arguments</li><li><strong>事件触发（Event Trigger）</strong>：事件描述中最能代表事件发生的词汇，决定事件类别的重要特征，一般是动词或者名词</li><li><strong>事件元素（Event Argument）</strong>：事件的重要信息，或者说是实体描述（entity mention），主要由实体、属性值等表达完整语义的细粒度单位组成</li><li><strong>元素角色（Argument Role）</strong>：事件元素在事件中扮演的角色，事件元素与事件的语义关系，可以理解为 slot</li><li><strong>事件类型（Event Type）</strong></li></ul><p>事件抽取基础任务是在 mention 中抽取一个 trigger 和多个 arguments，并找到每个 argument 对应的 role，以及 trigger 的 type。</p><p>因此基础的方法分成几步[9]：</p><ol><li>Trigger Identification</li><li>Trigger Type Classification</li><li>Argument Identification</li><li>Argument Role Classification</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> 信息抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Open Domain Event Extraction</title>
      <link href="/2020/11/09/a-survey-of-open-domain-event-extraction/"/>
      <url>/2020/11/09/a-survey-of-open-domain-event-extraction/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0. 摘要"></a>0. 摘要</h2><p>为了从无结构的文本语料库中挖掘有用的结构化知识，人们创造了许多的信息抽取系统。其中一种十分重要的就是开放域事件抽取，它是在没有预先给定域假设（domain assumption）的情况下抽取有用的事件信息。</p><p>在这篇综述中，<strong>首先介绍事件抽取的pipeline是怎样一个流程，然后介绍一些目前比较好的开放域事件抽取。</strong></p><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>事件抽取的主要任务有：</p><ol><li><strong>判别事件的类型</strong>；</li><li>**提取出事件的每一个<code>role</code>所对应的<code>argument</code>**。</li></ol><p>一个简单的例子如下：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj9wmhf3tj20jh09yjsd.jpg" alt="an example for event extraction"></p><p>在整篇综述中：</p><ul><li>Section 2介绍有哪些不同种类的事件抽取；</li><li>Section 3介绍两个东西，一是常用于事件抽取中的知识库，二是可自动生成用于事件抽取训练数据的技术；</li><li>Section 4，5，6介绍最近比较火的事件抽取方法和开放域事件抽取方法；</li><li>最后就是总结</li></ul><h2 id="2-Types-of-Event-Extraction"><a href="#2-Types-of-Event-Extraction" class="headerlink" title="2. Types of Event Extraction"></a>2. Types of Event Extraction</h2><p>有两种划分事件抽取的方法：</p><ol><li>根据事件抽取的目标划分成开放域和特定域；</li><li>根据事件抽取使用的方法划分成<strong>数据驱动方法</strong>，<strong>语义驱动方法</strong>和<strong>混合驱动方法</strong>。</li></ol><h3 id="2-1-Event-Extraction-Types-by-Approaches"><a href="#2-1-Event-Extraction-Types-by-Approaches" class="headerlink" title="2.1 Event Extraction Types by Approaches"></a>2.1 Event Extraction Types by Approaches</h3><ul><li>data-driven approach</li><li>semantic-driven approach</li><li>hybrid-driven approach</li></ul><h3 id="2-2-Event-Extraction-Types-by-Objectives"><a href="#2-2-Event-Extraction-Types-by-Objectives" class="headerlink" title="2.2 Event Extraction Types by Objectives"></a>2.2 Event Extraction Types by Objectives</h3><p><strong>open domain</strong>的特点：</p><ul><li>社交媒体和大量文本语料库上的事件抽取常被看作是开放域的；</li><li>缺乏相关的ontologies，以及存在着大量待发掘的事件类型和模式使得开放域事件抽取非常地困难；</li></ul><p><strong>specific domain</strong>的特点：</p><ul><li>有相关的ontologies和knowledge bases</li></ul><h2 id="3-Data-and-Ontologies"><a href="#3-Data-and-Ontologies" class="headerlink" title="3. Data and Ontologies"></a>3. Data and Ontologies</h2><p>目前有许多的开放域事件抽取模型的训练数据是hand-labeled data，hand-labeled data存在三点弊端：</p><ol><li>价格昂贵</li><li>事件类型的覆盖率少</li><li>数量有限</li></ol><p>综上，自动产生标签数据是非常重要的，接下来首先安利一些有利于事件抽取的数据和ontologies资源，再讲解两种自动产生标签数据的方法：</p><ol><li>通过识别出<code>key arguments</code>和<code>trigger words</code>来产生标签数据；</li><li>通过识别<code>key arguments</code>来产生标签数据。</li></ol><h3 id="3-1-Resources-of-Data-for-Event-Extraction"><a href="#3-1-Resources-of-Data-for-Event-Extraction" class="headerlink" title="3.1 Resources of Data for Event Extraction"></a>3.1 Resources of Data for Event Extraction</h3><p>为了产生labeled data，不仅需要word knowledge，也需要linguistic or semantic knowledge。下面是一些可以用于生成labeled data或者直接作为labeled data的knowledge base和ontologies：</p><h4 id="3-1-1-FreeBase"><a href="#3-1-1-FreeBase" class="headerlink" title="3.1.1 FreeBase"></a>3.1.1 FreeBase</h4><p>FreeBase是一个semantic knowledge base。FreeBase使用<code>Compound Value Types</code>来将多个值组合成一个值。（有点类似于面向对象中，一个类有多个属性）</p><p>为了利用FreeBase来产生labeled data：</p><ul><li>CVT常被看作是events</li><li>type of CVTs看作是event types</li><li>CVT instances看作是event instance</li><li>values in CVTs看作是arguments in events</li><li>roles of CVTs看作是roles of arguments in the event</li></ul><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkjvat3j4rj20lc03ogm4.jpg" alt="An example sentence from Wikipedia"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkjvboz2ilj20kh06tabc.jpg" alt="the corresponding CVT entry of the sentence in Figure 2 in FreeBase"></p><h4 id="3-1-2-FrameNet"><a href="#3-1-2-FrameNet" class="headerlink" title="3.1.2 FrameNet"></a>3.1.2 FrameNet</h4><p>FrameNet是一个语言库，FrameNet里面的每一个frame都可以看做一种event的语义框架。每一个frame都有许多的Lexical Units。例如在下面的句子中bake.v就是frame Cooking_creation的LU之一，这样的frame可以映射到FreeBase中，从而用作为文本数据打标签。</p><pre><code class="tex">Michelle baked her mother a cake for her birthday.</code></pre><p><a href="https://framenet.icsi.berkeley.edu/fndrupal/WhatIsFrameNet">FrameNet的介绍</a></p><h4 id="3-1-3-Wikipedia"><a href="#3-1-3-Wikipedia" class="headerlink" title="3.1.3 Wikipedia"></a>3.1.3 Wikipedia</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkjwc4vcwsj20gc09pabg.jpg" alt="Table 1:Wikipedia Multilingual statics"></p><p>维基百科是一个半结构化以及多语言的知识库，它每年的增长速度很快，关于它的作用，在Section 5中会详细提到。</p><h3 id="3-2-Generate-Labeled-Data-for-Event-Extraction-Automatically"><a href="#3-2-Generate-Labeled-Data-for-Event-Extraction-Automatically" class="headerlink" title="3.2 Generate Labeled Data for Event Extraction Automatically"></a>3.2 Generate Labeled Data for Event Extraction Automatically</h3><h4 id="3-2-1-Generate-labeled-data-for-event-extraction-by-identifying-trigger-words-and-key-arguments"><a href="#3-2-1-Generate-labeled-data-for-event-extraction-by-identifying-trigger-words-and-key-arguments" class="headerlink" title="3.2.1 Generate labeled data for event extraction by identifying trigger words and key arguments"></a>3.2.1 Generate labeled data for event extraction by identifying trigger words and key arguments</h4><p>这种方法来产生的标注数据标注了trigger words，event types，arguments和arguments role。如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkk15zv0jcj20k506owfg.jpg" alt="figure4"></p><p>在<strong>关系抽取</strong>中，有一种称为distant supervision的方法，它假设：如果两个实体在知识库中存在关系，那么所有包含这两个实体的句子都会在一定程度上体现这种关系。</p><p>然后这种DS的思想，并不适用于<strong>事件抽取</strong>，主要有两个原因：</p><ol><li>在现有的知识库中，事件的trigger words并没有被体现出来；</li><li>一个事件实例的arguments可能出现在文章的不同地方，而不是集中出现在一个句子之中，所以很难产生高质量的打标训练数据；</li></ol><p>为了解决DS算法用于事件抽取上的不足，有一种同时使用了world knowledge（FreeBase）和linguistic knowledge（FrameNet）的算法。它的pipeline如下：</p><ol><li>选择key arguments；</li><li>只是用key arguments来标记events以及提取trigger words；</li><li>使用linguistic knowledge resource（FrameNet）来过滤噪声trigger words和扩大triggers；</li><li>使用Soft Distant Supervision来自动回标数据。</li></ol><p>第一步选择key arguments的方法是计算key rate，计算key rate包含两个部分：</p><ol><li><p>Role Saliency（作用特点）</p><p>given an event type, an argument A is more salient than another argument B if one tends to use an argument A to distinguish one event instance.</p></li><li><p>Event Relevance（事件相关性）</p><p>if an argument A only occurs in a specic event type, the argument A has high event relevance.</p></li></ol><p>基于key rate然后选取topK。</p><p>第二步使用key arguments来标记trigger word。首先找到所有包含了key arguments的句子，然后筛选出这些句子中的动词（verb），出现频率较高的动词就是我们要找的trigger words。</p><p>第三步是trigger words的过滤和扩展。使用的方法是将FreeBase中的event映射到FrameNet中的frame。</p><p>第四步是自动回标，产生训练数据。这一步基于两个假设：</p><ol><li>对于句子中的一个事件，FreeBase中所有的key arguments和对应的trigger words代表了这个事件；</li><li>句子中出现的arguments承担了在事件中相应的role</li></ol><p>基于上面的两个鸡舍，我们使用soft distant supervision来生成数据。</p><h4 id="3-2-2-Generate-labeled-data-for-event-extraction-only-by-identifying-key-arguments"><a href="#3-2-2-Generate-labeled-data-for-event-extraction-only-by-identifying-key-arguments" class="headerlink" title="3.2.2 Generate labeled data for event extraction only by identifying key arguments"></a>3.2.2 Generate labeled data for event extraction only by identifying key arguments</h4><p>传统的事件抽取方法需要抽取trigger words和赋予事件类型。然而有些论文认为identification of trigger并不是必须的，仅仅只有key arguments就能够表示一个事件的类型！</p><p>由于这个方法只使用了key arguments，所以它的pipeline会短一些：</p><ol><li>从一个CVT表中识别出key arguments；</li><li>通过已有的结构化表/列表，产生打标数据。</li></ol><p>在第一步中，论文使用了一个叫做importance score的东西来度量arguments的重要性。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkk2u5t6qqj20lh0853zv.jpg" alt="Figure 5"></p><p>key arguments的筛选策略如下：</p><ul><li>计算importance score，然后排名最高的前二分之一的arguments；</li><li>总是选择time-related的arguments作为key arguments；</li><li>从得到的数据集中，删除dependency distance between two key arguments大于2的句子。</li></ul><h2 id="4-Contemporary-Approaches-Evaluation"><a href="#4-Contemporary-Approaches-Evaluation" class="headerlink" title="4. Contemporary Approaches Evaluation"></a>4. Contemporary Approaches Evaluation</h2><h3 id="4-1-Event-Schema-Extraction-By-Trigger-Clustering"><a href="#4-1-Event-Schema-Extraction-By-Trigger-Clustering" class="headerlink" title="4.1 Event Schema Extraction By Trigger Clustering"></a>4.1 Event Schema Extraction By Trigger Clustering</h3><p>有一种事件抽取的方法叫做<strong>ACE</strong>，它的思想是人工地定义事件的schema。图6的左半部分表示了ACE中的一个event schema。这种方法非常的昂贵，因为消费者和专家在标注事件类型和arguments role之前需要浏览大量的数据。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkk3e2kxw1j21540ghwnf.jpg" alt="Figure 6"></p><p>paper[8]介绍了一种新的方法来进行事件抽取。为了从句子中抽取事件，论文首先从句子中抽取了trigger words和对应的arguments。对于每个trigger word，对它进行聚类然后使用聚类的名字作为事件类型。然后它基于事件类型来指定arguments。</p><p>在这种方法中最重要的一个子任务就是<strong>trigger words的表示</strong>。通常trigger words是通过distributional vectors进行表示的。更进一步，有两种word embedding的方法。</p><p>第一种是出现在相似上下文，同时有着相同的含义的trigger word可能有相似的类型，因此一个trigger word的distributional vectors应该包含语义信息。</p><p>第二种是trigger type是依赖于它的arguments，arguments role以及其他上下文中与这个trigger相关的单词，所以distributional vectors应该包含它的argument语义信息。为了获得arguments语义信息，paper[8]使用语义关系来说明distributional semantics of relevant word是怎样来促进全局的时间结构表示的。？？？</p><p>接下来讲解如何产生聚类，找到每个聚类的arguments和命名每个聚类。</p><p>第一步是识别candidate triggers和arguments。为了识别triggers，首先考虑被WSD赋予了一个OntoNotes意义的所有名字和动词作为candidate triggers，然后剩下的单词如果是FrameNet中的LU，那么也考虑为candidate triggers。为了为每一个trigger识别arguments，paper[8]使用了一种AMR的方法。</p><p>第二步是表示trigger和argument成distributional vectors。首先使用WSD在WordNet上找到每个word的释义，然后将WordNet的释义变成OntoNotes的释义。这样，对于每一个trigger candidate，都能得到得到它的OntoNotes释义，利用这个释义，学习得到embedding即可。在通常意义上，word embedding都是在大数据集上使用skip-gram model得到的。对于arguments，paper[8]使用一个一般的词法embedding来表示。</p><p>第三步是事件结构的组成和表示。对于每个事件trigger，使用一系列的compositional functions来产生trigger event结构表示。</p><p>每一个compositional function是语义关系特异的，而且函数是操作在embedding空间上的。对于每个argument，它的表示是以一种附属产物的形式生成的。</p><p>第四步是使用一个联合的约束聚类框架来聚类arguments和triggers。有一个关于trigger的假说是如果两个triggers，arguments有相同的类型和role，那么他们很肯定属于同一个type。因此，论文[8]使用一个限制函数来执行互依赖triggers和arguments来有相同的type。至于聚类函数，paper[8]设计了一个联合约束聚类方法，这种方法迭代地生产新的聚类结果。</p><p>最后一步是命名triggers和每个trigger arguments。paper[8]使用meaning representation与semantic role description之间的映射来解决这个问题。</p><h3 id="4-2-Deep-Learning-Approaches"><a href="#4-2-Deep-Learning-Approaches" class="headerlink" title="4.2 Deep Learning Approaches"></a>4.2 Deep Learning Approaches</h3><h4 id="4-2-1-基于RNN的方法"><a href="#4-2-1-基于RNN的方法" class="headerlink" title="4.2.1 基于RNN的方法"></a>4.2.1 基于RNN的方法</h4><blockquote><p>RNN是啥呢~</p></blockquote><h4 id="4-2-2-基于CNN的方法"><a href="#4-2-2-基于CNN的方法" class="headerlink" title="4.2.2 基于CNN的方法"></a>4.2.2 基于CNN的方法</h4><p>这一部分，我们介绍一篇论文，这篇论文是使用了Zero-Shot Learning的CNN。许多传统的监督学习方法无法处理新的event type，因为新的event type意味着无法使用到老的events。特别地，这些方法通常将事件抽取任务当做是分类问题，将事件的特征用事件和老事件的相似性大小来表示。模型中的事件类型和参数角色扮演强制限制的原子符号角色。然而在这篇论文中，作者提出了一个新的方法，能够帮助解决这种问题。</p><p>他们发现的有意思的事情是无论是event types还是event mentions都能够通过一些<strong>结构形式</strong>表示出来。例如，如果我们用AMR表示每个event mention，用Entity Relation Entity structure表示event types，我们能够发现两种结构之间是有一定的关联关系的。事件类型中的事件提及共享类似的结构信息。根据理论“事件结构的语义可以以系统和可预测的方式概括和映射到事件提到结构。”他们采取的方法主要是将每一事件映射到本体中语义上最接近的事件类型。（迷迷糊糊）</p><blockquote><ul><li><code>Event mention</code>：描述事件信息的短语或者句子，即同时包含了触发词（trigger）和论元（argument）。</li><li><code>Event trigger</code>: 短语或者句子中，最能表示一个事件出现的单词，在ACE数据中，这个词通常是动词或名词。</li><li><code>Event argument</code>：短语或者句子中，包含的实体、时间表达式、或者数值。</li><li><code>Argument role</code>: 一个论元在事件中扮演的角色。</li></ul></blockquote><h2 id="一些概念定义"><a href="#一些概念定义" class="headerlink" title="一些概念定义"></a>一些概念定义</h2><h3 id="Close-domain"><a href="#Close-domain" class="headerlink" title="Close-domain"></a>Close-domain</h3><p>Closed-domain事件抽取使用<strong>预定义的事件模式</strong>从文本中发现和提取所需的特定类型的事件。事件模式包含多个事件类型及其相应的事件结构。我们使用ACE术语来介绍如下事件结构：</p><p>事件提及</p><blockquote><p>描述事件的短语或句子，包括触发词和几个参数。</p></blockquote><p>事件触发词</p><blockquote><p>最清楚地表达事件发生的主要词，尤指动词或名词</p></blockquote><p>事件论元</p><blockquote><p>在事件(即参与者)中涉及的实体提及、时间表达或值(例如工作头衔)。</p></blockquote><p>论元角色</p><blockquote><p>论元与它所参与的事件之间的关系。</p></blockquote><p>D.Ahn首先提出将ACE事件提取任务分成四个子任务:<strong>触发词检测</strong>、<strong>事件/触发词类型识别</strong>、<strong>事件参数检测</strong>和<strong>参数角色识别</strong>。</p><h3 id="Open-domain"><a href="#Open-domain" class="headerlink" title="Open-domain"></a>Open-domain</h3><p>在<strong>没有预定义的事件模式</strong>的情况下，开放域事件提取的目的是从文本中检测事件，在大多数情况下，还可以通过提取的事件关键字聚类相似的事件。事件关键字指的是那些主要描述事件的词/短语，有时关键字还进一步分为触发器和参数。</p><p>故事分割</p><blockquote><p> 从新闻中检测故事的边界。</p></blockquote><p>第一个故事检测</p><blockquote><p>检测新闻流中讨论新话题的故事。</p></blockquote><p>话题检测</p><blockquote><p>根据讨论的主题将故事分组。</p></blockquote><p>话题追踪</p><blockquote><p>检测讨论先前已知话题的故事。</p></blockquote><p>故事链检测</p><blockquote><p>决定两个故事是否讨论同一个主题。</p></blockquote><p>前两个任务主要关注事件检测;其余三个任务用于事件集群。虽然这五项任务之间的关系很明显，但每一项任务都需要一个不同的评价过程，并鼓励采用不同的方法来解决特定问题。</p><h3 id="What-is-ontology？"><a href="#What-is-ontology？" class="headerlink" title="What is ontology？"></a>What is ontology？</h3><p>outside philosophy，ontology is used in a different，more narrow meaning。</p><p>Here,an ontology is the description of what exist specifically within a determined field. For example,every part that exists in a specific information system. This includes the relationship and hierarchy between these parts.</p><p>Unlike the philosophers,these researchers are not primarily interested in discussing if these things are the true essence,core of the system.</p><p>Nor are they discussing if the parts within the system are more real compared to the processes that take place within the system.</p><p>Rather,they focus on naming parts and processed and grouping similar ones together into categories.</p><p>The word ontology is also use,for example,in social ontology. Here,the idea is to describe society and its different parts and processes. The purpose of this is to understand and describe the underlying structures that affect individuals and group.</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> NLP </tag>
            
            <tag> 综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>「开放域事件抽取」入门了解</title>
      <link href="/2020/11/08/kai-fang-yu-shi-jian-chou-qu-ru-men-liao-jie/"/>
      <url>/2020/11/08/kai-fang-yu-shi-jian-chou-qu-ru-men-liao-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="中科院赵军：开放域事件抽取-CCF-GAIR-2018"><a href="#中科院赵军：开放域事件抽取-CCF-GAIR-2018" class="headerlink" title="中科院赵军：开放域事件抽取 | CCF-GAIR 2018"></a>中科院赵军：开放域事件抽取 | CCF-GAIR 2018</h2><blockquote><p>文章摘自知乎——<a href="https://zhuanlan.zhihu.com/p/41207156">中科院赵军：开放域事件抽取 | CCF-GAIR 2018</a></p></blockquote><p>以下是赵军教授发表的题为「开放域事件抽取」的演讲全文，<strong>雷锋网 AI 科技评论在赵军教授的帮助下，做了不改变原意的整理与编辑：</strong></p><p>非常感谢刘挺老师和 CCF-GAIR 2018 大会的邀请。刚才刘老师也提到了，我本次报告与知识有关。自然语言处理是人工智能的一个重要应用方向，上世纪五六十年代，人工智能的主要研究是，搜索的算法或者推理的算法。但是，人们在这个方面研究了一段时间后，意识到有一个问题不可回避，那就是知识。<strong>我们很多系统中没有领域的知识或者专家的知识，在这种情况下，无法去做推理算法。</strong>这时，知识就成为人工智能一个非常重要的问题。</p><p>1977 年，Feigenbaum 提出，知识是人工智能非常重要的方向。2012 年，Web2.0 已经面世，网络上有了维基百科、百度百科等宝贵的知识资源。再加上信息抽取等自然语言处理技术的进展，<strong>这使得以前依靠专家来建立知识库的传统方法发生了显著的变化</strong>，知识库的规模和类型也都发生了显著的变化，知识工程再次成为人工智能的一个热点，它跟深度学习和情感一样，都是新一代人工智能的很有代表性的工作。</p><p>知识图谱有以下几种：</p><ol><li><p><strong>实体图谱</strong>，是一种是我们常见的以实体为中心的图谱。例如，图谱中间的一块上，每一个节点都是一个实体，例如 Barack Obama 和 Michelle Obama，它们之间通过夫妻这种关系联系起来，现在的大多数图谱就是这样的。</p></li><li><p><strong>事件图谱</strong>，事件的知识图谱是应用中不可缺少的一类图谱，其中的每一个节点是一个事件，事件之间通过事件的关系（比如时序关系、因果关系等）相关联，这就叫做事件的图谱。</p></li></ol><p>实体图谱和事件图谱，对于我们做问答，以及其他应用来说都必不可少。<strong>无论是实体图谱还是事件图谱，我们不可能完全依靠人工去构建，我们需要关键技术的支撑，这个关键技术就是信息抽取的技术。</strong></p><blockquote><p>知识图谱是一种新的知识库。</p><p>知识图谱是由节点和节点之间的关系构成的。</p><p>信息抽取技术用于构建实体图谱和事件图谱。</p></blockquote><p>实体的识别是最基础的，有了实体以后，做实体图谱需要做关系抽取，比如（比尔盖茨是微软的 CEO），我们要转成三元组的结构化方式，CEO（比尔盖茨，微软）。当然。还存在多元的关系，它们都可以转成二元的关系，这样存储和应用的时候，效率更高。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkhteqej1ej20f70begmk.jpg" alt="事件抽取"></p><blockquote><p>事件可以由一个多元组构成——(Trigger,event-type,argument-1,role-1,argument-N,role-N)</p></blockquote><p><strong>事件的抽取与事件图谱相关联</strong>，举个例子，土耳其的飞机失事事件，这样一个事件的类别属于恐怖事件，袭击的目标是俄罗斯战机，袭击的工具是 F-16，还有地点和时间，这就是事件。<strong>我们要从一个文本当中找出这样的事件信息并进行结构化，这就是事件的抽取。</strong></p><p>有关事件的关系其实也有很多，目前，我们应用方面比较关注的是事件的同指关系，还有时序关系、因果关系、上下位关系等。</p><p>今天我讲的是事件的抽取，也捎带讲一点关系的抽取。</p><blockquote><p>其实接下来讲了很多关系的抽取……</p></blockquote><p>刚才，我主要讲解的是开放域的事件抽取。我们先了解什么是开放域，在谈到开放域之前，来看看传统的<del>关键抽取</del>（关系抽取）是什么样子的。传统的叫<strong>预定义的关系抽取</strong>。</p><p>我们抽取的<strong>目标关系类别是定义好的</strong>，我们<strong>给定的语料的结构是比较单一的</strong>，这种就是预定义的关系抽取。</p><p>学界、产业界在这方面做得很多，国际上也有评测，一般都给定一些标注语料，这都是有监督的关键抽取的技术。</p><p><code>ACE</code> <a href="ACE%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%9B%AE%E6%A0%87%E6%98%AF%E5%BC%80%E5%8F%91%E8%87%AA%E5%8A%A8%E5%86%85%E5%AE%B9%E6%8F%90%E5%8F%96%E6%8A%80%E6%9C%AF%EF%BC%8C%E5%AE%83%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A4%A7%E7%9A%84%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%8CACE2005%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E6%8B%AC%E8%8B%B1%E8%AF%AD%EF%BC%8C%E9%98%BF%E6%8B%89%E4%BC%AF%E8%AF%AD%E5%92%8C%E4%B8%AD%E6%96%87%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%94%A8%E6%9D%A5%E5%81%9A%E5%AE%9E%E4%BD%93%EF%BC%8C%E5%85%B3%E7%B3%BB%EF%BC%8C%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%AD%89%E3%80%82%5BCSDN%E9%93%BE%E6%8E%A5%5D(https://blog.csdn.net/bobobe/article/details/98791250)">^1</a>是美国的一个评测，评测的就是这个表中列出的实体关系类别。</p><p>这是 <code>SemEval</code> 的实体关系的类别。</p><p>这是 <code>TAC-KBP</code> 的实体关系类别，都是预定好的。</p><p>预定义的关系抽取，有各种各样的方法来做预定义的关系抽取。现在，大家所共同关注的还是深度学习的方法。2014 年 COLING 上，我们首次使用深度学习的方法做了基于卷积神经网络的预定义关系抽取。基于深度学习的关系抽取方法在性能方面，较传统方法有明显的提升。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkhtqna9q0j20f70bet9c.jpg" alt="基于卷积网络的预定义关系抽取"></p><p><strong>那么为什么要研究开放域关系抽取呢？</strong>我们来看：</p><p><code>Freebase</code> [^2]：4000 多万实体，上万个属性关系，24 多亿个事实三元组，这样级别的关系抽取，如果还是依靠刚才的那种方法，<strong>依靠人工标注的训练集</strong>，在这方面肯定行不通，这时我们就必须考虑一些自动的或者弱监督、半监督的方法，<strong>来做开放的关系抽取</strong>。</p><p>难点问题在于<strong>如何获取训练语料</strong>。有了语料还不行，我们还需要研究<strong>新的抽取方法</strong>。在这方面，国际上有两个有代表性的开放域关系提取的研究方法，<strong>一个是基于句法的方法</strong>，<strong>一个是基于知识监督的方法</strong>。</p><p><strong>基于句法的方法</strong>，是美国华盛顿大学图灵实验室做的一系列工作，例如，（华为，总部位于，深圳），语料库中有各种表述方法，我们可以抽出（华为总部位于深圳，华为总部设置于深圳，华为将其总部建于深圳），都是相关的知识。我们需要通过<strong>句法分析器</strong>，对这样的句子找出三元结构，抽取出来放在一起，这是我们所需要的知识。</p><p>基于句法的方法的核心是句法分析器。然而，很多找出来的句法三元组并不是我们需要的有实际含义的三元组，这时<strong>我们需要人工设计一些规则，把这些有实际含义的三元组过滤出来，这就是基于句法的关系抽取的主要思想。</strong>这种方法存在的问题是：这些实体关系三元组知识抽取出来放在了一起，它们到底代表什么语义还不明确，它的语义并没有和人类的知识库挂接，所以这还不是一种彻底的理解。而且，同样的关系有各种各样的语言表示，没有归一化，所以，如何应用还存在很多问题。</p><p><strong>基于知识监督的方法</strong>。2007 年 CIKM 的论文最早提出这样的思想，在 Wikipedia 中可以分成两个区域，一个区域是结构化部分，我们叫做 Infobox，另一个区域是自然语言表达的部分，这两部分描述的信息有重叠，比如描述清华大学和建校时间的知识，在 Infobox 和自然语言里面有重叠的描述，如果把这两部分对应起来，就可以对应两边区域的知识，一边是它的训练集。这是一个非常简单的思想。如果用 Infobox 的结构化信息在 wikipedia 条目的自然语言文本中进行回标，<strong>可以自动产生训练语料。</strong></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkhugl51vlj20f70bewfj.jpg" alt="简单远距离监督方法"></p><p>Mintz 发展了这种思想，提出了远距离监督方法或者说是<strong>知识监督方法</strong>。大家看这个例子，第一个例子是正例，剩下的都是反例（也就是噪音）。我们需要把里面标注的例子中的噪音例子给过滤掉。噪音问题目前是利用知识监督方法建立训练集的最大挑战。大家在这方面做了很多研究，主要思想是：<strong>正例有规律的出现，反例是零零散散出现，借助这样的思想进行过滤。</strong></p><blockquote><p>在上图中，噪音数据和正例都出现了“乔布斯”和“苹果公司”，基于DS假设，就会有问题！</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkhuk9rbqgj20f70bejs2.jpg" alt="基于噪音实例去除的DS方法"></p><p>这是我们做的开放关系抽取的研究工作。</p><p>我们用 <code>Freebase</code> 作为结构化知识，在纽约时报的文本上进行回标。作为过滤噪音的方法，我们用了<strong>多示例学习</strong>。在传统的方法中，假设回标的每个句子都表示这种关系，它的噪音就很多。在多学习示例当中，我们假定至少有一个句子表示了这种关系，目的就是要把最有可能的句子标注出来，这样它的准确率就比刚才那一个包里面的准确率高了，性能就会提高。</p><p>这件工作也是在深度学习框架下做的。因为做关系抽取，需要有两个实体，可以把句子分成三段。我们做深度学习和向量化的时候，不是一个句子做向量，而是把句子分成三部分，三部分分布做深度卷积操作，三部分的向量再合起来，来做整个句子向量化表示，这样可以保留句子的更多结构化信息，我们把这个模型叫分段卷积神经网络来自动学习特征。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkhusahzhnj20f70bedgh.jpg" alt="基于多示例学习的知识监督开放关系抽取"></p><p>这是我们标注的知识库和语料库，Freebase 和纽约时报。我们如果这样做的话，可以达到相对比较好的性能（大概在 70% 多的水平），虽然比较低，但已经比传统方法高，因为这是在 Freebase 的很多关系的类别上做的，能做到这个程度，其实就可以看到希望。</p><p>后面讲一下<strong>事件抽取</strong>。这是 ACE 的事件，可以定义的事件有这么多种。</p><p>预定义的事件抽取，预定义的事件抽取也是用神经网络做的。<strong>开放的事件抽取比开放的关系抽取要困难得多，为什么？</strong></p><p><strong>一个关系是由两个实体、以及它们之间的关系构成的。而一个事件不一样，比如一个婚姻事件，它有五个要素。</strong></p><p>对于关系抽取，我们可以把两个实体作为锚点，在文本中标注。而一个事件有五个要素，不可能在一个句子中找全五个要素，因为，事件经常横跨几句、甚至一 个段落才能找到要素。还有一个更重要的特征，中间这个是 Marriage，在 Freebase 里面表示为 ID 号，在文本当中不可能找到对应的位置，所以这个最鲜明的特征我们找不着，所以回标的过程中遇到了非常大的困难。换句话，事件里面最有表征意义的是那个触发词，但是在知识库 Freebase 中只是一个标号，所以触发词就没有，这就很困难。</p><blockquote><p>什么是触发词？什么是知识库？<code>shot down</code>？</p></blockquote><p>我们的方法，比如一类事件有 10 个要素，10 个要素不可能都出现，但是一个事件里面会有一些核心要素，我们就从一堆要素当中找出核心要素，用核心要素到句子当中找到触发词，将触发词和前面的要素关联到一起，再回标，就可以在文本当中找到更多数据，这就是我们的基本思想。在这件工作中，我们在 Freebase 上做了 21 类，ACE 只提供了 6000 个句子训练集，用我们这样的方法可以找到 42 万的语料，再过滤掉一些噪音，可靠性非常高的有 7 万多句，然后再训练事件抽取模型，触发词识别正确率达到 89%，元素标注正确率可以达到 85%。</p><p>今年，我们 ACL-2018 的一件工作也是在 Freebase 上做的，我们在一个具体的金融领域做一些项目（不是在通用领域），能不能发挥更好的作用。在金融领域做金融事件的挖掘，做了四类，冻结、质押、回购、增减持。能不能用知识监督方法<strong>建立训练语料</strong>把四类事件抽出来。<strong>我们主要的方法，利用金融知识库，回标的文本是上市公司年报，这是回标的句子，后面是回标以后具体的深度学习的方法，时间限制不做具体讲解。</strong></p><p>从我们的实验可以看出来，在一个上市公司年报相对比较规范的文本中，知识库也比较详细，我们可以比通用领域做得更好，基本上可以达到 90% 的水平，给企业做这样的知识库，他们再去做人工的编辑，做出来的知识资源还是非常有用的，这是我们的方法在金融领域的应用。</p><p>今天我大概讲了这几个事情：知识图谱很重要，事件图谱是知识图谱中很重要的类型，为了建立事件图谱，我们需要研究开放域关系抽取，开放域事件抽取等等，其实可以在这方面做出很多有意思的工作，也可以有很多的应用，是一种很有潜力的方法。</p><h2 id="开放域环境下文本事件抽取——Open-Domain-Event-Extraction-from-Texts"><a href="#开放域环境下文本事件抽取——Open-Domain-Event-Extraction-from-Texts" class="headerlink" title="开放域环境下文本事件抽取——Open Domain Event Extraction from Texts"></a>开放域环境下文本事件抽取——Open Domain Event Extraction from Texts</h2><p>Entity-Centric Knowledge Graph</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkipoac8rqj21g10q7hbc.jpg" alt="实体知识图谱"></p><p>Event-Centric Knowledge Graph</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkipp0gfeoj218h0q1aqu.jpg" alt="事件知识图谱"></p><p>Event Frame</p><p><strong>老师的研究任务的前提是Event Frame是预先给定好的。</strong></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkipsksztaj219i0oc12n.jpg" alt="Extract Events from Texts"></p><p>事件是由一些动作<strong>触发</strong>的，会触发不同的事件。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkipw777cej21ev0qtwtd.jpg" alt="事件的定义"></p><p>我们可以将<strong>事件抽取</strong>的任务看做下面4个子任务：</p><ol><li><strong>Event Identification（Trigger Words）</strong></li><li><strong>Event Type Identification</strong></li><li><strong>Argument Identification</strong></li><li><strong>Argument Role Identification</strong></li></ol><h3 id="Event-Extraction-vs-Relation-Extraction"><a href="#Event-Extraction-vs-Relation-Extraction" class="headerlink" title="Event Extraction vs. Relation Extraction"></a>Event Extraction vs. Relation Extraction</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki25t8a2wj20n60ca78v.jpg" alt="Event Extraction vs. Relation Extraction"></p><ol><li>关系的数量：关系抽取的目标是抽取两个实体之间的一个关系；而事件抽取的目标是抽取一个Trigger Word和每个Argument Words之间的关系；</li><li>事件在文本中一般没有显式表示，是通过Trigger Word表达的，因此在开放域下识别文本中的Trigger Word也是非常困难的。</li></ol><h3 id="Previous-Event-Extraction-Task"><a href="#Previous-Event-Extraction-Task" class="headerlink" title="Previous Event Extraction Task"></a>Previous Event Extraction Task</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki2bedsx6j20ks0ajtbg.jpg" alt="Previous Event Extraction Task"></p><ol><li><p>ACE</p><p>主要侧重<strong>事件的识别</strong>和<strong>事件相关信息的抽取</strong></p></li><li><p>TDT</p><p>主要侧重聚类，将描述相同topic的文本聚类到一起，在这里topic就是事件.</p></li></ol><h3 id="Challenge-in-Open-Domain-Event-Extraction"><a href="#Challenge-in-Open-Domain-Event-Extraction" class="headerlink" title="Challenge in Open Domain Event Extraction"></a>Challenge in Open Domain Event Extraction</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki2fnjm81j20lv0c077c.jpg" alt="Challenge in Open Domain Event Extraction"></p><p>在开放域下进行事件的抽取会带来很多的问题：</p><ol><li>如上图，即使在Event Frame给定的情况下，也很难得到一个比较大的标注好的数据集，因此，我们希望能够建立一个模型，来帮助我们来自动的标注数据，即使标注的数据是有噪声的，然后我们利用这些标注数据再来训练事件抽取器。</li><li>事件抽取器：<strong>How to represent features to indicate events</strong>，问题在于不同类型的事件有不同的表示，我们能否得到一种普适的特征表示方法；</li><li><strong>How to acquire sufficient training data for multiple event types/classes</strong></li></ol><p>接下来，刘康老师开始讲解他们组这几年在<strong>How to represent features to indicate events</strong>和<strong>How to acquire sufficient training data for multiple event types/classes</strong>的探索~</p><h3 id="Challenge-1：Feature-Representation"><a href="#Challenge-1：Feature-Representation" class="headerlink" title="Challenge 1：Feature Representation"></a>Challenge 1：Feature Representation</h3><h4 id="Traditional-Methods-for-Feature-Representation"><a href="#Traditional-Methods-for-Feature-Representation" class="headerlink" title="Traditional Methods for Feature Representation"></a>Traditional Methods for Feature Representation</h4><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki2qfquywj20nq0b9tdt.jpg" alt="Traditional Methods for Feature Representation"></p><ul><li>传统方法缺乏可扩展性</li><li><strong>老师尝试使用深度学习的方法来进行特征的提取</strong></li></ul><h4 id="Dynamic-Multi-pooling-Convolutional-Neural-Network"><a href="#Dynamic-Multi-pooling-Convolutional-Neural-Network" class="headerlink" title="Dynamic Multi-pooling Convolutional Neural Network"></a>Dynamic Multi-pooling Convolutional Neural Network</h4><p>下图是研究者比较早期的一个工作，目标是判别事件中argument role之间的关系（即cameraman和fired之间的关系）：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki33f4gb2j20o70d4gs2.jpg" alt="Dynamic Multi-pooling Convolutional Neural Network"></p><ul><li><p><code>Lexical Level Feature</code>：词法级别的特征；</p></li><li><p><code>Sentence Level Feature</code>：句子级别的特性；</p></li></ul><p>以上两种特征是事件抽取中常见的两类特征。</p><p>上图的亮点主要在于使用了Dynamic Multi-Pooling Layer的思想：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki3gd79myj216g0q8n9l.jpg" alt="Dynamic Multi-Pooling Layer"></p><p>看右图，两个蓝色的词其实天然的将句子划分成三个部分~所以不能简单暴力max-pooling</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkiqfms58jj21h70pre0s.jpg" alt="实验结果"></p><hr><p>利用更多的信息，继续对模型进行改进：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki4sdnpmbj21cm0k97ev.jpg" alt="Event arguments are important to the Event Detection"></p><p>考虑在模型训练的时候，事先标注好argument word以及argument word role，<strong>因为argument word会影响event的识别。</strong></p><h4 id="Attention-Supervision"><a href="#Attention-Supervision" class="headerlink" title="Attention Supervision"></a>Attention Supervision</h4><p>在事件识别里面加入attention的思想</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki4t9n9urj21e10oyqki.jpg" alt="More Attention on Arguments Words"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkiqw6oohbj216y0p9wso.jpg" alt="注意力的策略"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki4tqsw3ej21630l3q93.jpg" alt="Regularization in Learning Model"></p><p>目标是让算出来的$\alpha$尽可能接近人工标注的$\alpha$。</p><hr><p>在**<code>event detection</code>**任务下的结果表现：</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki4ugex4lj215x0mudsx.jpg" alt="Compared with State-of-the-arts"></p><h3 id="Challenge-2：Training-Data-Generation"><a href="#Challenge-2：Training-Data-Generation" class="headerlink" title="Challenge 2：Training Data Generation"></a>Challenge 2：Training Data Generation</h3><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki4unv97gj21am0o9wsx.jpg" alt="Generating Labeled Data from Structured KB"></p><p>简单远距离算法。</p><p>其实这种“<strong>回标</strong>”思想，在<strong>关系抽取</strong>中就已经用到了。但是自动回标会产生噪声数据~</p><p>那么我们来看看这种方法能否搬到我们的事件抽取中呢？很遗憾是不能的。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gki4v7tx1bj21as0r8as1.jpg" alt="The Strategy doesn't work for Event Extraction"></p><p>在目前已有的知识库中，事件并没有显式地表示出来，而是通过一个特殊的编号（例如：m:02nqglv）来表示的，这样就不能像关系抽取那样用于回标了。因此，我们使用trigger word，但是已有的知识库中，并没有给出trigger words。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj134rm45j20mk0cndj6.jpg" alt="生成训练数据"></p><p>关键在于如何找到trigger word，一个有效地办法是在knowledge base里面将跟这个事件有关的实体都进行回标时，我们把这句话里面的动词拿出来就是trigger word，但是一句话里面不一定会包含所有的argument！</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj16m7566j20nf0bhaek.jpg" alt="Argument Selection for Events"></p><p>如上图，不能使用所有的argument进行回标。我们需要寻找关键的argument——key argument。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj182g2gkj20mq09y0wf.jpg" alt="Generating Labeled Data in Event Extraction"></p><p>上图就是整个产生训练数据的流程。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj1cpt114j20nq0cm79i.jpg" alt="Generating Labeled Data in Event Extraction"></p><p>上图是一种找key argument的方法~使用的是基于TF-IDF的思想。使用topK的argument进行回标。</p><p>再使用TF-IDF进行trigger word的筛选！</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj1jisl5xj20nr0bm0xr.jpg" alt="Generating Labeled Data in Event Extraction"></p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7aizngaj21d60qjh2m.jpg" alt="回标"></p><p>trigger words和argument words。</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7bugoodj20lv0bxn1u.jpg" alt="避免噪声数据"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7de12omj20me0cndml.jpg" alt="实验"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7dseeb6j20ks08wjup.jpg" alt="结果"></p><p>ED是自动标注的，ACE是人工标注的。</p><hr><p>Event Types and Schema in Finical Domain</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7hn257hj20mj0c3n18.jpg" alt="金融领域的事件类型和模式"></p><hr><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7i18rypj20jz0budja.jpg" alt="9OUWE2D%$9@(7FL)I)R4RGL.png"></p><p>第一步是文本分类</p><p>第二步是找到</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7jdrgjaj20mo0bwn1t.jpg" alt="开放域事件抽取中的两个问题"></p><p>依然是之前说的那两个问题~</p><p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gkj7kz9aphj20hf0anac0.jpg" alt="结果"></p><h2 id="研究者"><a href="#研究者" class="headerlink" title="研究者"></a>研究者</h2><ol><li>赵军</li><li>刘康</li><li>刘挺</li></ol><h2 id="参考学习链接"><a href="#参考学习链接" class="headerlink" title="参考学习链接"></a>参考学习链接</h2><ol><li><a href="https://www.jianshu.com/p/9568109fedd6">论文简记 | 开放域事件抽取综述</a></li><li><a href="https://www.techbeat.net/talk-info?id=226">开放域环境下文本事件抽取</a></li></ol><h2 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h2><p>[^2]:Freebase(Bollacker et al.,2008)是一个巨大的，免费的事实数据库，组织成三元组的形式（subject Entity,Relationship,object Entity）。Freebase的实体和关系都有类别，并且类别和关系的词典类似 </p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>史上最烂的4种学习方法</title>
      <link href="/2020/10/30/shi-shang-zui-lan-de-4-chong-xue-xi-fang-fa/"/>
      <url>/2020/10/30/shi-shang-zui-lan-de-4-chong-xue-xi-fang-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="机械式记忆"><a href="#机械式记忆" class="headerlink" title="机械式记忆"></a>机械式记忆</h2><p>记忆有三个过程：</p><ol><li>编码</li><li>存储</li><li>提取</li></ol><p>机械式记忆可以完全前面两步，也就是成功编码存储进我们的脑子，但是不能被我们提取出来。</p><p>认知通化理论将学习</p>]]></content>
      
      
      <categories>
          
          <category> 学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 干货 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法岗面试准备</title>
      <link href="/2020/10/17/suan-fa-gang-mian-shi-zhun-bei/"/>
      <url>/2020/10/17/suan-fa-gang-mian-shi-zhun-bei/</url>
      
        <content type="html"><![CDATA[<p>准备算法岗的面试需要指导下面的东西：</p><ol><li>从迷茫到清晰的职业发展历程</li><li>大厂小厂的招聘逻辑</li><li>合格优秀的算法工程师的核心能力<ul><li><strong>模型理解</strong></li><li><strong>数据分析</strong></li><li><strong>工程能力</strong></li></ul></li></ol><h2 id="从迷茫到清晰"><a href="#从迷茫到清晰" class="headerlink" title="从迷茫到清晰"></a>从迷茫到清晰</h2><p>说到算法岗位，现在网上的第一反应可能就是内卷，算法岗位也号称是内卷最严重的岗位。针对这个问题，其实之前我也有写过相关的文章。这个岗位竞争激烈不假，但我个人觉得称作内卷有些过了。就我个人的感觉，这几年的一个大趋势是<strong>从迷茫走向清晰</strong>。</p><p>早在2015年我在阿里妈妈实习的时候，那个时候我觉得其实对于算法工程师这个岗位的招聘要求甚至包括工作内容其实业内是没有一个统一的标准的。可以认为包括各大公司其实对这个岗位具体的工作内容以及需要的候选人的能力要求都不太一致，不同的面试官有不同的风格，也有不同的标准。</p><p>我举几个例子，第一个例子是我当初实习面试的时候，因为是本科生，的确对机器学习这个领域了解非常非常少，可以说是几乎没有。但是我依然通过了，通过的原因也很简单，因为有acm的获奖背景，面试的过程当中主要也都是一些算法题，都还算是答得不错。但是在交叉面试的时候，一位另一个部门的总监就问我有没有这块的经验？我很明确地说了，没有，但是我愿意学。接着他告诉我，算法工程师的工作内容主要和机器学习相关，因此机器学习是基本的。当时我就觉得我凉了，然而<strong>很意外地是还是通过了面试</strong>。</p><p>另外一个例子是当时的一个小伙伴，他在最后转正面试的时候被要求<strong>用R语言写了一个树模型</strong>。具体是什么模型我不记得了，我只知道当时我还不会，所以没记住名字。但是了解行情的都知道，现在工业界应该已经基本上没有使用R语言的了。不论是当时的面试官对R语言比较熟悉，还是当时他的团队里用的都是R语言，都可以说明其实整个阿里内部当时对于算法工程师是没有统一标准的。实际上也的确如此，当时还有很多算法工程师和数据挖掘工程师的title混为一谈的现象。</p><p>我不太清楚现在校招算法岗位会问一些什么，但是就我的了解，机器学习的模型原理以及之前的一些使用经验肯定是跑不了的。这两块的技能没有，只靠acm的奖项以及算法能力想要通过面试估计很难，更不要说是阿里妈妈这样的核心部门了。</p><p>从这两个例子可以看出来，<strong>5年之前业内还是比较迷茫的，但是现在越来越清楚了</strong>，这个岗位究竟是做什么的，需要工程师有什么样的能力。并且我觉得以后应该会越来越清晰，现在可能还是会有一些个别的部门或者是上层的架构师对算法这块不是非常了解。比如工程出身的tech leader带算法团队这样的例子还不少见，但以后应该会越来越少。</p><p>所谓没有985，没有名校硕士，没有paper去面试大公司的算法岗位就是炮灰就是送的说法，有一定的道理。但是我们不能只看现象胡乱总结，仔细分析背后的原因其实是对算法工程师的要求越来越明确。但是要求虽然清晰，也还有问题，问题就是<strong>这些能力不是非常方便在面试当中体现</strong>，对于校招社招都有这样的问题。社招这个问题会好些，看之前的经历就看得出来，校招这个问题相对比较严重，我们怎么知道你数据处理能力怎么样？你对模型的细节了解多少？怎么样判断你来了之后能不能hold住这一块事情？</p><p>正因为有了这样的问题，所以很多面试官或者是hr才不得不抬高硬性要求。招一些基础好、背景出色人聪明的学生进来总没错，即使不对口，也可以现学。</p><h2 id="招聘逻辑"><a href="#招聘逻辑" class="headerlink" title="招聘逻辑"></a>招聘逻辑</h2><p>接下来和大家聊聊各种公司的招聘逻辑，这里面我也发现了一些规律。</p><p>这里面的规律就是<strong>越是小厂越务实，越是大厂越虚</strong>。这其实也很好理解，因为小厂的资金和预算都有限，所有岗位的人头都要精打细算，能不多招绝不多招。一旦招了一定是有具体的用处的，比如某一块事情人手不够，或者是某个问题没有人解决，必须要招人。在这种情况下，小厂的要求非常明确，就是技术栈越匹配越好。也就是你会的东西和他们用的东西越匹配越好，越匹配你上手和学习的成本越低，来了直接能干活的最好。</p><p>大厂则不然，越大的厂越不然。原因也简单，大厂招聘的目的除了满足人力需求之外，还有其他的意义就是<strong>人才储备</strong>。比如清华姚班一年毕业30人，这30人如果都去了腾讯，会给腾讯的发展起到多大的帮助？如果你是老马你能接受这样的事情吗？肯定不行，因为优秀的人才的数量是有限的，虽然竞争的人多，但是头部用户一直就那么多。其他公司多招走了一些，留给你们公司的就少一些。身为大公司肯定是会努力争取的，<strong>包括招聘实习生什么的，其实本质上的目的都是招聘人才进行储备。</strong></p><p>某种程度上来说，<strong>大公司把一个优秀的学生培养成优秀工程师花费的代价要小于去市场上招聘一个同样优秀的工程师的代价</strong>。算法岗位当中至少有80%以上的顶级人才都在大公司手里攥着，广大二线、三线的小公司争抢剩下的20%。大公司想要在市场上招聘一个优秀的工程师是非常难的，远远比大家想的要难。这样的人手里往往不缺offer，加上大公司里层级以及待遇的限制，想要抢得过那些财大气粗的二线公司真的不容易。</p><p>所以面试小公司的核心逻辑就是对口，他们需要的能力你有，他们需要的技术你会，就行了，你搞出很多花里胡哨的东西来其实没什么用。有些小公司的面试官连acm是什么都不知道，你和他说你是亚洲区的银牌有用么？</p><p>大公司的招聘要求就比较琐碎了，一般来说比较<strong>看重基础</strong>。这里的基础并不仅仅是基础知识，准确地说是基础能力。比如数据结构和算法的能力，比如思维的灵活度，也就是给你出个算法题，看你能不能灵活地做出来。<strong>再比如机器学习的基础，模型的一些基础原理等等。</strong></p><p>除了基础之外另外一个很重要的是你的<strong>软实力</strong>，比如你的表达能力，你的情商，遇到困难的时候的反应。比如遇到难题你有没有一个清晰的思考过程，简单尝试了一下就放弃了还是愿意不停地努力。还有你的性格，比如你是服从性的还是比较有主见的，如果你很有个性，老板可能会觉得你比较难管，用术语来说就是管理成本有点高，这也会让面试官打退堂鼓。</p><p>还有一点常说的是<strong>潜力</strong>，潜力这是一个很虚的概念，完全是面试官的主观感受，很难有实际的支撑。根据我的理解大概是这么几个方面，一个是<strong>智商</strong>，聪明的人学东西快，潜力高，这个是毋庸置疑的。即使你很多东西不会，如果你能让面试官觉得你能很快学会，这些不是问题，那么这就不是一个减分项。还有<strong>年龄</strong>，比如中间经过了一些曲折浪费了几年比其他应聘者年长很多（两岁以上），可能会让面试官觉得你潜力折损。再比如你过去的一些经历，比如你在某个领域坚韧不拔从0做起，成长迅速，也会让面试官觉得你潜力不错。</p><p>这样说起来<strong>实践的能力反而是加分项而不是主要的了</strong>，这也有一定的道理。因为对于大公司而言，整个技术架构往往都是自己独立建的，和外面的都不一样。也就是说除非之前在里面待过的，否则几乎找不到技术栈完全懂的。再加上校招生原本实战经验就要少一些，<strong>相比于招一个立即能上手的，更加倾向于能够快速学会的。</strong></p><h2 id="核心能力"><a href="#核心能力" class="headerlink" title="核心能力"></a>核心能力</h2><p>由于我已经很久没有接触校招了，所以也很难说校招面试应该怎么样准备，只能说说如果是我来招聘，我会喜欢什么样的学生。也可以理解成我理解的一个合格优秀的算法工程师应该有的能力。</p><h3 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h3><p>算法工程师和模型打交道，那么理解模型是必须的。其实<strong>不用说每一个模型都精通，这没有必要</strong>，面试的时候问的模型也不一定用得到。但更多地是看重这个人在<strong>学习的时候的习惯</strong>，他是浅尝辄止呢，还是会刨根究底，究竟能够学到怎样的地步。</p><p>在实际的工作当中我们可能会面临各种各样的情况，比如说新加了特征但是没有效果，比如升级了模型效果反而变差了等等，这些情况都是有可能发生的。当我们遇到这些情况之后，需要我们根据已知的信息来推理和猜测导致的原因从而针对性的采取相应的手段。因此这就需要我们对当前的模型有比较深入地了解，否则推导原因做出改进也就无从谈起。</p><p><strong>所以面试的时候问起哪个模型都不重要，重要的是你能不能体现出你有过深入的研究和理解。</strong></p><h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><p>算法工程师一直和数据打交道，那么<strong>分析数据、清洗数据、做数据的能力也必不可少</strong>。说起来简单的数据分析，这当中其实牵扯很多，简单来说至少有两个关键点。</p><p>第一个关键点是<strong>处理数据的能力</strong>，比如SQL、hive、spark、MapReduce这些常用的数据处理的工具会不会，会多少？是一个都不会呢，还是至少会一点。由于各个公司的技术栈不同，一般不会抱着候选人必须刚好会和我们一样的期待去招人，但是候选人如果一无所知肯定也是不行的。由于学生时代其实很少接触这种实践的内容，很多人对这些都一无所知，如果你会一两个，其实就是加分项。</p><p>第二个关键点是<strong>对数据的理解力</strong>，举个简单的例子，比如说现在的样本训练了模型之后效果不好，我们要分析它的原因，你该怎么下手？这个问题日常当中经常遇到，也非常考验算法工程师对数据的分析能力以及他的经验。数据是水，模型是船，我们要把船驶向远方，只懂船只构造是不行的，还需要对水文、天象也有了解。这样才能从数据当中捕捉到trick，对一些现象有更深入的看法和理解。</p><h3 id="工程能力"><a href="#工程能力" class="headerlink" title="工程能力"></a>工程能力</h3><p>虽然是算法工程师，但是并不代表工程能力不重要，相反工程能力也很重要。当然这往往不会成为招聘的硬性指标， 比如考察你之前做过什么工程项目之类的。但是<strong>会在你的代码测试环节有所体现</strong>，你的代码风格，你的编码能力都是你面试的考察点之一。</p><p>并不只是在面试当中如此，在实际工作当中，工程能力也很关键。往小了说可以开发一些工具、脚本方便自己或者是团队当中其他人的日常工作，往大了说，你也可以成为团队当中的开发担当，负责其团队当中最工程的工作。比如说<strong>复现一篇paper，或者是从头撸一个模型</strong>。这其实也是一种差异化竞争的手段，你合理地负担起别人负担不了的工作，那么自然就会成为你的业绩。</p><p>时代在变化，行业在发展，如今的校招会问些什么早已经和当年不同了。但不管怎么说，这个岗位以及面试官对于人才的核心诉求几乎是没有变过的，我们从核心出发去构建简历、准备面试，相信一定可以有所收获。</p><p>作者：一枚程序媛酱<br>链接：<a href="https://www.zhihu.com/question/406974583/answer/1508488223">https://www.zhihu.com/question/406974583/answer/1508488223</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>作者：一枚程序媛酱<br>链接：<a href="https://www.zhihu.com/question/406974583/answer/1508488223">https://www.zhihu.com/question/406974583/answer/1508488223</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>作者：一枚程序媛酱<br>链接：<a href="https://www.zhihu.com/question/406974583/answer/1508488223">https://www.zhihu.com/question/406974583/answer/1508488223</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>作者：一枚程序媛酱<br>链接：<a href="https://www.zhihu.com/question/406974583/answer/1508488223">https://www.zhihu.com/question/406974583/answer/1508488223</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>作者：一枚程序媛酱<br>链接：<a href="https://www.zhihu.com/question/406974583/answer/1508488223">https://www.zhihu.com/question/406974583/answer/1508488223</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 工作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>清华大学2021年推荐免试直硕、直博生履行正式报名手续的通知及资格复审要求</title>
      <link href="/2020/10/10/qing-hua-da-xue-2021-nian-tui-jian-mian-shi-zhi-shuo-zhi-bo-sheng-lu-xing-zheng-shi-bao-ming-shou-xu-de-tong-zhi-ji-zi-ge-fu-shen-yao-qiu/"/>
      <url>/2020/10/10/qing-hua-da-xue-2021-nian-tui-jian-mian-shi-zhi-shuo-zhi-bo-sheng-lu-xing-zheng-shi-bao-ming-shou-xu-de-tong-zhi-ji-zi-ge-fu-shen-yao-qiu/</url>
      
        <content type="html"><![CDATA[<h1 id="清华大学2021年推荐免试直硕、直博生履行正式报名手续的通知及资格复审要求"><a href="#清华大学2021年推荐免试直硕、直博生履行正式报名手续的通知及资格复审要求" class="headerlink" title="清华大学2021年推荐免试直硕、直博生履行正式报名手续的通知及资格复审要求"></a>清华大学2021年推荐免试直硕、直博生履行正式报名手续的通知及资格复审要求</h1><h2 id="1-外校推免生查询拟录取信息"><a href="#1-外校推免生查询拟录取信息" class="headerlink" title="1. 外校推免生查询拟录取信息"></a>1. 外校推免生查询拟录取信息</h2><p>即日起，被我校拟录取的外校推免生可登录“清华大学研究生招生网”（<a href="http://yz.tsinghua.edu.cn/%EF%BC%89%E6%9F%A5%E8%AF%A2%E6%8B%9F%E5%BD%95%E5%8F%96%E4%BF%A1%E6%81%AF%E3%80%82">http://yz.tsinghua.edu.cn/）查询拟录取信息。</a></p><h2 id="2-外校拟录取推免生网上报名手续"><a href="#2-外校拟录取推免生网上报名手续" class="headerlink" title="2. 外校拟录取推免生网上报名手续"></a>2. 外校拟录取推免生网上报名手续</h2><p>所有被我校拟录取的推免生应及时完成“推免服务系统”报名手续。10月8日起，推免生可登录<a href="http://yz.chsi.com.cn/tm%E8%BF%9B%E8%A1%8C%E6%B3%A8%E5%86%8C%EF%BC%8C10%E6%9C%8812%E6%97%A5%E8%B5%B7%E8%BF%9B%E8%A1%8C%E7%BD%91%E4%B8%8A%E6%8A%A5%E5%90%8D%E3%80%82">http://yz.chsi.com.cn/tm进行注册，10月12日起进行网上报名。</a></p><p>所有被我校拟录取的推免生应于<font color="red"><strong>10月13日中午12：00前</strong></font>完成报名、网上支付报名费等手续。</p><p><font color="red"><strong>推免生在“推免服务系统”中所填报志愿的直硕/直博类型、拟录取院系、专业、研究方向、导师（直博生需填）等信息必须与在我校研究生招生系统中查询到的拟录取信息保持一致。</strong></font></p><p><font color="red"><strong>10月13日下午14时左右</strong></font>，我校将向所有完成报名手续并正确填写了志愿信息的推免生发送复试通知，推荐免试生应于<font color="red"><strong>10月14日12：00前</strong></font>在“推荐免试服务系统”中接受复试。</p><p><font color="red"><strong>10月14日下午14时左右</strong></font>陆续发出待录取通知，请及时签约待录取。</p><p><strong>请所有被拟录取的推免生严格按照以上时间安排进行相应操作，未及时进行相应操作将影响正常录取。未按要求完成相关报名及相关确认手续者视为自动放弃</strong>。</p><h2 id="3-外校拟录取推免生资格复审"><a href="#3-外校拟录取推免生资格复审" class="headerlink" title="3. 外校拟录取推免生资格复审"></a>3. 外校拟录取推免生资格复审</h2><p>我校将在<font color="red"><strong>2021年6月</strong></font>对所有拟录取的推免生进行资格复审。通过资格复审并经我校调档案审查合格后，方可被正式录取，我校予以发放录取通知书。未通过资格复审者，取消其研究生录取资格。</p><p>现对资格复审的要求规定如下。推免生应已完成本科培养方案规定的所有课程及实践环节的学分要求，且符合以下条件：</p><ol><li><p><font color="red"><strong>本科最后一学年课程考核成绩不能出现明显下降</strong></font>；</p></li><li><p>未受任何处分；</p></li><li><p><font color="red"><strong>本科综合论文训练（毕业设计）成绩达到80分（含）以上</strong></font>；</p></li><li><p><font color="red"><strong>2021年我校研究生新生入学</strong></font>前获得本科毕业证书和学士学位证书；</p></li><li><p>符合我校各拟录取院（系、所）自定的不低于学校复审要求的标准（具体咨询相应院系）。</p></li></ol><p>推免生应在2021年6月本科最后一学期结束前将本人<font color="red"><strong>最后一学年正式成绩单（含毕业设计成绩，加盖教务处公章）</strong></font>交（寄）到我校拟录取院（系、所）研究生教学办公室，便于及时审查。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 保研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研究生推免系统操作流程</title>
      <link href="/2020/10/07/yan-jiu-sheng-tui-mian-xi-tong-cao-zuo-liu-cheng/"/>
      <url>/2020/10/07/yan-jiu-sheng-tui-mian-xi-tong-cao-zuo-liu-cheng/</url>
      
        <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/9b63ed6fgy1gjh2q9empxj21h90p17b5.jpg" alt="推免服务系统说明"></p><p>伴随着推免系统开放的时间越来越近，小伙伴们一定很焦虑：</p><ul><li><p>不知道需要走哪些流程?</p></li><li><p>怎么填报个人信息资料?</p></li><li><p>填报志愿的形式和规则又是怎样的?</p></li></ul><p><strong>在最后关头更要注重细节！</strong></p><p><strong>所以，推免系统填报全攻略来啦！</strong></p><p><strong>我们决不爽约！！！</strong></p><p>从本校获取推免资格后，便可以登录学信网“推免服务系统” 进行查询，看是否最终具有推免资格，如果出现问题的，请及时和相关部门联系，以免影响后续的志愿填报工作。</p><p>（<a href="https://link.zhihu.com/?target=http://yz.chsi.com.cn/kyzx/kp/201409/20140905/1265419991.html">“推免服务系统”推免生操作流程</a>）</p><hr><p><strong>推免生填报推免服务系统主要包括以下6个流程：</strong></p><p><img src="https://pic4.zhimg.com/80/v2-0fa903342843a2ae23b46938a9a0773f_720w.jpg" alt="推免流程"></p><h2 id="01-注册学信网账号并登录系统"><a href="#01-注册学信网账号并登录系统" class="headerlink" title="01 注册学信网账号并登录系统"></a>01 注册学信网账号并登录系统</h2><p><strong>登录中国高等教育学生信息网（简称学信网）注册学信网账号。</strong></p><p>学信网注册网址为：</p><p><a href="https://link.zhihu.com/?target=https://account.chsi.com.cn/account/preregister.action">https://account.chsi.com.cn/account/preregister.action</a></p><p>一般说来每个人大一的时候核查学籍的时候都注册了学信网账号，但是百分之九十都忘记了密码和账号，所以账号密码问题可以找回或者重新注册。重新注册的需要上传手持证件照正反面的照片。这里找回账号的问题以及新注册相信大家都会，和一般网站差不多流程，这里就不用多说明了。</p><p>登录学信网学信档案后，可核实本人学籍学历信息有误错误，并查看自己是否拥有本科学校推荐的保研资格。</p><p>如已有学信网账号可直接登录系统。登录学信档案后，可核实本人学籍学历信息，<strong>如学籍学历信息有误，请务必及时联系本校教务办以及研究生办相关负责人员进行修改。</strong></p><p><img src="https://pic2.zhimg.com/80/v2-0d9e0f8e97f5f1bc5eb9eb3030c92299_720w.jpg" alt="学信网信息"></p><h2 id="02-填报个人资料"><a href="#02-填报个人资料" class="headerlink" title="02 填报个人资料"></a>02 填报个人资料</h2><p>推免生注册后应<strong>登录“推免服务系统”，按要求填报个人资料信息、上传个人照片。</strong>请务必保证填写的信息真实有效，并注意查看系统页面的相关说明。</p><p><strong>姓名拼音建议半角</strong>（将输入法调成默认英文状态输入就可，那些前后鼻音，边音什么的主要不要搞错）。</p><p>注意<strong>“现役军人类型”一栏不要填错</strong>，一般本科生选择非军人，会自动生成，不需要填写。<strong>自动生成的信息核对一下，一般不会有错。</strong></p><p>这里主要需要<strong>上传证件照，格式的大小按照要求</strong>来即可，如果现实上传不了，建议更换为IE浏览器。<strong>籍贯信息别错了。</strong></p><p><img src="https://pic3.zhimg.com/80/v2-9a45f912665ecf14b880ed6af8292d7e_720w.jpg" alt="登录推免系统"></p><hr><p><img src="https://pic3.zhimg.com/80/v2-099653b8bf4dcf135e2c424dd7e4867a_720w.jpg" alt="基本信息"></p><h2 id="03-网上支付"><a href="#03-网上支付" class="headerlink" title="03 网上支付"></a>03 网上支付</h2><p>推免生需网上支付报名费；<strong>具体的报名费数额根据推免生所在地区而有所差别</strong>，部分地区的报名费在推免生接受了预录取之后才会要求进行支付，如：天津、云南地区。</p><p><img src="https://pic3.zhimg.com/80/v2-d72c461a99184b2132831c0a19218b9e_720w.jpg" alt="网上交费"></p><h2 id="04-填报志愿"><a href="#04-填报志愿" class="headerlink" title="04 填报志愿"></a>04 填报志愿</h2><p>一般系统开放信息填写的时候，志愿填写还没有开通。显示为第一张图，开通后显示第二张图，按要求选择你的志愿。</p><p><strong>注意除了直博选择博士外，其他都是选硕士</strong>；另外注意<strong>专硕学硕别选错了，专硕下面没有研究方向，学硕有。</strong></p><p>填写完成后，可以查看是否有录取选项或者复试通知之类的。</p><p>一般的志愿后两项选择无专项计划和非定向就业，支教推免按照实际情况填写。</p><p>除有特殊政策要求的专项计划推免生外，推免生可同时在系统填报三个平行志愿（不分主次），<strong>每个志愿在提交后的48小时内不允许修改</strong>；若志愿提交48小时后仍未接到复试通知，复试未通过，或拒绝待录取通知的推免生可继续填报其他志愿。志愿提交48小时内，如招生单位通过系统明确拒绝推免生申请，推免生可立即填报其他志愿。</p><p><img src="https://pic4.zhimg.com/80/v2-64b0db9271ae1f16eb7d1ee32dec9ef3_720w.jpg" alt="不可填报志愿"></p><hr><p><img src="https://pic3.zhimg.com/80/v2-27d54023bea0c0569840734972efbae2_720w.jpg" alt="我的志愿"></p><hr><p><img src="https://pic4.zhimg.com/80/v2-88106953b301870baf09f02bcda058b7_720w.jpg" alt="照片要求"></p><h2 id="05-复试"><a href="#05-复试" class="headerlink" title="05 复试"></a>05 复试</h2><p><strong>提交报考志愿后，招生单位将通过“推免服务系统”反馈是否同意推免生参加复试的通知。</strong>推免生应及时登录系统，查看相关通知信息。</p><p>如收到复试通知，应在招生单位规定时间内通过系统回复是否同意参加复试，否则招生单位可取消复试通知。未通过系统确认接受复试通知的考生不能被招生单位录取。推免生如同意参加复试，应按招生单位要求办理相关手续并参加招生单位组织的推免生复试。</p><p><img src="https://pic3.zhimg.com/80/v2-abaaf2ca46fc2475b6740a18e3e7ae92_720w.jpg" alt="复试通知"></p><h2 id="06-查看并确认待录取通知"><a href="#06-查看并确认待录取通知" class="headerlink" title="06 查看并确认待录取通知"></a>06 查看并确认待录取通知</h2><p>通过复试的推免生，将收到招生单位通过系统发送的“待录取”通知，推免生应在招生单位规定的时间内通过系统答复是否接受录取通知，否则招生单位可取消待录取通知。</p><p><strong>待录取通知可拒绝多个，但只能接受一个。</strong>如接受，推免生即成为该招生单位的待录取考生。推免生接受招生单位发放的待录取通知后，不能再接受其他待录取通知。</p><p>PS：这一步主要的注意事项就是<strong>不要手一抖点错了拒绝选项！</strong>今年听说有点错的，系统里面点错了改不了。<strong>万一点错了，立即联系你报的学校让对方单位给你重新发送一遍通知</strong>，但是据说比较麻烦，所以请千万千万不要点错了。</p><p>如果志愿显示了一下页面，那么你的推免就成功了！</p><p><img src="https://pic4.zhimg.com/80/v2-fc82631b35810b2170c473b7ac70b3d7_720w.jpg" alt="录取"></p><hr><p><img src="https://pic3.zhimg.com/80/v2-0ed10106ccf2d3d9b3b2b553fd7eb336_720w.jpg" alt="最终结果"></p><p>（注：以上步骤截图参考往年，请各位保研er们切勿纠结时间）</p><p><strong>注意：</strong></p><ol><li><p>请妥善保管登录账号和密码，如出现已注册但登录不了的状况，请及时和学校相关部门予以联系；</p></li><li><p>如对系统自动填入的基本学籍（学历）信息有疑问，请尽快向就读高校的学籍管理部门咨询。请推免生与招生单位密切联系，及时完成网上确认；</p></li><li><p>新政策下2015年起推免生不再进行现场确认，<strong>以“推免服务系统”最终报考录取信息为准</strong>。推免生因填写信息虚假、错误造成的后果由本人承担，因此，在注册填报信息时，一定要谨慎。</p></li></ol><p>祝大家在今年这个前所未有的“十推”里都能够好运连连，offer满满！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 保研 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
